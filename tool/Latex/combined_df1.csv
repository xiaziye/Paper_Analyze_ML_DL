Introduction,Methodology,Conclusion
"\\section{Introduction}\n\nLet $X = (X_n: n \\geq 0)$ be a Markov chain with a large finite or infinite discrete state space $S$. Suppose that the dynamics of $X$ are affected by a scalar decision or control parameter $\\theta$ taking values in some open interval $\\Theta$. (Of course, the theory then trivially extends to vector decision parameters, since the computation of each partial derivative is a scalar problem.) Then, the one-step transition matrix $P(\\theta) = (P(\\theta,x,y): x,y \\in S)$ depends on $\\theta$. Given a reward function $r: S \\to \\mathbb{R}_+$, it is often of importance to analyze the equilibrium (or steady state) reward per unit time, given by \n\\begin{align*}\n   \\alpha(\\theta) = \\sum_{x \\in S} \\pi(\\theta,x) r(x),\n\\end{align*}\nwhere $\\pi(\\theta) = (\\pi(\\theta,x): x \\in S)$ is the equilibrium distribution of $X$. Such an equilibrium distribution exists uniquely when $X$ is irreducible and positive recurrent under $P(\\theta)$. When $r$ is of mixed sign, we can apply our methodology to the positive and negative parts of $r$.\n\nIn many settings, it is also important to compute the derivative $\\alpha'(\\theta)$ with respect to $\\theta$, assuming that it exists. Such derivatives appear naturally in the numerical optimization of Markov chain models and in the statistical analysis of Markov chains and their associated sensitivity analysis; see \\cite{glynn1990likelihood} for a discussion. This gradient information also plays a key role within the policy improvement step found within algorithms designed to compute optimal policies for average reward Markov decision processes; see, for example, \\cite{sutton2018reinforcement} for applications within the context of reinforcement learning. \n\nWhen stochastic simulation is used to compute performance measures, there is already a large literature on numerical computation of such derivatives; see, for example, \\cite{glynn1987likelihood,pflug1992gradient,glasserman1992derivative,glynn1995likelihood}.  \nOn the other hand, when linear algebraic methods are used, \\cite{golub1986using}  \ndescribes the basic computation of the derivative. However, we are unaware of any literature that discusses the truncation issue that inevitably appears when $S$ is large or infinite. This paper introduces this problem and provides the first convergent truncation algorithm with computable rigorous a posteriori error bounds. \n\nThe approach we follow in this paper extends the ideas used in \\cite{zheng2024regenerationbased} and \\cite{infanger2022new}   \nto derive computable error bounds for the quantity $\\alpha(\\theta_0)$, when based on a suitable truncation approximation. In both papers, equilibrium quantities are expressed in terms of expectations involving cycles corresponding to some fixed finite subset of $S$. \\cite{zheng2024regenerationbased} uses a singleton $\\{z\\}$ as the fixed finite subset, whereas \\cite{infanger2022new} uses a more general finite subset within their algorithm and bounds. In this paper, we also use cycles defined in terms of a singleton $\\{z\\}$ as the starting point for our analysis, since it avoids the complications associated with needing to compute error bounds for the gradient of the equilibrium probabilities for the chain observed at return times to $K$, as would appear if we used the more general cycle structure associated with returns to $K$. In particular, the approach we adopt here involves selecting a return state $z \\in S$, and letting $\\tau(z) = \\inf\\{n\\geq1: X_n = z \\}$ be the first return time to $z$. If $\\mathbb{E}^{\\theta}(\\cdot)$ is the expectation on the path-space of $X$ under which $X$ evolves according to the transition matrix $P(\\theta)$ and $\\mathbb{E}^{\\theta}_x(\\cdot) \\overset{\\Delta}{=} \\mathbb{E}^{\\theta}(\\cdot| X_0 =x )$ for $x\\in S$, it is well known from the theory of regeneration that $\\alpha(\\theta)$ can be expressed as  \n\\begin{align}\n\\alpha(\\theta) = \\frac{\\mathbb{E}^{\\theta}_z \\sum_{j=0}^{\\tau(z)-1} r(X_j)}{\\mathbb{E}^{\\theta}_z \\tau(z)};\n\\label{eq:eqn1_1}\n\\end{align}\nsee, for example, \\citet{asmussen2003applied}.\nWe then show how the numerator and denominator and their derivatives can be well approximated via a suitable truncation of the state space $S$. Furthermore, we are able to obtain a posteriori error bounds on the numerator and denominator of (\\ref{eq:eqn1_1}), as well as their derivatives, thereby yielding computable error bounds on the derivative $\\alpha'(\\theta_0)$ for $\\theta_0 \\in \\Theta$.  \n\nThe use of (\\ref{eq:eqn1_1}), rather than working directly with the stationary equation $\\pi(\\theta) = \\pi(\\theta) P(\\theta)$ (where $\\pi(\\theta)$ is encoded as a row vector), provides an expression for $\\alpha(\\theta)$ involving path \\emph{excursions} from $z$ back to $z$ that allow one to use Lyapunov functions to numerically bound the path expectations that arise in computing the derivative.\n Thus, (\\ref{eq:eqn1_1}) plays a key role in our approach. The resulting approximation to $\\alpha'(\\theta_0)$ is, in significant generality, convergent to $\\alpha'(\\theta_0)$ as the truncation set expands to the entire state space $S$; see Proposition \\ref{prop1}.","\section{The Truncation Approximation to the Gradient}\label{sec:trunc_approx_grads}\n\nLet $e: S \to \mathbb{R}_+$ be the function for which $e(x) = 1$ for $x \in S$ (the \emph{all one's} function). Also, for $f: S \to \mathbb{R}$ and $x \in S$, let \n\begin{align}\n w(\theta,x,f) = \mathbb{E}_x^{\theta} \sum_{j=0}^{\tau(z)-1} f(X_j). \n\label{eq:eqn2_1}\n\end{align}\nWe note that the approach outlined in the Introduction for computing our truncation approximation to $\alpha'(\theta)$ involves computing approximations to $w(\theta,z,f)$ and $w'(\theta_0,z,f)$ for $f=r$ and $f=e$. Consequently, we show now how to approximate $w(\theta,z,f)$ and $w'(\theta_0,z,f)$ for generic $f$. We assume throughout this paper that $X$ is an irreducible recurrent Markov chain for $\theta \in \Theta$.\n\nLet $A \subseteq S$ be a finite set for which $z$ is an element. Put $\kappa = A - \{z\}$. We write $P(\theta)$ and $f$ in partitioned form as  \n\begin{align}\nP(\theta) = \begin{blockarray}{cccc}\n                 & \{z\} & \kappa & A^c \\\n                 \begin{block}{c[ccc]}\n                 \{z\} & P_{11}(\theta) & P_{1\kappa}(\theta) & P_{14}(\theta) \\\n                 \kappa & P_{\kappa1}(\theta) & P_{\kappa\kappa}(\theta) & P_{\kappa4}(\theta) \\\n                 A^c & P_{41}(\theta) & P_{4\kappa}(\theta) & P_{44}(\theta) \\\n                 \end{block}\n              \end{blockarray},\nf = \begin{blockarray}{cc}\n                     \\\n                 \begin{block}{c[c]}\n                 \{z\} & f_{1}  \\\n                 \kappa & f_{\kappa}  \\\n                 A^c & f_{4}  \\\n                 \end{block}\n              \end{blockarray}.\n\end{align}\n(We index $A^c$ as $4$ to be consistent with Section \ref{sec:computable_error_bounds}.)\nIf $T = \inf\{ n \geq 0: X_n \in A^c \}$, we can approximate $w(\theta,x,f)$ for $x \in A$ via  \n\begin{align}\n \undertilde{w}(\theta,x,f) = \mathbb{E}_x^{\theta} \sum_{j=0}^{(\tau(z) \wedge T) -1} f(X_j).\n\label{eq:eqn__2_2}\n\end{align}\nBut\n\begin{align}\n \undertilde{w}(\theta,x,f) = f(x) + \sum_{y \in \kappa}P(\theta,x,y)\undertilde{w}(\theta,y,f).\n\label{eq:eqn2_4}\n\end{align}\nfor $x \in \kappa$.\nSo, if $\undertilde{w}_{\kappa}(\theta,f) = (\undertilde{w}(\theta,x,f): x \in \kappa)$ is encoded as a column vector, (\ref{eq:eqn2_4}) implies that \n\begin{align*}\n \undertilde{w}_{\kappa}(\theta,f) = f_{\kappa} + P_{\kappa\kappa}(\theta) \undertilde{w}_{\kappa}(\theta,f)\n\end{align*}\nso that\n\begin{align*}\n \undertilde{w}_{\kappa}(\theta,f) = \sum_{n=0}^{\infty} P^n_{\kappa\kappa}(\theta) f_{\kappa}.\n\end{align*}\nWhen $P(\theta)$ is irreducible, it follows that $P^n_{\kappa\kappa}(\theta) \to 0$ as $n \to \infty$ so that\n\begin{align*}\n \undertilde{w}_{\kappa}(\theta,f) = (I - P_{\kappa\kappa}(\theta))^{-1} f_{\kappa};\n\end{align*}\nsee, for example, \citet{kemeny1960finite}. So, we arrive at the approximation\n\begin{align}\n \undertilde{w}(\theta,z,f) = f(z) + P_{1\kappa}(\theta) \mathcal{I}_{\kappa\kappa}(\theta) f_{\kappa},\n\label{eq:eqn2_5}\n\end{align}\nto $w(\theta,z,f)$, where $\mathcal{I}_{\kappa\kappa}(\theta)= (I - P_{\kappa\kappa}(\theta) )^{-1} = \sum_{n=0}^{\infty} P^n_{\kappa\kappa}(\theta)$. \nThe approximation to $w'(\theta_0,z,f)$ is therefore given by\n\begin{align}\n \undertilde{w}'(\theta_0,z,f) =& P'_{1\kappa}(\theta_0) \mathcal{I}_{\kappa\kappa}(\theta_0) f_{\kappa} \nonumber \\\n                                       & + P_{1\kappa}(\theta_0) \mathcal{I}_{\kappa\kappa}(\theta_0) P'_{\kappa\kappa}(\theta_0) \mathcal{I}_{\kappa\kappa}(\theta_0) f_{\kappa}.\n\label{eq:eqn2_6}\n\end{align}\n\nWe now prove that as the truncation set $A$ expands to the entire state space $S$, our approximations converge. In particular, suppose that $z \in A_1$ and that $A_1 \subseteq A_2 \subseteq A_3 \subseteq \ldots$ such that $\bigcup_{n=1}^{\infty} A_n = S$. Let $\undertilde{w}_n(\theta_0,z,f)$ and $\undertilde{w}'_n(\theta_0,z,f)$ be the truncation approximations corresponding to setting $A = A_n$. Under suitable regularity conditions, it is known that \n\begin{align*}\n w(\theta,z,f) = \mathbb{E}_z^{\theta_0} \sum_{j=0}^{\tau(z)-1}f(X_j) \prod_{k=0}^{j-1} \frac{P(\theta,X_k,X_{k+1})}{P(\theta_0,X_k,X_{k+1})}, \n\end{align*}\nand\n\begin{align}\n w'(\theta_0,z,f) &= \mathbb{E}_z^{\theta_0} \sum_{j=0}^{\tau(z)-1}f(X_j) \sum_{k=0}^{j-1} \frac{P'(\theta_0,X_k,X_{k+1})}{P(\theta_0,X_k,X_{k+1})} \nonumber \\\n &= \mathbb{E}_z^{\theta_0} \sum_{j=0}^{\tau(z)-1}  \frac{P'(\theta_0,X_j,X_{j+1})}{P(\theta_0,X_j,X_{j+1})} \sum_{k=j+1}^{\tau(z)-1} f(X_k), \n\label{eq:eqn2_7}\n\end{align}\nwhere\n\begin{align}\n \mathbb{E}_z^{\theta_0} \sum_{j=0}^{\tau(z)-1} |f(X_j)| \sum_{k=0}^{j-1} \frac{|P'(\theta_0,X_k,X_{k+1})|}{P(\theta_0,X_k,X_{k+1})} < \infty; \n\label{eq:eqn2_8}\n\end{align}\nsee \citet{glynn1995likelihood} and \citet{rhee2023lyapunov}. Put $|f| = (|f|(x): x \in S)$.\n\n\begin{proposition}\n\begin{enumerate}\n  \item[(a)] If $w(\theta_0, z, |f|) < \infty$, then  $\undertilde{w}_n(\theta_0, z, f) \to w(\theta_0, z, f)$ as $n \to \infty$.\n  \item[(b)] If (\ref{eq:eqn2_8}) holds, then $\undertilde{w}'_n(\theta_0, z, f) \to w'(\theta_0, z, f)$ as $n \to \infty$.\n\end{enumerate}\n\label{prop1}\n\end{proposition}\n\n\begin{proof}\n Let $T_n = \inf\{ k\geq0: X_k \in A^c_n  \}$, and note that $T_n \to \infty$. Also, (\ref{eq:eqn__2_2}) implies that  \n\begin{align*}\n \undertilde{w}_n(\theta,z,f) = \mathbb{E}_z^{\theta} \sum_{j=0}^{(\tau(z) \wedge T_n) -1} f(X_j).\n\end{align*}\nBut $\sum_{j=0}^{(\tau(z) \wedge T_n) -1} f(X_j) \to \sum_{j=0}^{\tau(z) -1} f(X_j)$ a.s. as $n \to \infty$, and \n\begin{align*}\n\left| \sum_{j=0}^{(\tau(z) \wedge T_n) -1} f(X_j)\right| \leq \sum_{j=0}^{\tau(z) -1} |f(X_j)|,\n\end{align*}\nso that the Dominated Convergence Theorem yields (a) in the presence of $w(\theta_0, z, |f|) < \infty$.  \n\nAs for (b), it is easily verified that  \n\begin{align*}\n \undertilde{w}'(\theta_0,z,f) = \mathbb{E}_z^{\theta_0} \sum_{j=0}^{(\tau(z)\wedge T)-1}f(X_j) \sum_{k=0}^{j-1} \frac{P'(\theta_0,X_k,X_{k+1})}{P(\theta_0,X_k,X_{k+1})}, \n\end{align*}\nso that\n\begin{align*}\n \undertilde{w}'_n(\theta_0,z,f) = \mathbb{E}_z^{\theta_0} \sum_{j=0}^{(\tau(z)\wedge T_n)-1}f(X_j) \sum_{k=0}^{j-1} \frac{P'(\theta_0,X_k,X_{k+1})}{P(\theta_0,X_k,X_{k+1})}. \n\end{align*}\nAgain,\n\begin{align*}\n  \sum_{j=0}^{(\tau(z)\wedge T_n)-1}f(X_j) \sum_{k=0}^{j-1} \frac{P'(\theta_0,X_k,X_{k+1})}{P(\theta_0,X_k,X_{k+1})} \to \sum_{j=0}^{\tau(z)-1}f(X_j) \sum_{k=0}^{j-1} \frac{P'(\theta_0,X_k,X_{k+1})}{P(\theta_0,X_k,X_{k+1})}\n\end{align*}\nas $n \to \infty$. In view of (\ref{eq:eqn2_8}), the Dominated Convergence Theorem yields (b). \n\end{proof}",
"\\section{Introduction}\n\\noindent \nThe purpose of this note is to find inequalities for positive linear functionals and study the numerical range and numerical radius on $\\mathbf{C}^*$-algebras, which extend some results of classical numerical range and numerical radius, studied in \\cite{Bhunia_RMJ, Bhunia_LAA_2021,Bhunia_RIM_2021,Bhunia_BSM_2021,Dragomir-2008, Hirzallah, Kittaneh2005,Kittaneh2003}.  Let us first introduce the necessary notations and terminologies.\n\n\\noindent \n\nThroughout this note, let $\\mathcal{A}$  be a unital $\\mathbf{C}^*$-algebra with unit $e.$ Let $S(\\mathcal{A})$ denote the set of all normalized states, i.e., $ S(\\mathcal{A})=\\{ f \\in \\mathcal{A}' : f(e)=\\|f\\|=1\\},$ where $\\mathcal{A}'$ denotes the space of all continuous linear functionals on $\\mathcal{A}.$ An element $f\\in \\mathcal{A}'$ is said to be positive if $f(a^*a)\\geq 0$ for all $a\\in \\mathcal{A}$, and then we write $f\\geq 0.$ It is well known that $ S(\\mathcal{A})=\\{ f \\in \\mathcal{A}' : f\\geq 0,\\, f(e)=1\\}.$\nThe algebraic numerical range of $a\\in \\mathcal{A}$, denoted by $V(a)$, is defined as $V(a)=\\{ f(a): f\\in S(\\mathcal{A})\\},$ see \\cite{Bonsal} and \\cite[p. 59]{book1}. It is well known that $V(a)$ is a compact and convex subset of the complex plane $\\mathbb{C}$. The algebraic numerical radius of $a\\in \\mathcal{A}$, denoted by $v(a)$, is defined as $v(a)=\\sup\\{ |f(a)|: f\\in S(\\mathcal{A})\\}.$ The algebraic numerical radius $v(\\cdot): \\mathcal{A}\\to \\mathbb{R}$ defines an equivalent norm on $\\mathcal{A}$ via the relation $\\frac{1}{2}\\|a\\| \\leq v(a)\\leq \\|a\\|$ for every $a\\in \\mathcal{A}$. Here the inequalities are sharp, $\\frac{1}{2}\\|a\\| = v(a)$ when $a^2=0$ and $ v(a)= \\|a\\|$ when $a$ is normal. Like as the usual norm on $\\mathcal{A},$ the algebraic numerical radius also satisfies the power inequality, i.e., $v(a^n)\\leq v^n(a)$ for all $n=1,2, \\ldots.$\nFor $a\\in \\mathcal{A}$, let $Re(a)=\\frac12 (a+a^*)$ and $Im(a)=\\frac1{2i} (a-a^*),$ where $i=\\sqrt{-1}$ denotes the imaginary unit. Then clearly $Re(a)$ and $Im(a)$ are the self-adjoint elements in $\\mathcal{A}$ and it is easy to verify that $\\max_{|\\lambda|=1 }\\|Re(\\lambda a)\\|=v(a),$ see in \\cite[Lemma 4.2]{LAMA2016}.\nLet $|a|$ denote the absolute value of $a\\in \\mathcal{A}$, i.e.,  $|a|=(a^*a)^{1/2}.$ Then $|a|$ is a self-adjoint (positive) element in $\\mathcal{A}$ and $\\| |a|\\|=\\|a\\|.$\nFor a comprehensive account of the algebraic numerical radius in $\\mathbf{C}^*$-algebras, the reader can see \\cite{Alahmari,Bonsal,Mt3, LAMA2016, Mabrouk}.\n\n\\noindent \n\nIf $\\mathcal{A}=\\mathcal{B}(\\mathcal{H})$ is the set of all bounded linear operators on a complex Hilbert space $\\mathcal{H}$ and $A\\in \\mathcal{B}(\\mathcal{H})$, then $V(A)$ is the closure of the numerical range $W(A)=\\{ \\langle Ax,x\\rangle : x\\in \\mathcal{H}, \\, \\|x\\|=1\\}$, where $\\langle\\cdot,\\cdot\\rangle$ and $\\|\\cdot\\|$ denote as the usual inner product and its norm in $\\mathcal{H},$ respectively. The numerical range $W(A)$ is a bounded convex (not necessarily closed) subset of $\\mathbb{C}$. For finite dimensional Hilbert space $\\mathcal{H}$, $W(A)$ is closed.\nThe numerical radius of $A$ is $w(A)=\\sup\\{ |\\langle Ax,x\\rangle | : x\\in \\mathcal{H}, \\, \\|x\\|=1\\}$, which coincides with the algebraic numerical radius $v(A).$ To study the numerical range and the numerical radius of bounded linear operators, we refer to see \\cite{book1, book2}.\n\\noindent The concept of numerical range and the associated numerical radius of a bounded linear operator defined on a complex Hilbert space are very useful in investigating many analytic and geometric properties of the operator and the corresponding space. The numerical range (also the numerical radius) has found its applications in many fields of sciences, such as functional analysis (estimates of norms), operator theory (differential operators), matrix theory (location of eigenvalues), system theory (singular values), numerical analysis (convergence rate of algorithms), quantum information theory (quantum channels), quantum computing (quantum error correction) and quantum control (optimizing witnesses). Because of the importance of the numerical radius many researchers have been trying to study better approximation of it,\nwe refer to see \\cite{Bhunia-AdM, BhuniaLAA-1, Bhunia-IJPA, Bhunia-IJPA2,  Conde-RIM,Nayak,SababhehCAOT,SababhehFAA, Sahoo} and the references therein. \nThere are many generalizations of the classical numerical range and numerical radius, and there has been a great interest in their systematic properties and applications, we refer to see \\cite{Mt1, Mt2, BJMA2024, Bhunia-Kittaneh, Mabrouk, Mt11, Mt3,Mt4,Mt5,Mt6,Mt7,Mt8,Mt9}.\n\n\n\\noindent\n\nIn this note, we develop several inequalities for the algebraic numerical radius by developing the inequalities of positive linear functionals, and obtain equality characterizations. These results extend the results related to the classical numerical range $W(A)$ and the numerical radius $w(A)$ of a bounded linear operator $A$ acting on a complex Hilbert space.\nIn Section \\ref{sec2}, we provide lower bounds for $v(a)$, which \nimprove the bounds $\\frac{\\|a\\|}{2}\\leq v(a)$ and   $\\frac{1}{4}\\|a^*a+aa^*\\| \\leq v^2(a).$ We also study equality characterizations for these inequalities. Among other  characterizations, \n we show that $v(a)=\\sqrt{\\frac{1}{4}\\|a^*a+aa^*\\|}$ (resp., $v(a)=\\frac{\\|a\\|}{2}$) if and only if  $\\mathbb{S}_{\\frac12{ \\| a^*a+aa^*\\|}^{1/2}} \\subseteq V(a) \\subseteq \\mathbb{D}_{\\frac12 {\\| a^*a+aa^*\\|}^{1/2}}$ (resp., $\\mathbb{S}_{\\frac12 \\| a\\|} \\subseteq V(a) \\subseteq \\mathbb{D}_{\\frac12 \\| a\\|}$),\nwhere $\\mathbb{D}_k$ (resp., $\\mathbb{S}_k$) denotes the  circular disk (resp., semi-circular disk) with center at the origin and radius $k$. We provide a formula $v(a)=\\frac{1}{{\\sqrt{2}}} \\max_{|\\lambda|=1}\\left\\| {Re(\\lambda a)\\pm Im(\\lambda a)}\\right\\|.$ Here we also study some inequalities for the $(\\alpha,\\beta)$-normal elements in $\\mathcal{A}.$ For $0\\leq \\alpha\\leq 1\\leq \\beta,$ an element $a\\in \\mathcal{A}$ is said to be $(\\alpha,\\beta)$-normal if $\\alpha^2 f(a^*a)\\leq f(aa^*)\\leq \\beta^2f(a^*a)$ for all $f\\in \\mathcal{A}'$ with $f\\geq 0.$\nIn Section \\ref{sec3}, we obtain upper bounds for $v(a)$, which \nimprove as well as generalize the bounds\n$v(a)\\leq \\|a\\|$ and   $ v^2(a)\\leq \\frac{1}{2}\\|a^*a+aa^*\\|$. \nWe also study equality conditions of the above inequalities. \nIn Section \\ref{sec4}, we study the algebraic numerical radius inequalities for the sum of $n$ elements, sum of $n$ products of elements and commutators of elements.",,
"Introduction: This paper extends the results of \\cite{QQWZ21} to  algebras endowed with several operators, with applications to differential Rota-Baxter algebras and   integro-differential algebras.","""Methodology: In this section,  we recall  free objects in multi-operated setting and the construction of free $\Omega$-semigroups and related structures, and\n   define two new monomial  orders  $\leq_{\operatorname{PD}}$ and $\leq_{\operatorname{uPD}}$  on free multi-operated semigroups and monoids.\n    The  main results of this paper will  highly depend on these new monomial  orders.""","Conclusion: So far, we have completed the study of differential Rota-Baxter algebras."
"\\section{Introduction}\nGiven a commutative ring with identity $R$, we will denote by $\\mathcal S(R)$ the set of all sequences $\\mathbf a = (a_n)_{n \\geq 0}$ such that $a_n \\in R$, for all $n \\in \\mathbb N$. A sequence $\\mathbf a \\in \\mathcal S(R)$ is said to be a \\emph{linear recurrent sequence} with characteristic polynomial $p_a(t) = t^N - \\sum_{i=0}^{N-1} h_i t^{N-i}$ if its elements satisfy the following relation\n\\[a_n = \\sum_{i=0}^{N-1} h_i a_{n-i}\\]\nfor all $n \\geq N$ and the elements $a_0, \\ldots a_{N-1}$ are called \\emph{initial conditions}. We will denote by $\\mathcal W(R) \\subset \\mathcal S(R)$ the set of all linear recurrent sequences. Moreover, given $\\mathbf a \\in \\mathcal S(R)$, we will write $A_o(t)=\\sum_{n=0}^\\infty a_nt^n$ for the ordinary generating function (o.g.f.) and $A_e(t)=\\sum_{n=0}^\\infty \\cfrac{a_n}{n!}t^n$ for the exponential generating function (e.g.f.). It is well known that $\\mathcal S(R)$ and $\\mathcal W(R)$ can be equipped with several operations giving them interesting algebraic structures. \nWhen $R$ is a field, it is immediate to see that the element-wise sum or product (also called the Hadamard product) of two linear recurrent sequences is still a linear recurrent sequence, see, e.g., \\cite{RSbook}. In \\cite{CV}, the authors proved it in the more general case where $R$ is a ring, showing that $\\mathcal W(R)$ is an $R-$algebra and giving also explicitly the characteristic polynomials of the element-wise sum and Hadamard product of two linear recurrent sequences. Larson and Taft \\cite{LT, Taft} studied this algebraic structure characterizing the invertible elements and zero divisors. Further studies about the behaviour of linear recurrent sequences under the Hadamard product can be found, e.g., in \\cite{Cak, GN, Kaz, Zierler}.  \nSimilarly, $\\mathcal W(R)$ equipped with the element-wise sum and the convolution product (or Cauchy product) has been deeply studied. For instance,  $\\mathcal W(R)$ is still an $R-$algebra and the characteristic polynomial of the convolution product between two linear recurrent sequences can be explicitly found \\cite{CV}. The convolution product of linear recurrent sequences is very important in many applications and it has been studied also from a combinatorial point of view \\cite{ABCM} and over finite fields \\cite{Hau}. For other results, see, e. g., \\cite{Stoll, Sza, Sza2}. \nAnother important operation between sequences is the binomial convolution (or Hurwitz product). The Hurwitz series ring, introduced in a systematic way by Keigher \\cite{Keigher}, has been extensively studied by several authors \\cite{BCM2, BCM3, Ben, Ben2, Keigher2, Liu}. However, there are few results when focusing on linear recurrent sequences \\cite{Kur, Kur2}. \n\nIn this paper, we extend the studies about the algebraic structure of linear recurrent sequences considering in particular the Hurwitz product and the Newton product (which is the generalization of the Hurwitz product considering multinomial coefficients). In particular, we prove that $\\mathcal W(R)$ is an $R-$algebra when equipped with the element-wise sum and the Hurwitz product, as well as when we consider element-wise sum and Newton product. We also give explicitly the characteristic polynomials of the Hurwitz and Newton product of two linear recurrent sequences. For the Newton product we find explicitly also the inverses. Moreover, we study the isomorphisms between these algebraic structures, finding that $\\mathcal W(R)$ with element-wise sum and Hurwitz product is not isomorphic to the other algebraic structures, whereas if we consider the Newton product, there is an isomorphism with the $R-$algebra obtained using the Hadamard product. Finally, we provide an overview about the behaviour of linear recurrent sequences under all the different operations considered (element-wise sum, Hadamard product, Cauchy product, Hurwitz product, Newton product) with respect to the characteristic polynomials and their companion matrices.",,
,,
,"""\section{Quality of preconditioners}\label{subsubapp:we_sketch_subspaces}\n\nHere we consider preconditioners of the kind described in \cref{subsec:precond_gen}.\nThese are obtained by sketching a tall $m \times n$ data matrix $\mtx{A}$ in the embedding regime, factoring the sketch, and using the factorization to construct an orthogonalizer.\n\n\begin{proposition}[Adaptation of \cref{prop:left_sketch_precond}]\label{prop:left_sketch_precond_appendix}\n    Consider a sketch $\mtx{A}_{\mathrm{sk}} = \mtx{S}\mtx{A}$ and a matrix $\mtx{U}$ whose columns are an orthonormal basis for $\range(\mtx{A})$.\n    If $\rank(\mtx{A}_{\mathrm{sk}}) = \rank(\mtx{A})$ and the columns of  $\mtx{A}_{\mathrm{sk}}\mtx{M}$ are an orthonormal basis for the range of $\mtx{A}_{\mathrm{sk}}$, then singular values of $\mtx{A}\mtx{M}$ are the reciprocals of the singular values of $\mtx{S}\mtx{U}$.\n\end{proposition}\nObserve that this proposition is a linear algebraic result, i.e., there is no randomness.\nWhen applied to randomized algorithms, the randomness enters only via the construction of the sketch.\n\nThis result can be applied to problems with ridge regularization by working with augmented matrices in the vein of \cref{subsec:precond_gen}.\nIn that context it is necessary to not only replace $(\mtx{A},\mtx{A}_{\mathrm{sk}})$ by $(\mtx{\hat{A}},\mtx{\hat{A}}_{\mathrm{sk}})$, but also to replace $\mtx{S}$ by the augmented sketching operator $\mtx{\hat{S}}$ that takes $\mtx{\hat{A}}$ to $\mtx{\hat{A}}_{\mathrm{sk}}$.\nThe augmented sketching operator in question was already visualized in \cref{alg:saddle_to_ols_sap}.\n\nOur proof of \cref{prop:left_sketch_precond_appendix} requires finding an explicit expression for $\mtx{M}$.\nTowards this end, we prove the following lemma.\n\n\begin{lemma}\label{lem:tool_for_pinv_M}\n    Suppose $\mtx{A}_{\mathrm{sk}}$ is a tall $d \times n$ matrix and that $\mtx{M}$ is a full-column-rank matrix for which the columns of $\mtx{A}_{\mathrm{sk}}\mtx{M}$ form an orthonormal basis for $\range(\mtx{A}_{\mathrm{sk}})$.\n    If $\mtx{B}$ is a full-row-rank matrix for which $\mtx{A}_{\mathrm{sk}} = \mtx{A}_{\mathrm{sk}}\mtx{M}\mtx{B}$, then we have $\mtx{M} = \mtx{B}^{\dagger}$.\n\end{lemma}\n\begin{proof}[Proof of \cref{lem:tool_for_pinv_M}.]\n    Let $k = \rank(\mtx{A}_{\mathrm{sk}}) = \rank(\mtx{A}_{\mathrm{sk}}\mtx{M})$.\n    Since the columns of $\mtx{A}_{\mathrm{sk}}\mtx{M}$ are orthonormal we can infer that it has dimensions $d \times k$.\n    Similarly, since $\mtx{M}$ is full column-rank we can infer that it is $n \times k$.\n    We prove that $\mtx{B} = \mtx{M}^{\dagger}$, which amounts to showing four properties:\n    \begin{enumerate}\n        \item $\mtx{M}\mtx{B}\mtx{M} = \mtx{M}$,\n        \item $\mtx{B}\mtx{M}\mtx{B} = \mtx{B}$,\n        \item $\mtx{B}\mtx{M}$ is an orthogonal projector, and\n        \item $\mtx{M}\mtx{B}$ is an orthogonal projector.\n    \end{enumerate}\n    By the lemma's assumption we have the identity $\mtx{A}_{\mathrm{sk}} = \mtx{A}_{\mathrm{sk}}\mtx{M}\mtx{B}$.\n    Left multiply this expression through by $(\mtx{A}_{\mathrm{sk}}\mtx{M})^{\trans}$ to see that \n    \begin{equation}\label{eq:form_of_candidate_pinvM}\n        \mtx{M}^{\trans}\mtx{A}_{\mathrm{sk}}^{\trans}\mtx{A}_{\mathrm{sk}} = \mtx{B}.\n    \end{equation}\n    Next, we right multiply both sides of \eqref{eq:form_of_candidate_pinvM} by $\mtx{M}$ and use column orthonormality of $\mtx{A}_{\mathrm{sk}}\mtx{M}$ to obtain $\mtx{B}\mtx{M} = \mtx{I}_k$ --- this is sufficient to show the first three conditions for the pseudoinverse.\n    Showing the fourth and final condition takes more work.\n    For that we left multiply \eqref{eq:form_of_candidate_pinvM} by $\mtx{M}$ so as to express\n    \[\n        \mtx{M}\mtx{M}^{\trans}\mtx{A}_{\mathrm{sk}}^{\trans}\mtx{A}_{\mathrm{sk}} = \mtx{M}\mtx{B}.\n    \]\n    Therefore our task is to show that $\mtx{M}\mtx{M}^{\trans}\mtx{A}_{\mathrm{sk}}^{\trans}\mtx{A}_{\mathrm{sk}}$ is an orthogonal projector.\n   \n    Consider the compact SVD $\mtx{A}_{\mathrm{sk}} = \mtx{U}_{\mathrm{sk}}\mtx{\Sigma}_{\mathrm{sk}}\mtx{V}_{\mathrm{sk}}$.\n    Since $\mtx{A}_{\mathrm{sk}}$ is rank-$k$ we have that $\mtx{U}_{\mathrm{sk}}$ has $k$ columns and $\mtx{\Sigma}$ is a $k\times k$ invertible matrix.\n    Since the columns of $\mtx{A}_{\mathrm{sk}}\mtx{M}$ form an orthonormal basis for the range of $\mtx{A}_{\mathrm{sk}}$, it must be that $\mtx{A}_{\mathrm{sk}}\mtx{M} = \mtx{U}_{\mathrm{sk}}\mtx{W}$ for some $k \times k$ orthogonal matrix $\mtx{W}$.\n    Furthermore, this orthogonal matrix can be expressed as $\mtx{\Sigma}_{\mathrm{sk}}\mtx{V}_{\mathrm{sk}}^{\trans}\mtx{M} = \mtx{W}$, which implies\n    \begin{equation}\label{eq:vvproject_expr_for_M}\n            \mtx{V}_{\mathrm{sk}}\mtx{V}_{\mathrm{sk}}^{\trans}\mtx{M} = \mtx{V}_{\mathrm{sk}}\mtx{\Sigma}_{\mathrm{sk}}^{-1}\mtx{W}.\n    \end{equation}\n    We have reached a checkpoint in the proof.\n    Our next task is to obtain an expression for $\mtx{M}$ by simplifying \eqref{eq:vvproject_expr_for_M}.\n    \n    Consider the subspaces $X = \range\mtx{V}_{\mathrm{sk}}$, $Y = \ker\mtx{A}_{\mathrm{sk}}$, and $Z = \range\mtx{M}$, all contained in $\R^n$.\n    We know that $Y \cap Z$ is trivial since $\rank(\mtx{A}_{\mathrm{sk}}\mtx{M}) = \rank(\mtx{M})$.\n    At the same time, since $Y$ and $Z$ are of dimensions $n-k$ and $k$ respectively, it must be that $Z = Y^{\perp}$.\n    This fact can be combined with $Y = X^{\perp}$ (from the fundamental theorem of linear algebra) to obtain $Z = X$, which in turn implies $\mtx{V}_{\mathrm{sk}}\mtx{V}_{\mathrm{sk}}^{\trans}\mtx{M} = \mtx{M}$.\n    Therefore \eqref{eq:vvproject_expr_for_M} simplifies to\n    \[\n        \mtx{M} = \mtx{V}_{\mathrm{sk}}\mtx{\Sigma}_{\mathrm{sk}}^{-1}\mtx{W}.\n    \]\n    This expression is precisely what we need; when the dust settles, it tells us that\n    \[\n        \mtx{M}\mtx{M}^{\trans}\mtx{A}_{\mathrm{sk}}^{\trans}\mtx{A}_{\mathrm{sk}} = \mtx{V}_{\mathrm{sk}}\mtx{V}_{\mathrm{sk}}^{\trans}.\n    \]\n\end{proof}\n\n\begin{proof}[Proof of \cref{prop:left_sketch_precond_appendix}.]\n    Let $k = \rank(\mtx{A})$.\n    It suffices to prove the statement where $\mtx{U}$ is the $m \times k$ matrix containing the left singular vectors of $\mtx{A}$.\n    Our proof involves working with the compact SVD $\mtx{A} = \mtx{U}\mtx{\Sigma}\mtx{V}^{\trans}$, where $\mtx{V}$ is $n \times k$ and $\mtx{\Sigma}$ is invertible.\n    Noting that $\mtx{A}_{\mathrm{sk}} = \mtx{S}\mtx{U}\mtx{\Sigma}\mtx{V}^*$ holds by definition of $\mtx{A}_{\mathrm{sk}}$, we can replace $\mtx{S}\mtx{U}$ by its economic QR factorization $\mtx{S}\mtx{U} = \mtx{Q}\mtx{R}$ to see \begin{equation}\label{eq:prop:left_sketch_precond:factor1}\n        \mtx{A}_{\mathrm{sk}} = \mtx{Q}\mtx{R}\mtx{\Sigma}\mtx{V}^*.\n    \end{equation}\n    Furthermore, since $\rank(\mtx{A}_{\mathrm{sk}}) = k$ it must be that $\rank(\mtx{S}\mtx{U}) = k$.\n    This tells us that $\mtx{R}$ is invertible and that $\mtx{Q}$ provides an orthonormal basis for the range of $\mtx{A}_{\mathrm{sk}}$.\n    \n    By assumption on $\mtx{M}$, the matrix $\mtx{A}_{\mathrm{sk}}\mtx{M}$ is \textit{also} an orthonormal basis for the range of $\mtx{A}_{\mathrm{sk}}$.\n    Therefore there exists a $k \times k$ orthogonal matrix $\mtx{P}$ where $\mtx{Q}\mtx{P} = \mtx{A}_{\mathrm{sk}}\mtx{M}$.\n    We can rewrite \eqref{eq:prop:left_sketch_precond:factor1} as\n    \begin{equation*}\n        \mtx{A}_{\mathrm{sk}} = \left(\mtx{Q}\mtx{P}\right)\left(\mtx{P}^*\mtx{R}\mtx{\Sigma}\mtx{V}^*\right).\n    \end{equation*}\n    Since the left factor in the above display is simply $\mtx{A}_{\mathrm{sk}}\mtx{M}$, we have\n     \begin{equation}\label{eq:suggest_M_dagger}\n     \mtx{A}_{\mathrm{sk}} = \mtx{A}_{\mathrm{sk}}\mtx{M}\left(\mtx{P}^*\mtx{R}\mtx{\Sigma}\mtx{V}^*\right)\textcolor{blue}{.}\n    \end{equation}\n    The next step is to abbreviate $\mtx{B} = \mtx{P}^*\mtx{R}\mtx{\Sigma}\mtx{V}^*$ and apply \cref{lem:tool_for_pinv_M} to infer that $\mtx{B} = \mtx{M}^{\dagger}$.\n    Invoking the column-orthonormality of $\mtx{V}$ and invertibility of $(\mtx{\Sigma},\mtx{R},\mtx{P})$ we further have $\mtx{B}^{\dagger} = \mtx{M} = \mtx{V}\mtx{\Sigma}^{-1}\mtx{R}^{-1}\mtx{P}$.\n    Plug in this expression for $\mtx{M}$ to see that\n    \begin{equation}\label{eq:form_of_preconditioned_A}\n        \mtx{A}\mtx{M} = \left(\mtx{U}\mtx{\Sigma}\mtx{V}^*\right)\left(\mtx{V}\mtx{\Sigma}^{-1}\mtx{R}^{-1}\mtx{P}\right) = \mtx{U}\mtx{R}^{-1}\mtx{P}.\n    \end{equation}\n    The proof is completed by noting that the singular values of $\mtx{R}^{-1}$ are the reciprocals of the singular values of $\mtx{Q}\mtx{R} = \mtx{S}\mtx{U}$.\n\end{proof}\n\n\subsection{Effective distortion in sketch-and-precondition}\label{subsubapp:eff_dist_in_sap}\n\nRecall from \cref{subapp:effective_distortion} that if the columns of $\mtx{U}$ are an orthonormal basis for a linear subspace $L$, then the restricted condition number of $\mtx{S}$ on $L$ is\n\[\n    \cond(\mtx{S};L) = \cond(\mtx{S}\mtx{U}).\n\]\nThis identity combines with \cref{prop:left_sketch_precond_appendix} to make for a remarkable fact.\nNamely, if $L = \range(\mtx{A})$ and $\mtx{M}$ is an orthogonalizer of a sketch $\mtx{S}\mtx{A}$, then\n\begin{equation}\label{eq:restricted_condnum_eq_cond_precond}\n    \cond(\mtx{S};L) = \cond(\mtx{A}\mtx{M}).\n\end{equation}\nLet us contextualize this fact algorithmically.\n\begin{quote}\n    If $\mtx{A}$ is an $m \times n$ matrix ($m \gg n$) in a saddle point problem, and if that problem is approached by the sketch-and-precondition methodology from \cref{subsubsec:sketch_and_precond}, then the condition number of the preconditioned matrix handed to the iterative solver is equal to the restricted condition number of $\mtx{S}$ on $\range(\mtx{A})$.\n\end{quote}\nBut we can take this one step further.\nBy invoking \cref{prop:effective_distortion} and applying \eqref{eq:restricted_condnum_eq_cond_precond}, we obtain the following expression for the effective distortion of $\mtx{S}$ for $L$:\n\begin{equation}\label{eq:eff_dist_as_convergence_rate}\n    \mathscr{D}_{\mathrm{e}}(\mtx{S};L) = \frac{\cond(\mtx{A}\mtx{M}) - 1}{\cond(\mtx{A}\mtx{M}) + 1}.\n\end{equation}\nAlarm bells should be going off in some readers' heads.\nThe right-hand-side of \eqref{eq:eff_dist_as_convergence_rate} is none other than the convergence rate of LSQR (or CGLS) for a least squares problem with data matrix $\mtx{A}\mtx{M}$!\nThis shows a deep connection between our proposed concept of effective distortion and the venerated sketch-and-precondition paradigm in \RandNLA{}.\n%Again, we contextualize this algorithmically.\n%\begin{quote}\n%     Suppose $\mtx{A}$ is an $m \times n$ matrix ($m \gg n$ matrix) appearing in an unregularized saddle point problem.\n%     If \cref{alg:saddle_to_ols_sap} is applied to this problem and uses LSQR as its iterative solver, then the convergence rate of the algorithm's iterative phase is equal to the effective distortion of the sketching operator $\mtx{S}$ for $\range(\mtx{A})$.\n%\end{quote}""",
,,
,"""\section{A high-level plan}\n\label{sec2:rblas:high-level-plan}\n\nWe begin with a simple premise.\n\begin{quote}\n    The \RandBLAS{}' defining purpose should be to facilitate implementation of high-level \RandNLA{} algorithms.\n\end{quote}\nThis premise works to reduce the \RandBLAS{}' scope, as there are ``basic'' operations in \RandNLA{} which do not support this purpose.\footnote{For example, the problem of accepting two matrices and using randomization to approximate their product is certainly basic, and it is of conceptual value \cite{dkm_matrix1}.\nHowever, it is rarely used as an explicit building block in higher-level \RandNLA{} algorithms.}\nAnother way that we reduce the scope of the \RandBLAS{} is to only consider sketching dense data matrices.\nIt may be reasonable to lift this restriction in the future, and consider methods for producing dense sketches of sparse data matrices.\n\nOur premise for the \RandBLAS{} suggests that it should be concerned with \textit{data-oblivious sketching} -- that is, sketching without consideration to the numerical properties of a dataset.\nWe identify three categories of operations on this topic:\n\begin{itemize}\n    \item sampling a random sketching operator from a prescribed distribution,  % this includes constructing index sets\n    \item applying a sampled sketching operator to a data matrix, and\n    \item sketching that is not naturally expressed as applying a single a linear operator to a data matrix.\n\end{itemize}\nThese categories are somewhat analogous to \BLASlev{1}, \BLASlev{2}, and \BLASlev{3}, insofar as their implementations admit more and more opportunities for machine-specific performance optimizations.\nAt this time, however, we do not advocate for any formalization of ``\RandBLAS{} levels.''\n\nWe note that data-oblivious sketching is not the only kind of sketching of value in \RandNLA{}.\nIndeed, \textit{data-aware} sketching operators such as those derived from power iteration are extremely important for low-rank approximation (see \cref{subsubsec:data_aware}).\nMethods for row or column sampling based on leverage scores  are also useful for kernel ridge regression and certain tensor computations; see \cref{sec7:lev_scores,sec8:tensors}. \nAlthough important, most of the functionality for producing or applying these sketching operators should be addressed in higher-level libraries.\n\nIn the material under the next two headings, we address the questions of how to handle random number generation and reproducibility in the \RandBLAS{}.""",
,"""\chapter{Least Squares and Optimization}\n\label{sec3:LS_and_optim}\n\n\minitoc\n\bigskip\n\nNumerical linear algebra is the backbone of the most widely-used algorithms for continuous optimization.\nContinuous optimization, in turn, is a workhorse for many scientific computing, machine learning, and data science applications. \n\nThe connections between optimization and linear algebra are often introduced with \emph{least squares problems}.\nSuch problems have been used as a tool for curve fitting since the days of Gauss and Legendre over 200 years ago --- several decades before Cayley even defined linear algebraic concepts such as the matrix-inverse!\nThese problems are also remarkable because algorithms for solving them easily generalize to more complicated settings.\nIndeed, one of this \nameCref{sec3:LS_and_optim}'s key messages is that, by adopting a suitable perspective, one can use randomization in essentially the same way to solve a wealth of different quadratic optimization problems.\n\nOur perspective entails describing all least squares problems in terms of an $m \times n$ data matrix $\mtx{A}$ with at least as many rows as columns.\nSpecifically, we express the overdetermined problem as \n\begin{equation*}\n\min_{\vct{x} \in \R^n}\|\mtx{A}\vct{x} - \vct{b}\|_2^2\n\label{eqn:ls-overdet}\n\end{equation*}\nfor a vector $\vct{b}$ in $\R^m$, \nwhile we express the underdetermined problem as\n\begin{equation*}\n\min_{\vct{y} \in \R^m}\{ \|\vct{y}\|_2^2 ~|~ \mtx{A}^{\trans}\vct{y} = \vct{c} \}\n\label{eqn:ls-underdet}\n\end{equation*}\nfor a vector $\vct{c}$ in $\R^n$.\nOf course, both of these models could be expressed in the corresponding ``argmin'' formulation.\nWe generally prefer the ``min'' formulation for the optimization problem itself and use ``argmin'' only for the set of optimal solutions.\n\n%The bulk of this section is covered from \cref{subsec:optim_problem_classes,subsec:optim_drivers,subsec:optim_comp_routines}.\n\cref{subsec:optim_problem_classes} introduces the problems we consider: minimization of regularized quadratics and various generalizations of least squares problems.\nFor each problem, it provides high-level comments on structures and desired outcomes that can make randomized algorithms preferable to classical ones.\n\n%We note ahead of time that most of the randomized algorithms described herein are capable of computing accurate approximations to a given problem's exact solution.\n%As part of this, we comment on the structures that enable randomized algorithms to compute accurate solutions faster than classical NLA methods.\n\cref{subsec:optim_drivers} covers the drivers for these problems based on \RandNLA{}.\nIt details the problem structures that stand to benefit from a particular driver, and it highlights other linear algebra problems that largely reduce to solving problems amenable to these drivers.\n\cref{subsec:optim_comp_routines} details some essential computational routines that would power the drivers.\n\nThe rest of the \cref{sec3:LS_and_optim} is largely supplemental.\n\cref{subsec:other_opt_algs} reviews randomized optimization algorithms that we find notable but out-of-scope, as well as one type of deterministic computational routine that is potentially useful for (but not required by) the drivers.\nWe conclude by describing existing \RandNLA{} libraries for least squares and optimization in \cref{subsec:opt_libraries}.""",
,"""\chapter{Low-rank Approximation}\n\label{sec4:lowrank}\n\n\minitoc\n\bigskip\n\n\nModern scientific computing, machine learning, and data science applications generate massive matrices that need to be processed for reduced run time, reduced storage requirements, or improved interpretability.\n\textit{Low-rank approximation} is a workhorse approach for achieving these goals.\nHere, given a target matrix $\Ao$, the task is to produce a suitably factored representation of a low-rank matrix $\Aa$ of the same dimensions which approximates the matrix $\Ao$.\n\nWe can express the main aspects of a low-rank approximation as computing factor matrices $\mtx{E}$ and $\mtx{F}$ where\n\begin{equation}\n\begin{array}{cccccccc}\n\Ao & \approx & \Aa & := & \mtx{E} & \mtx{F} \\\nm\times n &   & m\times n & & m\times k & k\times n\n\end{array} \n\end{equation}\nfor some $k \ll \min\{m, n\}$.\nWe note that it is very common to have a $k \times k$ ``inner factor'' that appears in between $\mtx{E}$ and $\mtx{F}$ above.\n\nSuch representations facilitate data interpretation by choosing the factors to have useful structure, such as having orthonormal columns or rows, or being submatrices of the target.\nThe extent of storage reduction from low-rank approximation depends on whether $\Ao$ is dense or sparse.\nIn the dense case, $\Aa$ is stored in $O(mk + nk)$ space.\nIn the sparse case, one representation consists of a dense $k \times k$ inner factor, a slice of $k$ rows of $\Ao$, and a slice of $k$ columns of $\Ao$.\n\nThe rank $k$ used in a low-rank approximation is a tuning-parameter that the user can control to trade-off between approximation accuracy and data compression.\nThe best choice of this parameter depends on context. \nFor instance, one may want to choose $k$ small enough to graphically visualize coherent structure in the target.\nIn such a setting one would not expect that $\Aa$ is close to $\Ao$ in an absolute sense, but one can still ask that the distance is near the minimum among all approximations with the desired structure and rank.\nAlternatively, one might know that $\Ao$ can be well-approximated by a low-rank matrix, and yet not know the rank necessary to achieve a good approximation.\nSuch matrices are called \textit{numerically low-rank} and arise in applications across the social, physical, biological, and ecological sciences. \nFor example, they can arise as discretizations of differential operators, where the extent to which the matrix is numerically low-rank depends on the details of the operator and the discretization; and they can arise as noisy corruptions of general (hypothesized) data matrices with low exact rank.\nWhen dealing with such matrices one can iteratively build $\Aa$ until a desired distance $\|\Ao - \Aa\|$ is small.\nThis \nameCref{sec4:lowrank} covers a variety of efficient and reliable low-rank approximation algorithms for both of these scenarios.""",
,"""\section{Sketching operators}\n\label{subsec:Sketching-of-Kronecker-and-Khatri--Rao}\n\n\cref{subsubsec:Row-structured-sketches} introduces sketching operators that are distinguished by having rows with particular structures.\n\cref{subsubsec:Kronecker_SRFTs} discusses a variant of the SRFT with an additional tensor-produce structure.\n\cref{subsubsec:TensorSketch} discusses TensorSketch operators, which are analogous to CountSketch operators from \cref{subsubsec:randblas:ShortAxSparse}.\nIn \cref{subsubsec:Recursive_sketch} we describe sketching operators that are recursive and have multi-stage structure. \nThese incorporate some of the sketching operators discussed in the previous subsections as stepping stones.\n\cref{subsubsec:tensor_product_lev_scores} covers row sampling methods for tall matrices with tensor product structure.\n\nWe note that the sketching operators in Sections~\ref{subsubsec:Row-structured-sketches}--\ref{subsubsec:Recursive_sketch} are all oblivious, whereas the sampling-based methods in \cref{subsubsec:tensor_product_lev_scores} are not.\nWe also note that all of the oblivious sketching operators we discuss could be applied to \textit{unstructured} matrices.\nThis would yield no speed benefit compared to using their unstructured counterparts, but it would reduce the storage requirement compared to traditional dense sketching operators of the kind supported by the \RandBLAS{}.""",
,,
"Introduction\nThe computation of various cobordism rings was one of the main directions of research in homotopy theory in the 1960s. The reason why it is connected to algebraic topology is the link between cobordism theories and Thom spectra, known as the Pontryagin--Thom construction. In modern terms, it states that the cobordism theory $\\Omega^G_*$ as a generalized homology theory is isomorphic to the generalized homology theory represented by the Thom spectrum $\\mathrm{M}G$. Therefore, the computation of the respective cobordism ring is equivalent to the study of the homotopy groups of a spectrum, which is a problem of stable homotopy theory.\n\nThe simplest of cobordism theories, unoriented cobordism, was the subject of Thom's seminal paper \\cite{Thom}, who completely calculated the ring $\\pi_*(\\mathrm{MO})$. In the complex case, Milnor \\cite{Mil60} and Novikov \\cite{Nov0} obtain a complete description of the unitary cobordism ring $\\pi_*(\\MU)$, and later on, Quillen  prove that this ring is isomorphic to the coefficient ring of the universal formal group law \\cite{Quillen}. These results led to the emergence of the Adams--Novikov spectral sequence and the chromatic point of view, which have contributed immensely to the study of the stable  homotopy category, see \\cite{Ra04}. The description of the oriented cobordism ring $\\pi_*(\\mathrm{MSO})$ was treated by Novikov \\cite{Nov0, Nov} (the ring structure modulo\ntorsion) and by Wall \\cite{Wall} (completely). For this purpose Wall introduces the cobordism theory of manifolds with $\\RP^1$-reduction, which sits in between oriented and unoriented cobordism theories. In \\cite{CF} Conner and Floyd used this idea in the unitary context to introduce cobordism theory of complex manifolds with $\\CP^1$-reduction $\\pi_*(\\W)$. As an application, they compute cobordism ring of manifolds with a stable special unitary structure $\\pi_*(\\MSU)$ (see also \\cite{CLP}, \\cite{CP23} for a more modern exposition). The latter two constructions are the main topological insights for this paper. All of the above calculations are written uniformly in Stong's book \\cite{Sto}.\n\nIn the setting of motivic homotopy theory, the Thom spectrum $\\MGL$ was introduced by Voevodsky in his ICM address \\cite{Voe98}. This spectrum is the universal oriented commutative ring spectrum \\cite{PPR08}, where ``oriented'' means that it possesses Thom classes for vector bundles. In the same paper with the definition, Voevodsky proposes a conjecture that the $\\Proj^1$-diagonal of the coefficient ring of $\\MGL$ over a regular local ring should be isomorphic to the coefficient ring of the universal formal group law. This motivic version of the Quillen theorem was proved over fields of characteristic zero by Hopkins and Morel (unpublished) and over fields away from the characteristic by Hoyois \\cite{Hoy15}. The result of Hoyois was further generalized by Spitzweck to Noetherian local rings which are regular over discrete valuation rings \\cite{SpiMGL} (away from the characteristic of the residue field). These computations lead to a large number of applications in motivic homotopy theory, see e.g. \\cite{Chowt,RSO19,RSO24}. In addition, the construction of this spectrum serves as an inspiration for the Levine--Morel algebraic cobordism theory \\cite{LevMor}, which is extensively studied now and has many applications to the problems of algebra and algebraic geometry \\cite{LevPand,Vishik,SS21}. The connection between $\\MGL$ and Levine--Morel algebraic cobordism $\\Omega^*$ over fields of characteristic zero can be viewed as the motivic version of the Pontryagin--Thom theorem \\cite{Levine}. However, this comparison is actually a posteriori, since the only known proof uses computations of the corresponding coefficient rings.\n\nThe story continues with the definition of the special linear and symplectic motivic Thom spectra due to Panin and Walter \\cite{PW22}. Similarly to Voevodsky's algebraic cobordism, these spectra are universal ring spectra among those admitting Thom classes for oriented (or symplectic) vector bundles. An important difference between the motivic situation and the topological one, that such weakly oriented spectra play more important role here (even rationally); see \\cite{ALP, DFJK}. Also, we should note that the spectra $\\MSL$ and $\\MSp$ are ``closer'' to the motivic sphere spectrum $\\sph$ than $\\MGL$. Although the definition of these Thom spectra is standard, little is known about them at the moment. The goal of this work is to present a computation of the $\\Proj^1$-diagonal of the coefficient ring of the special linear algebraic cobordism spectrum $\\MSL$ over some bases. This computation can be viewed as an $\\SL$-analogue of the Voevodsky conjecture. Nevertheless, unlike in the case of $\\GL$-cobordism, the answer for $\\MSL$ depends on the base even for fields, as can be seen from Yakerson's description of the zero homotopy module \\cite{Yakerson}.",,"Conclusion\nIn this paper, we have presented a detailed computation of the $\\Proj^1$-diagonal of the coefficient ring of the special linear algebraic cobordism spectrum $\\MSL$ over local Dedekind domains. Our results generalize previous work by Bachmann and Hopkins, and provide a complete description of the geometric part of $\\MSL$ after inverting the exponential characteristic of the residue field. The key technical tool in our analysis was the construction of the $c_1$-spherical algebraic cobordism spectrum $\\MW$, which allowed us to establish cofiber sequences connecting $\\MSL$, $\\MW$, and $\\MGL$. These cofiber sequences, combined with the motivic version of the Conner--Floyd homology and Pontryagin characteristic numbers, enabled us to compute the homotopy groups of $\\MSL$ and understand their structure.\n\nOur main results include the computation of the $\\eta$-periodic part of $\\MSL$, the determination of the $2$-primary torsion subgroup, and the complete additive structure of the geometric diagonal. We also proved a motivic version of the Anderson--Brown--Peterson theorem, which states that the geometric diagonal of $\\MSL$ is determined by the $\\HZ$-characteristic numbers and the $\\KQ$-Pontryagin characteristic numbers. Furthermore, we showed that the ring $\\pi_{2*,*}(\\MSL)/\\mathrm{I}_\\MSL(k)[1/2e]$ is generated by classes of smooth projective Calabi--Yau varieties under the assumption that the base field is infinite.\n\nThese results open up several directions for future research. One natural question is whether the Voevodsky conjecture about the $\\Proj^1$-diagonal of $\\MGL$ can be proved integrally, which would allow us to extend our results without inverting the exponential characteristic. Another interesting direction is to explore the relationship between the motivic and topological computations further, particularly in the context of the $2$-primary torsion and the role of the motivic Hopf element $\\eta$. Finally, it would be valuable to investigate the implications of our results for other motivic spectra and their applications in algebraic geometry and number theory."
"\\section{Introduction}\\label{intro}\nThroughout the paper{,} $H$ is a complex Hilbert space{,} and $B(H)$ denotes the von Neumann algebra of bounded linear operators on $H$.  \n%When an operator algebraist studies a $^*$-subalgebra of $B(H)$, it is usually assumed to be  \\emph{closed}. \n%Indeed, the vast majority of self-adjoint operator algebraists investigate either C$^*$- or von Neumann algebras, both of which are closed $^*$-subalgebras of some $B(H)$. \n%What if we consider a not necessarily closed $^*$-subalgebra of $B(H)$? \n%It is hard to imagine that there exists a nice structural result that is valid for general $^*$-subalgebras of $B(H)$ (except for the trivial fact that the completion of a $^*$-subalgebra of $B(H)$ is a C$^*$-algebra). \n%Nevertheless, in studying operator algebras, we often encounter a situation where the assumption of closedness is not essential. \n%In this paper, we study a certain class of not necessarily closed $^*$-subalgebras of $B(H)$, where somehow rich examples exist but still we may get nice structural theorems under suitable assumptions. \n%\nA motivation for our research comes from a problem raised by Kadison. \nIn the celebrated conference in Baton Rouge in 1967, Kadison proposed a list of 20 problems on operator algebras. \n%Although the original list was not published, \nThe list was shared among operator algebraists worldwide, which brought a great influence to the development of the theory of operator algebras over the last half-century. \nIn 2003, Ge published a survey article \\cite{Ge} on Kadison's list of problems.\nAmong the 20 problems, 19 are about C$^*$- or von Neumann algebras. \nThe only exception is listed as the 15th problem in Ge's article, which asks the following.\n\\begin{problem}[Kadison]\\label{kadison}\nIf a self-adjoint operator algebra is finitely generated (algebraically) and each self-adjoint operator in it has finite spectrum, is it finite-dimensional?\n\\end{problem}\nIn the recent paper \\cite{ST}, Sz\\H{u}cs and Tak\\'acs discovered that for a $^*$-subalgebra $A$ of $B(H)$ several conditions are equivalent to the condition every self-adjoint element of $A$ has finite spectrum. \nIn this paper, we first observe in Theorem \\ref{equi} that these conditions are also equivalent to the condition $A$ is regular in the sense of von Neumann. \nWe call a $^*$-subalgebra of $B(H)$ that satisfies these equivalent conditions an R$^*$-algebra (``R'' meaning ``regular'').\nThe abundance of the conditions for a $^*$-subalgebra of $B(H)$ to be an R$^*$-algebra implies that R$^*$-algebras form more or less an important class of $^*$-algebras. \n\nEven though we assume no completeness, it turns out that R$^*$-algebras share some important properties with C$^*$-/von Neumann algebras. \nFor example, we may freely use the technique of functional calculus.\nRecall that there exists a correspondence between commutative C$^*$-algebras and locally compact Hausdorff spaces (Gelfand--Naimark theorem), and similarly, between commutative von Neumann algebras and certain measure spaces.\nIs there such a correspondence for R$^*$-algebras?\nBy von Neumann's study of $^*$-regular rings in \\cite[Part II]{N}, projections of a general R$^*$-algebra form a lattice.\nIt follows that the lattice of projections of a commutative R$^*$-algebra is a generalized Boolean algebra.\nIn fact, there exists a complete correspondence between the class of commutative R$^*$-algebras and that of generalized Boolean algebras. \n%In fact, we may verify this by straightforward reasoning.\nBy restricting ourselves to the unital case, we see that unital commutative R$^*$-algebras correspond to Boolean algebras. \nIn that sense (the projection lattice of) an R$^*$-algebra can be considered as a noncommutative (generalized) Boolean algebra. \nThis correspondence is closely tied with Stone's theorem on the duality between Boolean algebras and Boolean spaces. \n\n%Thus we are endowed with a strong motivation to study general R$^*$-algebras and their lattices of projections.\n%Since there are many Boolean algebras, it follows that the class of commutative R$^*$-algebras is quite large. \nAre there plenty of noncommutative R$^*$-algebras?\nThe answer is yes. \nThe class of ultramatricial R$^*$-algebras, that is, directed unions of finite-dimensional C$^*$-algebras, provides us with many examples of noncommutative R$^*$-algebras. \nIt turns out that Problem \\ref{kadison}, which remains to be open, is equivalent to asking whether every R$^*$-algebra is ultramatricial. \n%Observe that every commutative R$^*$-algebra can be considered to belong to this class.\nUltramatricial R$^*$-algebras with countable Hamel dimension are highly relevant to the theory of separable AF C$^*$-algebras, whose classification by Elliott \\cite{E} is one of the best{-}known results of the theory of C$^*$-algebras. \n%We clarify the relation between AF C$^*$-algebras and ultramatricial R$^*$-algebras.\nAnother subclass of ultramatricial R$^*$-algebras, which we call purely atomic R$^*$-algebras, is constructed from inner product spaces. \nWe give a complete classification of purely atomic R$^*$-algebras in terms of linear isometries of inner product spaces.\nBy applying Banach space theory and results on operator ranges, we prove that there are many mutually nonisomorphic purely atomic R$^*$-algebras. \n\nThe literature of regular rings, Boolean algebras, and inner product spaces in connection with Banach spaces, as well as C$^*$-/von Neumann algebras provides us with many deep results and further questions. \nThe paper intends to collect some of them in order to give supporting {evidence} for the author's belief that the study of R$^*$-algebras is interesting enough to be considered in more depth. \nThis paper should give much motivation to the research on Problem \\ref{kadison}, which seems to have gotten less attention among operator algebraists compared to other problems in the list \\cite{Ge}. \nThe choice of materials in this paper is more or less biased by the author's preference.\n\nThis paper is formulated in the following manner. \nIn the next section{,} we give the definition and basic properties of R$^*$-algebras. \nIn particular, we characterize R$^*$-algebras by closedness of the ranges of operators, the uniqueness of C$^*$-norm of $^*$-subalgebras as well as von Neumann regularity (Theorems \\ref{equi}, \\ref{c*norm}).\nSection \\ref{example} provides examples of R$^*$-algebras and discusses Problem \\ref{kadison}. \nThe correspondence between commutative R$^*$-algebras and Boolean algebras is given in Section \\ref{com}. \nSection \\ref{atom} is devoted to giving the relation between purely atomic R$^*$-algebras and inner product spaces. \nSome results that arise by comparing the projection lattice of an R$^*$-algebra with that of a von Neumann algebra or a Baer $^*$-ring are given in Section \\ref{comparison}.\nThe consequence of Elliott's classification theorem of AF C$^*$-algebras is summarized in Section \\ref{countable}. \nWe also revisit Elliott's theorem from a lattice theoretic viewpoint (Theorem \\ref{sumlattice}).\nIn Section \\ref{nd} we collect results that derive from von Neumann, Dye and Gleason's theorems. \nWe close this paper by proposing questions on R$^*$-algebras for further research (Section \\ref{question}). \n\nIn the paper{,} we frequently use basic results on C$^*$-/von Neumann algebras as in \\cite{KR1,KR2,Ta}, and von Neumann's theory on complemented modular lattices and regular rings as in (the first four chapters of) \\cite[Part II]{N}. \nNone of the proofs is dependent on any results by Sz\\H{u}cs and Tak\\'acs in \\cite{ST}, although the underlying idea may have some overlap implicitly.",,
"Introduction: Let $G$ be a group and $\\varphi$ an automorphism of $G$. The \\emph{$\\varphi$-twisted conjugacy} action of $G$ on itself is defined as the map $G\\times G\\rightarrow G$ given by $(g,x)\\mapsto gx\\varphi(g^{-1})$, for all $g,x\\in G$. Let $\\mathcal{R}(\\varphi)$  be the set of all orbits of this action and $R(\\varphi)$ the cardinality of $\\mathcal{R}(\\varphi)$. An orbit $[x]_\\varphi$ ($x\\in G$) under the twisted action is also called the \\emph{Reidemeister class} of $x$. The reason for this nomenclature is probably because the study of such actions can be traced back to the Nielsen-Reidemeister fixed point theory (c.f. \\cite{jiang}). In what follows, $R(\\varphi)=\\infty$ (respectively, $R(\\varphi)<\\infty$) will mean that the set $\\mathcal{R}(\\varphi)$ is infinite (respectively, finite).\n\nA group $G$ is said to have the \\emph{$R_\\infty$-property} if $R(\\varphi)=\\infty$ for every automorphism  $\\varphi$ of $G$. The study of groups with this property has its origin in \\cite{fh94}. The reader may refer to \\cite{ft} for an overview and more literature.  Some recent works in this direction include \\cite{bdr}, \\cite{ms2020}, \\cite{timur19}, and  \\cite{gsw21}, where the $R_\\infty$-property has been studied for twisted Chevalley groups, for the general and special linear groups over certain subrings of  $\\overline{\\mathbb{F}_p}(t)$, for unitriangular groups over an integral domain, and for fundamental groups of geometric $3$-manifolds, respectively. In the realm of linear algebraic groups, an early instance of considering the notion of twisted conjugacy appears in \\cite{gant}. The reader is urged to look at \\cite{St}, \\cite{mohr03}, \\cite{mw04}, and \\cite{springer} for a host of interesting results. \n\nA linear algebraic group $G$ over an algebraically closed field, is said to have the \\emph{algebraic $R_\\infty$-property} if $R(\\varphi)=\\infty$ for every algebraic group automorphism $\\varphi$ of $G$. In the sequel an algebraic group will always mean a linear algebraic group over an algebraically closed field, an automorphism $\\varphi$ of an algebraic group $G$, will mean an abstract automorphism such that $\\varphi$ and $\\varphi^{-1}$ are morphisms of the underlying affine variety of the group, and the group of all such automorphisms will be denoted by $\\mathrm{Aut}_{\\mathrm{alg}}(G)$. In a previous paper \\cite[Corollary 18]{bb} it has been shown that if $G$ is an algebraic group such that its connected component $G^\\circ$ is non-solvable, then $G$ has the algebraic $R_\\infty$-property. The aim of the present paper is to study this property for solvable algebraic groups.","""Methodology: In Section \ref{sg} we show that if $G$ is a connected solvable algebraic group which admits an automorphism $\varphi$ such that $R(\varphi)<\infty$, then the $\varphi$-twisted action is necessarily transitive (Theorem \ref{solv}). From a theorem due to Steinberg \cite[Theorem 10.1]{St} it follows that if a connected algebraic group $G$ has the algebraic $R_\infty$-property, then the fixed-point subgroup $G^\varphi$ is infinite for every automorphism $\varphi$ of $G$. We deduce that the condition is also sufficient (Theorem \ref{iff}). We also prove that if $G$ is a Borel subgroup of a semisimple algebraic group, then it has the algebraic $R_\infty$-property (Theorem \ref{borel}).\n\nA  unipotent algebraic group of Chevalley type is defined as the unipotent radical of a Borel subgroup of a simple algebraic group (equivalently, a maximal connected unipotent subgroup of a simple algebraic group). Let $G$ be such a group and assume that the characteristic of the base field is different from $2$ and $3$. From the works of Fauntleroy \cite{faunt} and Gibbs \cite{gibbs} one obtains a description of all  automorphisms of $G$. We derive a necessary and sufficient condition for an automorphism $\varphi$ of $G$, for which  $R(\varphi)=1$ (Theorem \ref{maxunipotent}).\n\nIn Section \ref{ex} we compute $R(\varphi)$ for certain automorphisms $\varphi$ of some solvable algebraic groups. We observe that tori, groups of the form $\mathbb{G}_a^n$ and the $n$-dimensional Witt groups fail to have the algebraic\n$R_\infty$-property for all $n\geq 1$. A connected nilpotent algebraic group has the algebraic $R_\infty$-property if and only if its unipotent radical has this property. Example \eqref{ex3} describes two distinct semidirect products of  $\mathbb{G}_m^n$ ($n\geq 1$) and $\mathbb{G}_a^r$ ($r\geq 2$) such that one of them has the algebraic $R_\infty$-property, while the other does not.\n\nIt has been shown in \cite{timurjaa, nas20} that if $k$ is an algebraically closed field of infinite transcendence degree over $\mathbb{Q}$, and $G$ is one of the groups $\mathrm{GL}_n(k)$, $\mathrm{SO}_n(k)$ or $\mathrm{Sp}_n(k)$, then there exists an abstract automorphism $\varphi$ of $G$ (induced by a non-trivial automorphism of $k$) such that $R(\varphi)=1$. The proof of this result has been carried out on a case by case basis. Therefore it is desirable to have an argument which may possibly work for any reductive algebraic group. This consideration forms a part of our ongoing work. However, it turns out that the proof of \cite[Theorem 6]{timurjaa} can be modified to show that if $k$ is an algebraically closed field of countable transcendence degree over $\mathbb{Q}$, then a Borel subgroup of any simple algebraic group over $k$, admits an abstract automorphism $\varphi$ such that $R(\varphi)=1$ (Theorem \ref{abstbor}).""",Conclusion: Let $k$ be an algebraically closed field of countable transcendence degree over $\\mathbb{Q}$ and $B$ a Borel subgroup of a simple algebraic group over $k$. Then there exists an abstract automorphism of $B$ such that the associated twisted conjugacy action of $B$ on itself is transitive.
"\\section{Introduction}\\label{S: Introduction}\n\nIn Section~\\ref{S: Infinite-Dimensional Linear Algebra}-\\ref{S: Linear Maps and Operators} we present the basic results of  \\emph{infinite-dimensional linear algebra}, an old branch of mathematics initiated in1905 by Georg Hamel~\\cite{gHamel}, dealing with infinite-dimensional vector spaces in terms of algebraic (Hamel) bases rather than topological or orthonormal Hilbert bases. The approach is mostly algebraic. In Theorem~\\ref{T: Surjective Dual} we show that a linear operator is injective if and only if its dual operator is surjective; a result well-known for finite-dimensional vector spaces but less-known for infinite-dimensional spaces. This gives rise to the Definition~\\ref{D: Regular Operators} of a \\emph{regular linear operator} - a surjective operator on the dual space with injective co-dual. \n\nSeveral discussions of the earlier versions of this text convinced us that \nthe algebraic (Hamel) bases have gradually been falling out of popularity in the last several decades. That is why the first part of the article (Section~\\ref{S: Infinite-Dimensional Linear Algebra}-\\ref{S: Linear Maps and Operators}) is written in somewhat tutorial manner, with many illustrative examples (Section~\\ref{S: Examples of Infinite-Dimensional Spaces}). A reader who knows Theorem~\\ref{T: Surjective Dual} from the finite-dimensional linear algebra and who believes in its validity for  infinite-dimensional vector spaces might skip reading the first several sections and start directly from Section~\\ref{S: Linear Functionals in D*(Omega) as Generalized Functions: The Main Result}. \n\nIn Sections~\\ref{S: Linear Functionals in D*(Omega) as Generalized Functions: The Main Result} we apply infinite-dimensional linear algebra to the particular case of the vector space $\\mathcal D(\\Omega)$ and its algebraic dual $\\mathcal D^*(\\Omega)$. Here $\\Omega$ is an open set of $\\R^d$ in the \\emph{usual topology of} $\\R^d$. Somewhere in this section we abandon the realm of algebra and start involving concepts and methods from functional analysis and the theory of partial differential operators (H\\\",,
,"""\section{Optimal projections: skew-symmetric operators}\n\label{sec:optmal_skew_sym}\nWe now turn our attention to a more general class of problems involving skew-symmetric operators with the problem stated as\n\begin{gather}\n    \mathcal{C}\phi + \mathcal{L} \phi = f, \quad \text{in } \Omega \label{eq:pde_non_sym} \\\n    \phi = g, \quad \text{on } \Gamma_d \\\n    \nabla \phi \cdot \hat{n} = h, \quad \text{on } \Gamma_n \\\n    \text{with } \Gamma_d \cup \Gamma_n = \partial \Omega,\n\end{gather} \nwith $\mathcal{C}$ being a \emph{linear} skew-symmetric operator (an advection term for example) and $\mathcal{L}$ being a symmetric operator. We still work in function spaces dictated by the symmetric operator, i.e. a Hilbert space $V$ defined in $\Omega$ where we seek $\phi \in V$ at the continuous level and $\bar{\phi} \in \bar{V}$ for the discrete setting with the optimal projector $\mathcal{P} : V \xrightarrow{} \bar{V}$. However, unlike the case of the symmetric problem, it is not possible to derive a weak form of \eqref{eq:pde_non_sym} from an energy minimisation principle unless we move to a Lagrangian formulation where the skew-symmetric part vanishes. We choose to stick to an Eulerian formulation, hence, we derive a weak form of \eqref{eq:pde_non_sym} by testing it with a test function $v \in V$ which yields\n\begin{gather}\n    a(v, \phi) + c(v, \phi) = (v, f)_{L^2(\Omega)} + b(v, \{g, h\}), \quad \forall v \in V.\n    \label{eq:weak_non_symm}\n\end{gather}\nThe resulting weak form is similar to \eqref{eq:weak_bilinear} with the addition of the bilinear form $c(v, \phi)$ emerging from the skew-symmetric term. The inability to derive the weak form from an energy minimisation principle also implies that we cannot directly derive a finite-dimensional variational form of \eqref{eq:weak_non_symm} using the notion of an optimal projector as we had demonstrated for the symmetric case. In short, this means that if we were to directly derive a finite-dimensional system from \eqref{eq:weak_non_symm} by restricting the test and trial space to $\bar{V}$ (classic Galerkin formulation), the discrete solution we would obtain by solving the system would \emph{not} be the optimal projection of the exact solution! \n\nIn order to ensure we get the optimal projection from our discretisation using \eqref{eq:weak_non_symm}, we employ Variational Multiscale (VMS) analysis wherein the unresolved components of the solution are explicitly accounted for in the variational equation. We know that invoking the optimal projector associated with the symmetric operator yields the following\n\begin{gather}\n    a(v^h, \bar{\phi}) = a(v^h, \phi).\n\end{gather}\nThe projector allows us to separate the resolved ($\bar{\phi}$) and unresolved (fine) scales ($\phi^{\prime}$) where the continuous $\phi$ can be uniquely expressed as \n\begin{gather}\n    \phi := \bar{\phi} + \phi^{\prime}, \quad \bar{\phi} \in \bar{V}, \: \phi^{\prime} \in V^{\prime}, \label{eq:phi_split} \n\end{gather}\nand further implies orthogonality between the resolved and unresolved (fine) scales\n\begin{gather}\n    a(\bar{\phi}, \phi^{\prime}) = 0. \label{eq:orth_subscale}\n\end{gather}\nIf we substitute the split form from \eqref{eq:phi_split} into \eqref{eq:weak_non_symm} we get\n\begin{gather}\n    a(v, \bar{\phi}) + a(v, \phi^{\prime}) + c(v, \bar{\phi}) + c(v, \phi^{\prime}) = (v, f)_{L^2(\Omega)} + b(v, \{g, h\}), \quad \forall v \in V.\n\end{gather}\nIf we restrict the test space to $\bar{V}$, we get\n\begin{gather}\n    a(v^h, \bar{\phi}) + \cancel{a(v^h, \phi^{\prime})} + c(v^h, \bar{\phi}) + c(v^h, \phi^{\prime}) = (v^h, f)_{L^2(\Omega)} + b(v^h, \{g, h\}), \quad \forall v^h \in \bar{V} \\\n    a(v^h, \bar{\phi}) + c(v^h, \bar{\phi}) + c(v^h, \phi^{\prime}) = (v^h, f)_{L^2(\Omega)} + b(v^h, \{g, h\}), \quad \forall v^h \in \bar{V}\n\end{gather}\nwhere the term $a(v^h, \phi^{\prime}) = 0$ due to the condition in \eqref{eq:orth_subscale}. We thus find that we need information on the fine-scales $\phi^{\prime}$ to find the optimal solution $\bar{\phi}$. Naturally, $\phi^{\prime}$ only appears in the bilinear form associated with the skew-symmetric operator. In the absence of the skew-symmetric term we have the formulation to find the optimal solution of the symmetric problem that was previously discussed. \n\nThe unresolved scales $\phi^{\prime}$ are naturally unknowns and thus must be solved simultaneously with $\bar{\phi}$. We employ the Fine-Scale Greens' function established in \cite{Shrestha2024ConstructionScales} to compute $\phi^{\prime}$ and its gradients. An important point to note is that we only use Greens' function of the \emph{symmetric problem} and not the full skew-symmetric problem. The latter case could be argued as `cheating' as the Greens' function of the full PDE would directly yield the exact solution to the original problem. Additionally, finding the Greens' function for the full PDE is as difficult as solving the original problem, which would thus deem the VMS approach obsolete. Hence, we proceed with the Greens' function of only the symmetric operator which overcomes the aforementioned flaws/shortcomings. We express the fine-scale problem in a generic setting by substituting the split form from \eqref{eq:phi_split} into the original PDE in \eqref{eq:pde_non_sym} and constraining it as follows\n\begin{gather}\n    \mathcal{L} \phi' + \mathcal{P}^T \bar{\lambda} = f - \mathcal{L} \bar{\phi} - (\mathcal{C} \bar{\phi} + \mathcal{C} \phi^{\prime}) \label{eq:fine_scale_con0}\\\n    \mathcal{P} \phi' = 0, \label{eq:fine_scale_con1}\n\end{gather}\nwhich is the same format used in \cite{Shrestha2024ConstructionScales, Hughes2007VariationalMethods} where $\bar{\lambda} \in \bar{V}$ is the Lagrange multiplier used to constraint $\phi^{\prime}$ to $V'$. Given that $\mathcal{L}$ in invertible ($\mathcal{L}^{-1} = \mathcal{G}$ i.e. the Greens function), we may write\n\begin{gather}\n    \phi' = \mathcal{G}(r - \mathcal{P}^T \bar{\lambda}), \quad \text{with } r := f - \mathcal{L} \bar{\phi} - (\mathcal{C} \bar{\phi} + \mathcal{C} \phi^{\prime}).\n\end{gather}\nWhen substituted into \eqref{eq:fine_scale_con1} and rearranged for $\bar{\lambda}$, we get\n\begin{gather}\n    \bar{\lambda} = \left(\mathcal{P} \mathcal{G} \mathcal{P}^T\right)^{-1} \mathcal{P} \mathcal{G} r.\n\end{gather}\nHence, we have \n\begin{gather}\n    \phi' = (\mathcal{G} - \mathcal{GP}^T (\mathcal{PGP}^T)^{-1}\mathcal{PG}) r = \mathcal{G}^{\prime} r,\n\end{gather}\nwhere $\mathcal{G}^{\prime}$ is the Fine-Scale Greens' function, see \cite{Shrestha2024ConstructionScales, Hughes2007VariationalMethods} for more details. Expanding the full expression for $r$ gives\n\begin{gather}\n    \phi' = \mathcal{G}^{\prime}f - \cancel{\mathcal{G}^{\prime}\mathcal{L} \bar{\phi}} - \mathcal{G}^{\prime}\mathcal{C} \bar{\phi} - \mathcal{G}^{\prime} \mathcal{C} \phi^{\prime}) = \mathcal{G}^{\prime}(\mathscr{R}\bar{\phi} - \mathcal{C} \phi^{\prime}),\n    \label{eq:phi_prime}\n\end{gather}\nwhere we define $\mathscr{R}\bar{\phi} := f - \mathcal{C} \bar{\phi}$ as the coarse scale residual with $\mathcal{L} \bar{\phi}$ being eliminated as it lives exactly in the finite-dimensional dual space $\bar{V}^*$ which is orthogonal to $\mathcal{G}^{\prime}$ given the property $\mathcal{G}^{\prime} \mathcal{P}^T = 0$. With a closed form expression for $\phi^{\prime}$ we now have a coupled system we can solve to obtain $\bar{\phi}$, namely\n\begin{gather}\n    a(v^h, \bar{\phi}) + c(v^h, \bar{\phi}) + c(v^h, \phi^{\prime}) = (v^h, f)_{L^2(\Omega)} + b(v^h, \{g, h\}), \quad \forall v^h \in \bar{V} \\\n    \phi' = \mathcal{G}^{\prime}(\mathscr{R}\bar{\phi} - \mathcal{C} \phi^{\prime}). \label{eq:phi_prime_2}\n\end{gather}\nHowever, due to the fact that $\phi^{\prime}$ appears on both sides of \eqref{eq:phi_prime_2} we cannot directly solve the coupled system for an arbitrary operator $\mathcal{C}$. \n% In such cases, we have to opt for an iterative approach where we start with an initial guess for $\bar{\phi}$ and $\phi^{\prime}$ and iterate until the solutions have converged. \nHowever, we can simplify \eqref{eq:phi_prime_2} using the linearity of $\mathcal{C}$. We first realise that the bilinear form $c(\cdot, \cdot)$ is given by\n\begin{gather}\n    c(\alpha, \beta) := (\alpha, \mathcal{C}\beta)_{L^2(\Omega)},\n\end{gather}\nhence we are interested in finding $\mathcal{C}\phi^{\prime}$ which we can compute by simply applying $\mathcal{C}$ to \eqref{eq:phi_prime_2}. \n\begin{gather}\n    \mathcal{C}\phi' = \mathcal{C}\mathcal{G}^{\prime}(\mathscr{R}\bar{\phi} - \mathcal{C} \phi^{\prime}).\n\end{gather}\nWe can then rearrange the equation to get\n\begin{gather}\n    \mathcal{C} \phi' + \mathcal{C} \mathcal{G}^{\prime} \mathcal{C} \phi^{\prime} = \mathcal{C} \mathcal{G}^{\prime}\mathscr{R}\bar{\phi} \\\n    (\mathbb{I} + \mathcal{C} \mathcal{G}^{\prime}) \mathcal{C} \phi^{\prime} = \mathcal{C} \mathcal{G}^{\prime}\mathscr{R}\bar{\phi} \\\n    \mathcal{C} \phi^{\prime} = (\mathbb{I} + \mathcal{C} \mathcal{G}^{\prime})^{-1} \mathcal{C} \mathcal{G}^{\prime}\mathscr{R}\bar{\phi} \\\n    \mathcal{C} \phi^{\prime} = \sigma^{\mathcal{C}}_{SG} \mathscr{R}\bar{\phi},\n\end{gather}\nwhere we have defined a new operator $\sigma^{\mathcal{C}}_{SG}$ which we refer to as the Suyash-Greens' operator. A common theme in the literature on the algebraic VMS approach is to express the unresolved scales as $\phi^{\prime} = \tau \mathscr{R}\bar{\phi}$ where $\tau$ is some parameter determined through some scaling arguments and is usually based on some a-priori expected/desired behaviour of the fine scales, see \cite{ShakibFarzin1989FiniteEquations, Bazilevs2007VariationalFlows, Brezzi1997BG, Holmen2004SensitivityFlow, tenEikelder2018CorrectContext, Koobus2004AShedding} for examples. However, the Suyash-Greens' operator is derived directly from the variational form of the PDE and the Fine-Scale Greens' function of the symmetric operator, hence it does not rely on any ad-hoc derivation.""",
,,
,,
"\\section{Introduction}\\label{sect:introduction}\nSingularly perturbed differential and evolution equations have been analyzed for many decades already (see~\\cite{KadP03} for a review) and cover the entire spectrum of elliptic, parabolic, %~\\cite{Fri68}, \nas well as hyperbolic systems. In this paper, we are concerned with the asymptotics of hyperbolic--parabolic singular systems, i.e., hyperbolic systems with a parabolic limit case. These systems are well-studied in the unconstrained case: In~\\cite{Lio73,GhiG06}, linear and rooty convergence rates for the variables of the linear damped wave equation are proven. \nLater, these results were extended to nonlinear systems \\cite{EshW88,Esh90,HasY07,GhiG12} as well as integral expressions~\\cite{Esh87b,LiaLX05}. \nResulting estimates can be used, e.g., in the theory of boundary layers or as a tool for the design of numerical algorithms~\\cite{Esh87}, which also serves as motivation for the present work. \n\n\n%% PDAEs\nFor the particular case of one-dimensional gas networks, i.e., a coupled system of hyperbolic equations, perturbation results have been derived in~\\cite{EggK18}. In general, such network structures can be modeled as constrained partial differential equations, where the constraints are naturally given by the junctions within the network, reflecting fundamental physical properties. Such an approach leads to {\\em partial differential--algebraic equations} (PDAEs), cf.~\\cite{EmmM13,LamMT13,Alt15}, which may be interpreted as differential--algebraic equations in Banach spaces. For the sake of completeness, we would like to mention that one may also consider the network as a domain on which the differential equations are stated~\\cite{Mug14}. In this approach, however, inhomogeneous boundary conditions still account for constraints on the solution. \n\n%% hyperbolic PDAEs\nIn this paper, we focus on singularly perturbed (linear) PDAEs of first order (in time) that are hyperbolic, i.e., we consider hyperbolic partial differential equations including a small parameter~$\\eps$, which underlie an additional constraint. \n% eps\nThe singular perturbation of the considered PDAEs is characterized through the property that the system is of hyperbolic nature for~$\\eps>0$ and parabolic in the limit case~$\\eps=0$.\n% examples\nThroughout the paper, we present three examples in more detail. \n%\n\\begin{example}[Damped acoustic wave equation]\\label{exp:wave}\nConsider the damped wave equation~$\\ddot u + d\\dot u - c^2\\Delta u = 0$ with the damping parameter~$d$ being proportional to the square of the speed of sound~$c$.  \nSetting $\\eps = 1/c^2$, we can rewrite this as \n%\n\\begin{align*}\n  \\dot u + \\nabla\\cdot w &= 0,\\\\\n  \\eps\\, \\dot w + \\nabla u + \\tfrac{d}{c^2} w &= 0,  \n\\end{align*}\n%\ncf.~\\cite[Ch.~I.1]{Bra07}. In particular, $\\eps$ is small (i.e., $c$ is large) if we consider the propagation of acoustic waves in solids~\\cite{CRC13}. Note that, for the limit case $\\eps=0$, this system reduces to the (parabolic) heat equation in mixed form with thermal diffusivity~$c^2/d$. \n%\nIn both cases, Dirichlet boundary conditions can be incorporated with the help of a Lagrange multiplier leading to a PDAE~\\cite{HinPUU09,Alt15}.\n\\end{example}\t\n%\n\\begin{example}[Viscoelastic Stokes problem]\\label{exp:Stokes}\nFlows of fluids with complex microstructure, e.g., molten polymers, can be modeled by\n%\n\\begin{align*}\n  \\rho\\,\\dot v - \\nabla\\cdot T + \\nabla p &= \\g,\\\\\n  \\eps\\, \\dot T - \\eta\\, \\big(\\nabla v + (\\nabla v)^\\top \\big)+ T &= \\f, \n\\end{align*}\n%\ntogether with the constraint $\\nabla \\cdot v = 0$, cf.~\\cite{Ren89}. Here, $v$ denotes the velocity field, for which we assume homogeneous Dirichlet boundary conditions, $T$ the Cauchy stress tensor, and~$p$ the pressure. Moreover, the density~$\\rho$, the zero-shear-rate viscosity $\\eta$, and the relaxation time~$\\eps$ are positive constants. This model can be seen as the linearized Navier--Stokes equation for viscoelastic fluids. In particular, the system reduces to the unsteady Stokes equation for a vanishing relaxation time~$\\eps$.\n\\end{example}\n%\n\\begin{example}[Electro-magnetic energy propagation in power networks]\\label{exp:pressure}\nThe third example describes the electro-magnetic energy propagation in power networks~\\cite{MagWTA00,GoeHS16} as well as the propagation of pressure waves in a network of gas pipes~\\cite{Osi87,BroGH11,JanT14,EggKLMM18}. \n%\nThe geometry of the underlying network can be encoded by a directed graph. In power networks, the edges represent transmission lines, whereas the vertices model a customer, a power supplier, or an interconnection. \n\n% single pipe\t\nFrom an analytical point of view, it is sufficient to consider a single transmission line~\\cite{EggK18}. Hence, we consider the unit interval as physical domain. Under certain simplifying model assumptions, the associated system in its strong form is given by\n%\n\\begin{align*}\n\t\\dot p + a\\, p + \\partial_x m &= \\g,  \\\\ %&&\\qquad \\text{in } (0,1), \\\\ \n\t\\eps\\, \\dot m + \\partial_x p + d\\,m &= \\f, % && \\qquad \\text{in } (0,1).\n\\end{align*}\n%\nwhere $a$ and $d$ are (space-dependent) damping parameters describing the resistance and conductance in the transmission line. This system is also known as {\\em telegrapher's equation}~\\cite{GoeHS16}. \nSimilar to the first example, constraints may occur due to inhomogeneous Dirichlet boundary conditions for the voltage~$p$ or as modeling tool to include the network structure. \n\nFor the propagation of pressure waves, where we have $a=0$ (cf.~\\cite{EggK17ppt,AltZ18ppt}), $\\eps$ equals the product of the adiabatic coefficient and the square of the Mach number and is of order~$10^{-3}$, cf.~\\cite{BroGH11}. \n\\end{example}\n%% results\nThe first main result of this paper compares the two corresponding solutions and shows that they only differ by a term of order~$\\eps$, as long as the initial data is chosen appropriately. At this point, we would like to emphasize that such a condition does not occur in the respective finite-dimensional setting, which was analyzed in~\\cite[Ch.~2.5]{KokKO99}. \nAlso in the infinite-dimensional setting, one can renounce such a condition if the initial values are sufficiently regular~\\cite[Ch.~2]{GhiG06}.  \n% bei glatteren Daten wird die Approximation von $p$ besser, nicht von $m$. \nFor general initial data, on the other hand, we loose half an order, leading to an order of~$\\sqrt{\\eps}$ only.  \nThe second main contribution considers a second-order approximation of the original solution. Again, sufficient conditions on the initial data and regularity assumptions on the right-hand sides are discussed, which guarantee the full approximation order of two. \nSimilarly as before, general initial data reduces the order to~$\\eps^{\\sfrac 32}$ or even~$\\sqrt{\\eps}$. To show that the presented estimates are indeed sharp, we examine specific numerical examples. \n\\smallskip\n\n%% Aufbau\nThe remaining parts of the paper are organized as follows. In Section~\\ref{sect:PDAE} we introduce the functional analytic setting for linear first-order PDAEs of hyperbolic type, including a small parameter~$0<\\eps\\ll 1$. \n%\nMoreover, we show that the particular examples introduced above fit into the presented framework. \nThe existence of mild and classical solutions as well as stability estimates are then discussed in Section~\\ref{sect:existence}. This also covers the discussion of the limit case for~$\\eps=0$, which is of parabolic nature. %Here, the existence of weak solutions under various regularity assumptions is studied.\nThe main results of the paper in Section~\\ref{sect:expansion} are devoted to the comparison of the solutions of the original and the limit equations, leading to first and second-order estimates in terms of the parameter~$\\eps$. Finally, the theoretical approximation orders a numerically verified by a number of experiments in Section~\\ref{sect:numerics}.\n\\smallskip\n\nThroughout this paper, we use for estimates the notion $a \\lesssim b$ for the existence of a generic constant $c>0$ such that $a \\le cb$.  \n%",,"\\section{Conclusion}\\label{sect:conclusion}\nIn this paper, we have considered linear PDAEs of hyperbolic type with a small parameter~$\\eps>0$, which turn parabolic in the limit case, i.e., for $\\eps=0$. Depending on the consistency of the initial data and the regularity of the right-hand sides, we have shown first- and second-order estimates of the corresponding expansion in terms of~$\\eps$. In a number of numerical experiments, we have validated these results and compared them with the finite-dimensional setting. \n%\nThe presented expansion may be used for the construction of novel numerical methods. For this, the approach needs to be combined with integration schemes for PDAEs of parabolic type such as splitting schemes~\\cite{AltO17}, Runge-Kutta methods~\\cite{AltZ18,Zim21}, discontinuous Galerkin methods~\\cite{VouR18}, or exponential integrators~\\cite{AltZ20,Zim21}. \n%"
,,
"The focus of this article is on the notion of good reduction for algebraic varieties and particularly for algebraic groups. Among the most influential results in this direction is the Finiteness Theorem of G.~Faltings, which confirmed Shafarevich's conjecture that, over a given number field, there exist only finitely many isomorphism classes of abelian varieties of a given dimension that have good reduction at all valuations of the field lying outside a fixed finite set. Until recently, however, similar questions have not been systematically considered in the context of linear algebraic groups. Our goal is to discuss the notion of good reduction for reductive linear algebraic groups and then formulate a finiteness conjecture concerning forms with good reduction over arbitrary finitely generated fields. This conjecture has already been confirmed in a number of cases, including for algebraic tori over all fields of characteristic zero and also for some semi-simple groups, but the general case remains open. What makes this conjecture even more interesting are its multiple connections with other finiteness properties of algebraic groups, first and foremost, with the conjectural properness of the global-to-local map in Galois cohomology that arises in the investigation of the Hasse principle. Moreover, it turns out that techniques based on the consideration of good reduction have applications far beyond the theory of algebraic groups: as an example, we will discuss a finiteness result arising in the analysis of length-commensurable Riemann surfaces that relies heavily on this approach. So, we hope that mathematicians working in different areas will find familiarity with good reduction quite rewarding and potentially useful. We therefore invite the reader to explore with us the fascinating symbiosis of ideas from the theory of algebraic groups and arithmetic geometry that lead to conjectures that are likely to be at the center of current efforts to develop the arithmetic theory of algebraic groups over fields more general than global.","""In this section, we will discuss several concrete examples of reduction of algebraic groups defined over the field $\mathbb{Q}$ of rational numbers at rational primes. In these examples, we will just analyze the result of reduction of a set of defining equations with integer coefficients. These considerations will motivate the formal and more general definition of good reduction in the next section. We recall that a {\it linear algebraic group} is a subgroup $G \subset \mathrm{GL}_n(\Omega)$ (where $\Omega$ is some algebraically closed field) that is defined by polynomial equations over $\Omega$ in terms of the matrix entries. Moreover, if $K$ is a subfield of $\Omega$ and the ideal of all polynomial functions over $\Omega$ that vanish on $G$ is generated by polynomials with coefficients in $K$, we say that $G$ is $K$-{\it defined} or is an {\it algebraic $K$-group}. We note that if $\mathrm{char}\: \Omega = 0$, then $G$ is $K$-defined if and only if it can be defined by polynomial equations with coefficients in $K$. For the purposes of this article, however, it is convenient to use a more functorial point of view and think of $G$ as an {\it affine group scheme} $\mathrm{Spec}\: K[G]$ over $K$ represented by the algebra of $K$-defined regular functions $K[G]$: concretely, this means that for any $K$-algebra $R$, the group $G(R)$ of $R$-points of $G$ can be identified with $\mathrm{Hom}_{K\mathrm{-alg}}(K[G] , R)$. Furthermore, the group operation $G \times G \to G$ results in the operation of co-multiplication $\Delta \colon K[G] \to K[G] \otimes_K K[G]$ on $K[G]$, eventually making the latter into a commutative {\it Hopf $K$-algebra}. We will now illustrate this point of view, along with the notion of reduction, in a couple of examples. For simplicity, we will take $K = \mathbb{Q}$, but this can be easily generalized.""","We now return to Question 5.3 about the forms of a reductive group having good reduction at a given set of discrete valuations and discuss two natural choices for the field $K$ and the set of discrete valuations $V$ of $K$. We will first consider the case where $K$ is the fraction field of a Dedekind ring $R$ and $V$ is the set of discrete valuations associated with the maximal ideals of $R$. After that, we will turn to finitely generated fields $K$ equipped with a divisorial set $V$. The Dedekind case. Here, the field $K$ is the fraction field of a Dedekind ring $R$ and $V$ is the set of discrete valuations of $K$ associated with the maximal ideals of $R$. Early interest in this case can be traced back to the work of G.~Harder (1967) and J.-L.~Colliot-Th\'el\`ene and J.-J.~Sansuc (1979). The basic case where $K = \mathbb{Q}$ and $R = \mathbb{Z}$, and hence $V$ is the set of all $p$-adic valuations of $\Q$, was considered by B.H.~Gross \cite{Gross} and B.~Conrad (2014). In particular, Gross showed that an absolutely almost simple simply connected $\Q$-group $G$ that has a good reduction at all primes $p$ necessarily splits over all $\mathbb{Q}_p$. Furthermore, the following finiteness result is true over any number field. Let $G$ be an absolutely almost simple simply connected algebraic group over a number field $K$, and let $V^K_f$ denote the set of all discrete valuations of $K$. Then for any subset $V \subset V_f^K$ such that $V_f^K \setminus V$ is finite, the number of $K$-isomorphism classes of $K$-forms of $G$ that have good reduction at all $v \in V$ is finite."
"\\section{Introduction}\n\nDistance regular graphs have many nice algebraic and combinatorial properties and have been extensively studied. \nFor instance, (the adjacency matrix $A$ of ) a distance regular graph $\\Gamma=(X,E)$ with vertex set $X$ and edge set $E$ has $d+1$ distinct eigenvalues $\\theta_0 > \\theta_1 > \\cdots > \\theta_d$ and the corresponding eigenspaces $V_0, V_1, \\ldots, V_d$ form a direct sum decomposition of the vector space $\\RR^X:= \\{ f: X\\to \\RR \\} \\cong \\RR^{|X|}$, where $d$ is the diameter of $\\Gamma$.\nFurthermore, there is a general method to obtain the eigenvalues and eigenspaces of a distance regular graph; see, for example, Brouwer, Cohen and Neumaier~\\cite[\\S4.1]{DistReg1}.\n\nOne can define an interesting product on each eigenspace $V_i$ of a distance regular graph $\\Gamma$ by doing the entry-wise product of two eigenvectors in $V_i$ and projecting the resulting vector back to $V_i$.\nThe gives an algebra, known as the \\emph{Norton algebra}, which is commutative but not necessarily associative.\nIt was studied in group theory due to its interesting automorphism group~\\cite{Norton1, Norton2}.\n\nWhen $\\Gamma$ belongs to certain important families of distance regular graphs (i.e., the Johnson graphs, Grassmann graphs, dual polar graphs, and hypercube graphs), Levstein, Maldonado and Penazzi~\\cite{DualPolarGraph,NortonAlgebra} constructed the eigenspaces from a filtration of vector spaces corresponding to a graded lattice associated with $\\Gamma$, and derived an explicit formula for the Norton product on the eigenspace of $V_1$.\nRecently Terwilliger~\\cite{Terwilliger} obtained a more general formula for $Q$-polynomial distance-regular graphs. \nBut for $i\\ge2$ the Norton algebra structure on $V_i$ has not been determined.\n\nIn this paper we focus on the \\emph{Hamming graph} $H(n,e)$, whose vertex set $X$ consists of all words of length $n$ on the alphabet $\\{0,1,\\ldots,e-1\\}$ and whose edge set $E$ consists of all unordered pairs of vertices differing in exactly one position.\nAs an important family of distance regular graphs, the Hamming graphs are useful in multiple branches of mathematics and computer science.\nTheir eigenvalues are well known~\\cite[\\S9.2]{DistReg1} and their eigenspaces have been investigated from various perspectives.\nFor example, Valyuzhenich and Vorob'ev~\\cite{Supp} studied the minimum cardinality of the support of an eigenvector of a Hamming graph, and for certain Hamming graphs with special parameters, Sander~\\cite{Sander} constructed bases for their eigenspaces using vectors over $\\{0,1,-1\\}$.\n\nIf we allow extension of scalars to the complex field $\\CC$, there is a nice complex eigenbasis of the Hamming graph $H(n,e)$ consisting of all linear characters of its vertex set $X=\\ZZ_e^n$ viewed as a group, and a real eigenbasis can be obtained by taking the real and imaginary parts of these characters.\nIn fact, this is valid for any Cayley graph of a finite abelian group; see, for example, Lov\\'{a}sz~\\cite[Exercise~11.8]{Lovasz79} and DeVos--Goddyn--Mohar--\\v{S}\\'{a}mal~\\cite{CayleySum}.\nIt will be an interesting problem to determine whether an eigenbasis over $\\ZZ$ (or even over $\\{0,1,-1\\}$) can be obtained.\n\nAs an application of the linear character approach, we provide a formula for the Norton product on each eigenspace $V_i$ of the Hamming graph $H(n,e)$, and use this formula to study the automorphism group $\\Aut(V_i)$ of the Norton algebra $V_i$.\nIt is known that the automorphisms of the Hamming graph $H(n,e)$ form a group isomorphic to the wreath product $\\SS_e \\wr \\SS_n$~\\cite[Theorem~9.2.1]{DistReg1}.\nWe show that $\\Aut(V_1)\\cong \\SS_e\\wr\\SS_n$ by constructing all idempotents in $V_1$, but $\\Aut(V_i)$ could be much smaller or bigger than this group is $i\\ne 1$.\nIn general, we construct a large subgroup of $\\Aut(V_i)$, which is related to the wreath product of the semidirect product $\\ZZ_e\\rtimes\\ZZ_e^\\times$ of the group $\\ZZ_e$ and its multiplicative group $\\ZZ_e^\\times$ with the symmetric group $\\SS_n$.\nA complete description of $\\Aut(V_i)$ for $i\\ge2$ will be a problem for future study.\n\nWe also determine the extent to which the Norton product on $V_i$ is nonassociative. %using a quantitative measurement called the \\emph{associative spectrum} by Cs\\'{a}k\\'{a}ny and Waldhauser~\\cite{AssociativeSpectra1} or \\emph{subassociativity type} by Braitt and Silberger~\\cite{Subassociative}, which was also proposed independently by Hein and the author~\\cite{CatMod}.\nFor a given binary operation $*$ on a set $Z$, let $C_{*,m}$ be the number of distinct results obtained by inserting parentheses into the expression $z_0* z_1* \\cdots * z_m$, where $z_0, z_1,\\ldots, z_m$ are $Z$-valued indeterminates.\nIt is well known that $C_{*,m}$ is bounded above by the ubiquitous \\emph{Catalan number} $C_m:=\\frac{1}{m+1}\\binom{2m}{m}$.\nWe have $C_{*,m}=1$ for all $m\\ge0$ if and only if $*$ is associative, and we say $*$ is \\emph{totally nonassociative} if $C_{*,m}=C_m$ for all $m\\ge0$.\nIn general, $C_{*,m}$ measures how far the operation $*$ is from being associative.  \nThe sequence $(C_{*,m})_{m\\ge0}$ was called the \\emph{associative spectrum} of the binary operation $*$ by Cs\\'{a}k\\'{a}ny and Waldhauser~\\cite{AssociativeSpectra1}.\nBraitt and Silberger~\\cite{Subassociative} studied this sequence for a groupoid $(G,*)$ and called it the \\emph{subassociativity type} of $(G,*)$.\nIndependently, Hein and the author~\\cite{CatMod} also proposed the study of $C_{*,m}$ for a binary operation $*$.\nFor further investigations of this nonassociativity measurement, see, e.g., Hein and the author~\\cite{VarCat} and Liebscher and Waldhauser~\\cite{AssociativeSpectra2}.\n\n%We determine $C_{\\star,n}$ for the Norton product $\\star$ on each eigenspace of the Hamming graphs. \nWe show that the Norton product $\\star$ on each eigenspace $V_i$ is totally nonassociative except for some special cases, in which it is either associative for some trivial reasons or equally as nonassociative as the \\emph{double minus operation} $\\ominus$ defined by $a\\ominus b:=-a-b$ for all $a,b\\in\\CC$, in the sense that any two ways to parenthesize $z_0\\ominus z_1\\ominus \\cdots \\ominus z_m$ produce distinct results if and only if so do the same two ways to parenthesize of $z_0\\star z_1\\star\\cdots \\star z_m$.\nTherefore in the last case we have $C_{\\star,m}=C_{\\ominus,m}$ given by the sequence A000975~\\cite{A000975} in OEIS~\\cite{OEIS}, according to Cs\\'{a}k\\'{a}ny and Waldhauser~\\cite{AssociativeSpectra1} and work of the author, Mickey, and Xu~\\cite{DoubleMinus}.\n\nBelow is a summary of our results on the Norton algebra of the Hamming graph $H(n,e)$.",,
"\\centerline{\\bf{ $\\S$1.} Introduction and the main results}\n\\vskip 0.5 true cm\nNon-smooth differential systems has been widely used in the field of nature, economics [9], nonlinear oscillations [19], and biology [4]. Thus many scholars began to study the dynamical behaviors of this system in recent years. One of the most important problems is to study the existence and number of limit cycles of non-smooth differential systems.\n\nPiecewise smooth differential systems is an important non-smooth differential systems. Many scholars have studied the number of limit cycles for piecewise smooth systems separated by a straight line. In [3,8,10,13,20], these scholars considered some piecewise smooth differential systems which are defined in two zones separated by $x=0$. Some scholars studied a series piecewise smooth differential systems with a switching line $y=0$, see [16,18,21]. In all of these papers, the two most used methods are Melnikov functions and averaging method, which are established in [8,12] and developed in [7,14,15] respectively.\n\nNow, one of the things that comes to mind is what will happen to the existence and number of limit cycles if piecewise smooth differential system with two zones separated by an algebraic curve of degree $n$?  Recently, in [1], a second order averaged functions has been developed and applied to study the number of limit cycles of piecewise linear differential systems with two zones separated by a cubic curve. For the switching curve of degree $n$, the authors [17] considered the crossing limit cycles of a class of discontinuous piecewise linear differential systems formed by two linear differential systems having only centers with a switching curve $y=x^{n}$. The authors in [6] showed that for each $n\\in\\mathbb{N}$ there exist piecewise linear differential systems separated by an algebraic curve of degree $n$ having $\\left[\\frac{n}{2}\\right]$ hyperbolic limit cycles. In [22], the author studied a piecewise Near-Hamilton systems separated by $y=\\pm x^{2}$.\n\nIn this paper, motivated by the above analysis, we study the number of limit cycles bifurcating from the linear center with an algebraic switching curve $y=x^{m}$ by the first order Melnikov function, where $m\\geq 1$ is an arbitrary positive.\n\nConsider the following perturbed piecewise smooth differential system\n$$\n\\left(\n  \\begin{array}{c}\n    \\dot{x} \\\\\n    \\dot{y} \\\\\n  \\end{array}\n\\right)\n=\\left\\{\n\\begin{aligned}\n\\left(\n  \\begin{array}{c}\n    y+\\epsilon p^+(x,y) \\\\\n    -x+\\epsilon q^+(x,y) \\\\\n  \\end{array}\n\\right)\n,\\ y\\geq x^m,\\\\\n\\left(\n  \\begin{array}{c}\n    y+\\epsilon p^-(x,y) \\\\\n    -x+\\epsilon q^-(x,y) \\\\\n  \\end{array}\n\\right)\n,\\ y<x^m, \\\\\n\\end{aligned}\n\\right.\\eqno(1.1)_\\epsilon$$\nwhere $$p^{\\pm}(x,y)=\\sum^{n}_{i+j=0}a^{\\pm}_{i,j}x^{i}y^{j},~~q^{\\pm}(x,y)=\\sum^{n}_{i+j=0}b^{\\pm}_{i,j}x^{i}y^{j}$$ are any polynomials of degree $n$.\nLet $Z(m,n)$ be the upper bound of the number of limit cycles for Hamilton system $(1.1)_\\epsilon$(taking into account the multiplicity). Our main results are as follows.","""\vskip 0.2 true cm\n\noindent{\bf Theorem 1.1.}\nSuppose that $m=2k+1$($k\in \mathbb{N})$. By using the first order Melnikov function in $\epsilon$, we have\n\n(i)~~For $(m,n)\in D_{1}=:\{(m,n)|0\le n<m-1\}$,\n  $$Z(m,n)\leq \frac{1}{2}\left[\frac{n+3}{2}\right]\left[\frac{n+5}{2}\right]+\left[\frac{n+2}{2}\right]\left[\frac{n+4}{2}\right]-2,$$\n  $$Z(m,n)\geq \left[\frac{n}{2}\right]\left[\frac{n+6}{2}\right]+\left[\frac{n-1}{2}\right]+2,\qquad \qquad\quad$$\n  and the lower bound can be reached by some system $(1.1)_{\epsilon}$.\n\n(ii)~~Let $\delta_n=0$ if $n$ is odd, and $\delta_n=-1$ if $n$ is even. Then we have\n$$Z(m,n)\leq \begin{cases}\n      \frac{1}{2}\left[\frac{n+3}{2}\right]\left[\frac{n+5}{2}\right]\n      +(2k+1)\left[\frac{n}{2}\right]-(k-1)^{2},~\text{if}~(m,n)\in D_{2};\\\n      (2k+1)\left(2\left[\frac{n}{2}\right]+\delta_n\right)+k(5-3k)+1, ~\text{if}~(m,n)\in D_{3};\end{cases}$$\n$$Z(m,n)\geq 2(k+1)\left[\frac{n}{2}\right]+\delta_n-(k-1)^{2}+2, ~~{\text {if $(m,n)\in D_{2}\cup D_{3}$}},  $$\nand the lower bound can be reached by some system $(1.1)_{\epsilon}$, where $D_{2}=\{(m,n)|m-1\leq n<2m-1\}$ and $D_{3}=\{(m,n)|n\geq 2m-1\}$.\n\n\vskip 0.2 true cm\n\noindent{\bf Theorem 1.2.}\nSuppose that $m=2k$($k\in \mathbb{N}$). By using the first order Melnikov function in $\epsilon$, we have\n\n(i)~~ For $(m,n)\in D_{4}=:\{(m,n)|0\leq n<m\}$, we have\n  $$Z(m,n)\leq\n      4\left[\frac{n}{2}\right]^{2}+\left(6\delta_{n}+11\right)\left[\frac{n}{2}\right]+\frac{1}{2}\delta_{n}\left(5\delta_{n}+17\right)+4,$$\n  $$Z(m,n)\geq \frac{1}{2}\left[\frac{n}{2}\right]\left(\left[\frac{n}{2}\right]+3\right)+\frac{1}{2}\left[\frac{n-1}{2}\right]\left(\left[\frac{n-1}{2}\right]+7\right)+3,$$\n  and the lower bound can be reached by some system $(1.1)_{\epsilon}$.\n\n(ii)~~ For $(m,n)\in D_{5}\cup D_{6}=:\{(m,n)|m\leq n<2m-2\}\cup\{(m,n)|n\geq 2m-2\}$, we have\n  $$Z(m,n)\leq (3k+1)\left[\frac{n-1}{2}\right]+k\left[\frac{n}{2}\right]-k(k-5)-1,$$\n  $$Z(m,n)\geq \begin{cases}\n  (k+2)\left[\frac{n-1}{2}\right]+k\left[\frac{n}{2}\right]-k(k-3)+1, ~if~(m,n)\in D_{5},\\\n  3k^{2}+4k-3,~if~n=2m-2,\\\n  3k^{2}+5k-2,~if~n=2m-1,\\\n  3k^{2}+6k-2,~if~n=2m.\n  \end{cases}$$\n  and the lower bound can be reached by some system $(1.1)_{\epsilon}$.""",
,,
,"""\subsection{Torsors and classifying torsors}\n\label{sous section torseurs classifiant}\n\nLet $G$ be a smooth linear algebraic $k$-group. The group $G$ is fixed for this subsection.\n\nFor any field extension $K/k$, one has non abelian Galois cohomology sets $\h^1(K,G)=\h^1_\text{Gal}(K,G)$ ($=\h^1_{\text{\'et}}(K,G)$ since $G$ is smooth) - see \cite[\S 29.]{KMRT}. The set $\h^1(K,G)$ classifies the set of isomorphism classes of $G$-torsor over $K$ (\cite[I.\S 5, Prop. 33.]{Serre_CohomologieGaloisienne}). There is a distinguished element of $\h^1(K,G)$, namely the class of the trivial $G$-torsor, which corresponds to the trivial cocycle or to the isomorphism class of the torsor $G_K/K$ on which $G$ acts by right multiplication. For a $G$-torsor $Y/K$ over a field $K/k$, let's write $[Y/K]$ for the isomorphism class of $Y/K$ as a $G$-torsor : $[Y/K] \in \h^1(K,G)$.\n\nWe will consider the functor \[ \h^1(\star,G) : \left\lbrace \begin{array}{ccc}\n\underline{Fields/k} & \to & \underline{Sets} \\\nK/k & \mapsto & \h^1(K,G)\n\end{array} \right. .\]\n\n\n\subsubsection{Versal torsor}\n\nWe call \emph{versal torsor of $G$} or \emph{versal $G$-torsor} a $G$-torsor $\pi : E \to X$, where $X$ is a smooth $k$-scheme, that satisfies the following property : for any field extension $K/k$ with $K$ \emph{infinite} (as a set), and any $G$-torsor $Y/K$, there exists a $K$-point of $X$, $x : \spec(K) \to X$, such that $Y/K$ is isomorphic to the pullback $x^{-1}(\pi)$ as $G$-torsors.\n\nA pratical version of a versal $G$-torsor is given by the quotient map $\pi : \mathrm{GL}_n \to \mathrm{GL}_n/G$ for any faithful linear representation $G \hookrightarrow \mathrm{GL}_n$ (we refer to \cite[\S 5.3.]{GMS}). In this article we will only consider such versal $G$-torsor and use the notation $\pi : E \to X$, where $E=\mathrm{GL}_n$ and $X=\mathrm{GL}_n/G$. Also we will often consider the generic fiber of $\pi$ : it is the $G$-torsor $\pi_{\text{gen}} : E_\text{gen} \to \spec(k(X))$. Roughly speaking it is the most general or complicated $G$-torsor over a field.\n\n\begin{rem}\nWhen $G$ is moreover connected, then even for finite fields $K$, the condition that every $G$-torsor over $K$ is the pullback of a versal $G$-torsor $\pi : E \to X$ is satisfied when $E(k) \neq \emptyset$ since all torsors over a finite fields are trivial for smooth and connected (linear) algebraic groups (see \cite{Lang_finite}).\n\end{rem}\n\n\subsubsection{Simplicial classifying torsor}\n\label{sous section torseur simplicial}\n\nWe follow here \cite[\S 6]{Deligne_Hodgeiii}.\n\nConsider the simplicial $k$-scheme $\e G^\bullet$ determined by $\e G^{(n)}= G^{n+1}=\underset{n+1 \text{ terms}}{\underbrace{G \times_k \cdots \times_k G}}$ with degeneracy maps \[ s_i : (g_0, \cdots,g_n) \mapsto (g_0,\cdots,g_{i-1},1,g_i,\cdots,g_n) \] and face maps \[ \delta_i : (g_0, \cdots,g_n) \mapsto  (g_0,\cdots,g_{i-1},g_{i+1},\cdots,g_n) . \] Letting $G$ act on the $\e G^{(n)}$'s by right multiplication), the face and degeneracy maps are $G$-equivariant and by quotient one gets a simplicial $k$-scheme $\mathrm{B} G^\bullet$ where $\mathrm{B} G^{(n)}= G^{n+1}/G$. The map $\e G^\bullet \to \mathrm{B} G^\bullet$ is a (simplicial) $G$-torsor, in the sense that each $\e G^{(n)}$ is a $G$-torsor over $\mathrm{B} G^{(n)}$, and the face and degeneracy maps are morphisms of $G$-torsors. The simplicial $k$-scheme $\mathrm{B} G^\bullet$ is the simplicial model of \emph{the classifying space} for $G$. We will call it the \emph{simplicial classifying space of $G$}.\n\n\nIn the same fashion, for a $G$-torsor $Y \to Z$, consider the simplicial $k$-scheme $[Y]^{\bullet}$ determined by the schemes $[Y]^{(n)}=G^{n+1} \times_k Y =\underset{n+1 \text{ terms}}{\underbrace{G \times_k \cdots \times_k G}} \times_k Y$, with degeneracy maps \[ s_i : (g_0, \cdots,g_n,y) \mapsto (g_0,\cdots,g_{i-1},1,g_i,\cdots,g_n,y) \] and face maps \[ \delta_i : (g_0, \cdots,g_n,y) \mapsto  (g_0,\\cdots,g_{i-1},g_{i+1},\cdots,g_n,y) . \] Letting $G$ act on $[Y]^{(n)}$, the maps $s_i$ and $\delta_i$ are $G$-equivariant. By taking quotients with respect to the $G$-actions, one gets a simplicial $k$-scheme $[Y | G]^{\bullet}$ where $[Y |G]^{(n)}=( G \times_k \cdots \times_k G \times_k Y )/G$. \n\n\vspace{\baselineskip}\n\nLet $\mathcal{F}$ be a \'etale sheaf on the big site of $\spec(k)$ and let $Y \to Z$ be a $G$-torsor. From the definition of torsors, it comes that $[Y | G]^{(n)}$ is isomorphic to $\underset{n+1 \text{ terms}}{\underbrace{Y \times_Z \cdots \times_Z Y}}$. Considering $Z$ as the constant simplicial $k$-scheme $C_k Z^\bullet$ and using projections on $Z$, one finds a map $[Y|G]^\bullet \to C_k Z^\bullet$. Whence a canonical homomorphism between cohomology groups : \begin{equation}\n\label{morphisme canonique cohomologie torseur simplicial}\n\h^d(Z,\mathcal{F}) \simeq \h^d(C_k Z^\bullet,\mathcal{F}) \to \h^d([Y | G ]^\bullet,\mathcal{F}), \: \forall d \geqslant 0.\n\end{equation} \n\n\begin{prop}[{\cite[Prop. A.3. \& Rem. A.4.]{EKLV}}]\n\label{iso cohomologie simplicial} \nThe natural homomorphism \eqref{morphisme canonique cohomologie torseur simplicial} is an isomorphism.\n\end{prop}\n\n\begin{rem}\nIn general the torsor $Y \to Z$ has no local section with respect to the Zariski topology, but it has local sections with respect to the \'etale topology (here $G$ is smooth). It is important to use \'etale cohomology in Proposition \ref{iso cohomologie simplicial} and not Zariski cohomology.\n\end{rem}\n\nThe simplicial sheme $\mathrm{B} G^\bullet$ is called \""",
,,
"\\section{Introduction}\n\nIn this thesis we investigate linear differential-algebraic equations (DAEs) with constant real coefficients of the form\n\\begin{align}\\label{eq:intro_1}\n\\tfrac{\\mathrm{d}}{\\mathrm{d}t}(Ex) = Ax + Bu\n\\end{align}\nwith constant real coefficients $(E,A,B)\\in\\Sigma_{\\ell,n,m} = \\mathbb{R}^{\\ell\\times n}\\times\\mathbb{R}^{\\ell\\times n}\\times\\mathbb{R}^{\\ell\\times m}$. A locally integrable control $u\\in L^1_{\\text{loc}}(\\mathbb{R}_{\\geq 0},\\mathbb{R}^m)$ and trajectory $x\\in L^1_{\\text{loc}}(\\mathbb{R}_{\\geq 0},\\mathbb{R}^n)$ so that $Ex\\in AC(\\mathbb{R}_{\\geq 0},\\mathbb{R}^{\\ell})$ is almost everywhere differentiable is called a solution of~\\eqref{eq:intro_1}, if~\\eqref{eq:intro_1} is almost everywhere fullfilled. By studying the set of all solutions $(x,u)$ of~\\eqref{eq:intro_1} it is possible to formulate some controllability concepts for DAEs of the form~\\eqref{eq:intro_1}. We are interested in topological properties of the set\n\\begin{align*}\nS := \\set{(E,A,B)\\in\\Sigma_{\\ell,n,m}\\,\\big\\vert\\,(E,A,B)~\\text{``controllable''}},\n\\end{align*}\nwhere ``controllable'' stands for these different controllability concepts. \\cite{DAEs} have shown that~$S$ can be described by certain algebraic conditions. Wonham proved in~\\cite[Theorem 1.3, p.\\,44]{Wonham} that the set of controllable linear ordinary differential equations (ODEs) of the form\n\\begin{align}\\label{eq:intro_ODE}\n\\tfrac{\\mathrm{d}}{\\mathrm{d}t} x = Ax + Bu,\\quad (A,B)\\in\\mathbb{R}^{n\\times n}\\times\\mathbb{R}^{n\\times m},~u\\in L^1_{\\text{loc}}(\\mathbb{R}_{\\geq 0},\\mathbb{R}^m)\n\\end{align}\nis generic using an algebraic condition, namely the well-known Kalman criterion. We take this result as a motivation and show that~$S$ is generic under certain conditions on~$\\ell,n,m.$","""\section{Genericity: definitions and elementary properties}\n\nFor the following observations let~$n\in\mathbb{N}^*$,~$\mathbb{F} = \mathbb{R}$ or~$\mathbb{C}$ and provide~$\mathbb{F}^n$ with the Euclidean norm~$\norm{\cdot}_2.$ Further, we identify any polynomial\n\begin{align}\n    p(x) = p(x_1,\ldots,x_n) = \sum_{k = 0}^\ell a_k x_1^{\nu_{k,1}}\cdots x_n^{\nu_{k,n}}\in \mathbb{F}[x_1,\ldots,x_n]\n\end{align}\nwith its polynomial function\n\begin{align}\n    p(\cdot): \mathbb{F}^n\to\mathbb{F},\quad x = (x_1,\ldots,x_n) \mapsto p(x) =  \sum_{k = 0}^\ell a_k x_1^{\nu_{k,1}}\cdots x_n^{\nu_{k,n}}.\n\end{align}\n\n\begin{Definition}[{Algebraic variety, genericity, see~\cite[p.\,28]{Wonham} and~\cite[p.\,50]{UndAlgGeom}}]~\label{def:gen_set}\nA set~$ \mathbb{V} \subseteq \mathbb{F}^n$ is called an \emph{algebraic variety}\footnote{The notion of \textit{algebraic varieties} differs from source to source. What~\cite{Wonham} calls algebraic variety (in fact he only calls them variety, but as suggested in~\cite[p.\,240]{GeomMeasTheory} there is an analytic analogon) is called \textit{algebraic set} or \textit{affine variety} in algebraic geometry books (e.g.~\cite{AlgVar,UndAlgGeom}). Since we are interested in applications in mathematical systems theory, we will stick to Wonhams nomenclature.}, if there exist finitely many polynomials\n\begin{align*}\n    p_1(x_1,\ldots, x_n),\ldots,p_k(x_1,\ldots, x_n) \in \mathbb{F}[x_1,\ldots, x_n]\n\end{align*}{}\nsuch that~$\mathbb{V}$ is the locus of their zeros, i.e.\n\begin{align}\n\mathbb{V} = \set{x\in \mathbb{F}^n\,\big\vert\, \forall\, i\in\underline{k}: p_i(x)=0} = \bigcap_{i=1}^k p_i^{-1}(\set{0}).\n\end{align}\nAn algebraic variety~$ \mathbb{V}$ is called \emph{proper} if ~$ \mathbb{V} \subsetneq \mathbb{F}^n$, and \emph{nontrivial} if~$\mathbb{V} \neq \emptyset.$ The set of all algebraic varieties in~$\mathbb{F}^n$ is denoted as\n\begin{align}\n    \mathcal{V}_n(\mathbb{F}) := \set{\mathbb{V}\subseteq\mathbb{F}^n\,\Big\vert\,\exists\, q_1(\cdot),\ldots,q_k(\cdot)\in\mathbb{F}[x_1,\ldots, x_n]: \bigcap_{i=1}^k q^{-1}(\set{0}) = \mathbb{V}}\n\end{align}\nand the set of all proper algebraic varieties as\n\begin{align}\label{eq:def_prop_var}\n    \mathcal{V}_n^{\text{prop}}(\mathbb{F}) := \mathcal{V}_n(\mathbb{F})\setminus\set{\mathbb{F}^n}.\n\end{align}\n\nA set~$S\subseteq\mathbb{F}^n$ is called \textit{generic}, if there exist a proper algebaric variety $\mathbb{V}\in\mathcal{V}_n^{\text{prop}}(\mathbb{F})$ so that $S^c\subseteq\mathbb{V}$. If the algebraic variety $\mathbb{V}$ is known, then we call $S$ \textit{generic with respect to (w.r.t.)~$\mathbb{V}$.}\n\end{Definition}""","\\section{Outlook}\n\nIn the case~$\\ell = n$ we find with Proposition~\\ref{prop:full_rank_property_block} and Lemma~\\ref{lem:product_of_generic_sets} that\n\\begin{align*}\nS = \\mathcal{GL}_n(\\mathbb{R})\\times\\mathbb{R}^{n\\times n}\\times\\mathbb{R}^{n\\times m}\n\\end{align*}\nis generic. For any~$(E,A,B)\\in S$ the DAE\n\\begin{align}\\label{eq:eine_DAE}\n\\tfrac{\\mathrm{d}}{\\mathrm{d}t}(Ex) = Ax+Bu\n\\end{align}\nis equivalent to the ODE\n\\begin{align}\\label{eq:eine_ODE}\n\\tfrac{\\mathrm{d}}{\\mathrm{d}t}x = E^{-1}Ax + E^{-1}Bu\n\\end{align}\nand hence~\\eqref{eq:eine_DAE} is controllable in any sense, if~\\eqref{eq:eine_ODE} is controllable. From the Kalman-criterion (Lemma~\\ref{lem:Kalmam_rank_condition}) we obtain that the latter is the case if, and only if\n\\begin{align*}\nn = \\rk[E^{-1}B,E^{-1}AE^{-1}B,(E^{-1}A)^2E^{-1}B,\\ldots,(E^{-1}A)^{n-1}E^{-1}B].\n\\end{align*}\nLet~$\\widehat{M}$ be a minor of order~$n$ w.r.t.~$\\mathbb{R}^{n\\times nm}$ and\n\\begin{align*}\nM: S\\to\\mathbb{R},\\quad (E,A,B) \\mapsto \\widehat{M}([E^{-1}B,E^{-1}AE^{-1}B,(E^{-1}A)^2E^{-1}B,\\ldots,(E^{-1}A)^{n-1}E^{-1}B]).\n\\end{align*}\nThen we find that~$M^{-1}(\\set{0})$ is a algebraic variety, which is proper since~$\\set{I_n}\\times\\Sigma_{n,m}^{\\text{cont}}\\subseteq\\left(M^{-1}(\\set{0})\\right)^c$. Since \n\\begin{align*}\n\\left(M^{-1}(\\set{0})\\right)^c\\subseteq\\set{(E,A,B)\\in\\Sigma_{n,n,m}\\,\\big\\vert\\,~\\eqref{eq:eine_DAE}~\\text{``controllable''}},\n\\end{align*}\nthe latter set is generic for any controllability concept from Section 5 and we have no information about the DAEs with singular~$E$. Hence we would rather consider the smaller set~$S^c$ and investigate the properties of\n\\begin{align}\\label{eq:eine_Menge}\n\\set{(E,A,B)\\in S^c\\,\\big\\vert\\,~\\eqref{eq:eine_DAE}~\\text{``controllable''}}.\n\\end{align}\nSince~$S^c\\not\\cong\\mathbb{R}^{k}$ for any~$k\\in\\mathbb{N}$, the concept of generic sets can not be applied. In Remark~\\ref{rem:Zariski_topology} we have given an alternative definition of generic sets as sets which contain some non-empty Zariski-open set. We could introduce the notion of ``relative generic'' sets of some set~$V\\subseteq\\mathbb{F}^n$ as sets which contain some nonempty set that is open in the relative topology on~$V$ induced by the Zariski-topology (i.e. some relative Zariski-open set). But this concept would contain no information for many sets, e.g. if~$V$ is a countable set. Then any nonempty subset of~$V$ is some nonempty relative Zariski-open set. So we should add one more condition for a set in order to be relative generic in~$V$. In Lemma~\\ref{lem:some_uninteresting_lemma} we have seen that Zariski open sets are dense w.r.t. the Euclidean topology. Thus we could say a set~$S'\\subseteq V$ is \\textit{relative generic} in~$V$, if~$S'$ contains some relative Zariski-open set which is dense w.r.t.~the Euclidean topology.\n\nThe next step is to verify that this concept of relative genericity is useful to describe the relation of~$S'\\subseteq\\mathbb{F}^n$ and~$V\\subseteq\\mathbb{F}^n$, if~$V\\not\\cong\\mathbb{F}^k$ for any~$k\\in\\mathbb{N}$. Then we check whether the set from~\\eqref{eq:eine_Menge} is relative generic in~$S^c$, if we replace the term ``controllable'' with the controllability concepts from Section 5 and the stabilizability concepts from Section 6.\n\nUnfortunately, this is beyond the scope of this thesis."
"\\section{Introduction}\nLet $K$ be a field and let $F$ be a field extension endowed with an endomorphism \n$\\phi$ such that $\\phi(K)\\subset K$. \nA linear $\\phi$-difference equation over $K$ is an equation of the form \n\\begin{equation}\\label{eq: phi0}\n\\phi^{n} (y) +a_{n-1}\\phi^{n-1}(y)+ \\cdots + a_{0} y=0 \\,,\n\\end{equation}\nwhere  $a_{0},...,a_{n-1} \\in K$. \nThe set ${\\rm Sol}_{\\phi,K,F}$, formed by all elements in $F$ that are solution to a linear $\\phi$-difference \nequation over $K$, is a ring  containing $K$.  Traditionally, the algebraic relations over $K$ between the elements of \n${\\rm Sol}_{\\phi,K,F}$ are studied through the difference Galois theory associated \nwith the endomorphism $\\phi$ \n(see  for instance, \\cite{VdPdifference}).  \nIf we assume that $K$ and $F$ are endowed with a second endomorphism $\\s$ \nthat is \\emph{sufficiently independent} from $\\phi$, we expect the intersection of \n${\\rm Sol}_{\\phi,K,F}$ and ${\\rm Sol}_{\\s,K,F}$ to be \\emph{small},  \neven possibly reduced to $K$.  \nSch\\\",,
"\\section{Introduction}\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\nLet $F$ be a field and let $\\mathcal{C}_{F}$ be the set of all sequences $\\pmb{s}=(s_{n})_{n\\geqslant 0}$ over $F$. Let us consider the following operations:\n\\begin{itemize}\n\\item componentwise sum $+$, defined by\n$$\\pmb{c}+\\pmb{d}=\\pmb{a},\\quad a_{n}=c_{n}+d_{n},\\quad \\forall n\\in \\mathbb{N};$$\n\\item Hadamard product $.$, defined by\n$$\\pmb{c}\\pmb{d}=\\pmb{c}.\\pmb{d}=\\pmb{a},\\quad a_{n}=c_{n}d_{n},\\quad \\forall n\\in \\mathbb{N};$$\n\\item Hurwitz product $\\ast$, defined by\n$$\\pmb{c}\\ast\\pmb{d}=\\pmb{a},\\quad a_{n}=\\sum_{i=0}^{n}\\binom{n}{i}c_{i}d_{n-i},\\quad \\forall n\\in \\mathbb{N}.$$\n\\end{itemize}\nIt is well known that $(\\mathcal{C}_{F},+,.)$ and $(\\mathcal{C}_{F},+,\\ast)$ are commutative rings with the same additive identity $\\pmb{0}=(0,0,0\\ldots)$\nand with multiplicative identities $\\pmb{1}=(1,1,1,\\ldots)$ and\n$\\pmb{1^{\\ast}}=(1,0,0,\\ldots)$, respectively.\n ~\\\\ \\indent\nLet $\\widetilde{\\mathcal{C}_{F}}$ be the set of all $F$-subspaces of $\\mathcal{C}_{F}$.It is easily seen that\n\\begin{enumerate}[label=(\\alph*)]\n \\item $(\\widetilde{\\mathcal{C}_{F}},+,.)$ is a commutative semiring, in which $\\Span\\{\\pmb{0}\\}$ and $\\Span\\{\\pmb{1}\\}$ are the additive identity and\n the multiplicative identity respectively, where $H+H'$ is the sum of the subspaces $H$ and $H'$, and $HH'$ is the subspace of $\\mathcal{C}_{F}$ spanned\n by all Hadamard products $\\pmb{c}\\pmb{d}$ with $\\pmb{c}\\in H$ and $\\pmb{d}\\in H'$.\n \\item $(\\widetilde{\\mathcal{C}_{F}},+,\\ast)$ is a commutative semiring, in which $\\Span\\{\\pmb{0}\\}$ and $\\Span\\{\\pmb{1^{\\ast}}\\}$ are the additive identity\n and the multiplicative identity respectively, where $H\\ast H'$ is the subspace of $\\mathcal{C}_{F}$ spanned\n by all Hurwitz products $\\pmb{c}\\ast\\pmb{d}$ with $\\pmb{c}\\in H$ and $\\pmb{d}\\in H'$.\n\\end{enumerate}\n~\\indent It is well known that the set of all linear recurrence sequences can be endowed with several interesting algebraic\nstructures~\\cite{Alecci, Cerrut, Chin, Kura1, Kura2, Lars, Peters}, and that the set $L(P)$ of all linear recurrence sequences over $F$ having $P$ as a characteristic polynomial is a $F$-vector space of dimension $\\deg(P)$.\n\\\\~\\indent In this paper we discuss some algebraic properties of the subset $\\mathcal{L}_{F}$ of $\\widetilde{\\mathcal{C}_{F}}$ consisting of all $L(P)$ where $P$ is monic and splits over $F$. it is well-known that For all $P,Q\\in F[C]$, $L(P)+L(Q)=L(H)$ where $H$ is the least common multiple of $P$ and $Q$. From this it follows clearly that $(\\mathcal{L}_{F},+)$ is a commutative monoid. In fact, we have more than this. we will show that $(\\mathcal{L}_{F},+,.)$\nand $(\\mathcal{L}_{F},+,\\ast)$ are, respectively, subsemirings of $(\\widetilde{\\mathcal{C}_{F}},+,.)$ and $(\\widetilde{\\mathcal{C}_{F}},+,\\ast)$ graded by\nthe multiplicative monoid $F$. In order to prove these we define a new commutative semiring structure on the set of nonnegative integers $\\mathbb{N}$.\nThe addition in $\\mathbb{N}$ is defined to be the the maximum $\\vee$ of two integers and the multiplication, $\\wedge$, is the disjunction operation introduced\nin~\\cite{Zier} for positive integers and extended to $\\mathbb{N}$ by setting $0\\wedge n=n\\wedge 0=0$ for all $n\\in \\mathbb{N}$.\n~\\\\ \\indent\nIn addition, we show, with the aid of this result and others presented in this paper, that if \\begin{equation*}\nP_{1}=X^{s_{1}}Q_{1},\\ldots,P_{m}=X^{s_{m}}Q_{m}\n\\end{equation*}\nare monic polynomials over $F$, where $Q_{i}(0)\\neq 0,\\, 1\\leq i\\leq m$, then\n\\begin{enumerate}[label=(\\arabic*)]\n\\item \\begin{equation*}\n\\prod_{i=1}^{m}L(P_{i})=L(X^{\\rho}\\Upsilon(Q_{1},\\ldots,Q_{m})),\n\\end{equation*}\nwhere\n\\begin{eqnarray*}\n\\rho=\\begin{cases}\n\\min\\{s_{i}/i\\in \\Theta\\}&\\quad\\text{if}\\quad \\Theta\\neq\\emptyset\\\\\n\\max\\{s_{i}/1\\leq i\\leq m\\}& \\quad\\text{otherwise}\n\\end{cases}\n\\end{eqnarray*}\nand\n\\begin{equation*}\n\\Theta=\\{i/Q_{i}=1\\}.\n\\end{equation*}\n\\item \\begin{equation*} L(P_{1})\\ast\\cdots\\ast L(P_{m})=L(\\Psi(P_{1},\\ldots,P_{m})).\\end{equation*}\n\\end{enumerate}\n$\\Upsilon(Q_{1},\\ldots,Q_{m})$ and $\\Psi(P_{1},\\ldots,P_{m})$ are polynomials over $F$ that we determine in terms of the roots of the polynomials\n$P_{1},\\ldots,P_{m}$ in an algebraic closure of $F$.\n\\\\Results $(1)$ and $(2)$ above however are not new and, in many works, They are obtained under identical hypothesis or a bit more restrictive one using\neither direct methods, or a Hopf Algebra approach.\n\\cite{Buck, Cak, Cerrut, Chin, Nied, Gott, Kura1, Kura2, Selm, Zier, Zya}. Our approach, however, gives more detailed information and allows obtaining\nresults $(1)$ and $(2)$ in a compact form to facilitate their application in future works. furthermore, we end the paper by discussing results obtained by Chin\nand Goldman~\\cite{Chin}. The authors of the aforementioned paper have derived several important results. However, in Remark~\\ref{remer} we will see that\nthe main result in~\\cite{Chin} is not correct, and also how this can be corrected.",,
"\\section{Introduction}\n\nGaussian probabilistic inference is an important tool in a number of fields including machine learning, computer vision, and robotics.  The aim is to produce a Gaussian belief, $p(\\mbf{x}|\\mbf{z})$, of the state of the world, $\\mbf{x}$, given a prior model, $p(\\mbf{x})$, and some evidence, $\\mbf{z}$, and hence we often take a Bayesian perspective. Even if the full Bayesian posterior is not Gaussian (e.g., in the case of nonlinear models), we might choose to find the best Gaussian approximation thereof.  \n\nAs discussed by \\citet{opper09} and more recently by \\citet{barfoot_ijrr20} we can start by considering that we want to find a Gaussian approximation, $q(\\mbf{x})$, that minimizes the \\ac{KL} divergence \\citep{kullback51} from the true Bayesian posterior, $p(\\mbf{x} | \\mbf{z})$:\n\\begin{equation}\n\\mbox{KL}(q||p) = - \\int^{\\mbs{\\infty}}_{-\\mbs{\\infty}} q(\\mbf{x}) \\ln \\left( \\frac{p(\\mbf{x} | \\mbf{z})}{q(\\mbf{x})} \\right) \\, d\\mbf{x} .\n\\end{equation}\nNoting that we can factor $p(\\mbf{x},\\mbf{z}) = p(\\mbf{x}|\\mbf{z})p(\\mbf{z})$ we can equivalently find the $q(\\mbf{x})$ that minimizes\n\\begin{equation}\n\\label{eq:functional}\nV(q) = \\mathbb{E}_q[ \\phi(\\mbf{x})] + \\frac{1}{2} \\ln \\left( |\\mbs{\\Sigma}^{-1}| \\right),\n\\end{equation}\nwith $\\phi(\\mbf{x}) = - \\ln p(\\mbf{x},\\mbf{z})$ and the second term being the well-known expression for the {\\em entropy} of a Gaussian.  This functional, $V(q)$, is sometimes referred to as the (negative) {\\em Helmholtz free energy} in statistical physics \\citep[\\S 11.1, p.385] {koller09} or the (negative) \\ac{ELBO} in machine learning.  In general, this approach is referred to as {\\em variational inference} or {\\em variational Bayes} \\citep{jordan99,bishop06}.  As we will restrict ourselves to Gaussian approximations of the posterior, we will refer to the approach as \\ac{GVI}.\n\nOur Gaussian approximation will take the standard multivariate form,\n\\begin{equation}\nq(\\mbf{x}) = \\mathcal{N}(\\mbs{\\mu}, \\mbs{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^N |\\mbs{\\Sigma}|}} \\exp\\left( -\\frac{1}{2} (\\mbf{x} - \\mbs{\\mu})^T \\mbs{\\Sigma}^{-1} (\\mbf{x} - \\mbs{\\mu}) \\right), \n\\end{equation}\nwhere $|\\cdot|$ is the determinant, $\\mbs{\\mu}$ is the mean, and $\\mbs{\\Sigma}$ is the covariance.  For many practical robotics and computer vision applications, the dimension of the state, $N$, can become very large and so we are very much concerned with finding efficient solutions that can exploit any structure in our problem.  \n\nProblem-specific structure derives from sparsity in the underlying graphical model of the system.  If we consider that the joint likelihood of the state and data factors, we can write its negative log-likelihood as\n\\begin{equation}\n\\phi(\\mbf{x}) =  \\sum_{k=1}^K \\phi_k( \\mbf{x}_k ), \n\\end{equation}\nwhere $\\phi_k(\\mbf{x}_k) = - \\ln p(\\mbf{x}_k,\\mbf{z}_k)$ is the $k$th (negative log) factor expression, $\\mbf{x}_k$ is a {\\em subset} of variables in $\\mbf{x}$ associated with the $k$th factor, and $\\mbf{z}_k$ is a subset of the data in $\\mbf{z}$ associated with the $k$th factor.  Substituting this into~\\eqref{eq:functional} we have\n\\begin{equation}\nV(q) = \\sum_{k=1}^K \\mathbb{E}_{q_k}[ \\phi_k(\\mbf{x}_k)] + \\frac{1}{2} \\ln \\left( |\\mbs{\\Sigma}^{-1}| \\right),\n\\end{equation}\nwhere critically the expectation for the $k$th factor reduces to being over $q_k(\\mbf{x}_k)$, the {\\em marginal} associated with the variables involved in that factor \\citep{barfoot_ijrr20}. \n\n\\citet{barfoot_ijrr20} go on to show that the following Newton-like iterative scheme can be used to seek the minimum of $V(q)$ in terms of $\\mbs{\\mu}$ and $\\mbs{\\Sigma}^{-1}$:\n\\begin{subequations}\\label{eq:iterstein}\n\\begin{eqnarray}\n\\left(\\mbs{\\Sigma}^{-1}\\right)^{(i+1)} & = &  \\underbrace{\\sum_{k=1}^K \\mathbb{E}_{q_k^{(i)}}\\left[ \\frac{\\partial^2}{\\partial \\mbf{x}_k^T \\partial \\mbf{x}_k} \\phi_k(\\mbf{x}_k)\\right]}_{\\mbf{A}}, \\label{eq:iterstein1} \\\\\n\\left(\\mbs{\\Sigma}^{-1}\\right)^{(i+1)} \\, \\delta\\mbs{\\mu} & = & \\underbrace{- \\sum_{k=1}^K \\mathbb{E}_{q_k^{(i)}}\\left[ \\frac{\\partial}{\\partial \\mbf{x}_k^T} \\phi_k(\\mbf{x}_k)\\right]}_{\\mbf{b}},  \\label{eq:iterstein2}   \\\\\n\\mbs{\\mu}^{(i+1)} & = & \\mbs{\\mu}^{(i)} + \\delta\\mbs{\\mu},  \\label{eq:iterstein3}\n\\end{eqnarray}\n\\end{subequations}\nwhere $(i)$ is the iteration index.  This scheme can be shown to be carrying out \\ac{NGD} \\citep{amari98,Amari2016}, which approximates the Hessian using the \\ac{FIM} \\citep{barfoot_arxiv20,barfoot_ijrr20,barfoot_amai20}.\n\n\nThe scheme thus proceeds in two alternating phases:\n\\begin{description}\n\\item[build the linear system:] We use the current marginals, $q_k(\\mbf{x}_k)$, which are also Gaussian, to assemble the two matrices, $\\mbf{A}$ and $\\mbf{b}$; here we can use Gaussian quadrature to compute the required expectations or even approximate further by evaluating only at the mean, which results in standard \\ac{MAP} estimation\n\\item[solve the linear system:] We solve $\\mbf{A} \\, \\delta\\mbs{\\mu} = \\mbf{b}$ for $\\delta\\mbs{\\mu}$ and then update $\\mbs{\\mu}$; we must also extract the new marginals, which requires calculating the entries of $\\mbf{A}^{-1}$ (the covariance matrix) corresponding to the nonzero entries of $\\mbf{A}$ (the inverse covariance matrix)\n\\end{description}\nIt is the second phase, `solve the linear system', with which we are concerned in this paper.  The inverse covariance matrix, $\\mbf{A}$, is typically quite sparse for key problems such as trajectory estimation, bundle adjustment \\citep{brown58}, simultaneous localization and mapping \\citep{durrantwhyte06a}, calibration, and pose-graph optimization.  However, the covariance matrix, $\\mbf{A}^{-1}$ is usually dense.  \\citet{barfoot_ijrr20} further discuss how in computing the marginals for the next `build the linear system' phase we need only compute the entries of $\\mbf{A}^{-1}$ associated with the nonzero entries of $\\mbf{A}$, although this can also be seen plainly in~\\eqref{eq:iterstein}.\n\nThe rest of this paper is organized as follows.  Section~\\ref{sec:problemsetup} provides the specific linear algebra problem with which we are concerned in this paper.  Section~\\ref{sec:globalsolution} uses Kronecker algebra along with so-called elimination and duplication matrices to provide a novel analysis and presentation of a global solution to the problem based on the method of \\citet{takahashi73}.  Section~\\ref{sec:localsolution} provides a novel method to carry out the global solution in a purely local manner using message passing amongst a collection of agents computing in parallel; this method is guaranteed to converge to the unique global solution in both the mean and desired covariance quantities.  Section~\\ref{sec:relatedwork} discusses related work in the literature; we chose to provide related work after the main technical results of the paper so as to allow several detailed connections to be made.  Section~\\ref{sec:conclusion} wraps things up and discusses possibilities for future work.","""\section{Problem Setup}\n\label{sec:problemsetup}\n\nMotivated by the discussion in the introduction, we can begin by stating the problem with which we are concerned in this paper:\n\n\begin{problem} \label{prob:flapogi}\n{\em (\acf{FLAPOGI})} Consider the linear system of equations, $\mbf{A} \mbf{x} = \mbf{b}$, where $\mbf{A}$ is $N \times N$, real, symmetric, positive definite, and having a known sparsity pattern; solve for (i) the unique $\mbf{x}$ and (ii) the entries of $\mbf{A}^{-1}$ corresponding to the nonzero entries of $\mbf{A}$.\n\end{problem}\n\nDisregarding any sparsity enjoyed by $\mbf{A}$, there is an obvious solution, which is to compute $\mbf{A}^{-1}$ and then (i) take $\mbf{x} = \mbf{A}^{-1} \mbf{b}$ and (ii) extract the desired entries of $\mbf{A}^{-1}$.  This will always be possible given that $\mbf{A} > 0$.  However, when the size of the linear system $N$ is very large the complexity of this brute-force approach is $O(N^3)$.\n\nWe can also convert the second part of Problem~\ref{prob:flapogi} into a linear system of equations using the vectorization, $\vec(\cdot)$, and Kronecker product, $\otimes$, operations detailed in Appendix~\ref{app:kronvec}.  We first note that\n\begin{equation}\n\mbf{A} \mbf{A}^{-1} = \mbf{1},\n\end{equation}\nwhere $\mbf{1}$ is the identity matrix.  Then we have\n\begin{equation}\label{eq:vecAAinv}\n( \mbf{1} \otimes \mbf{A} ) \vec(\mbf{A}^{-1}) = \vec(\mbf{1}),\n\end{equation}\nwhere we have used some basic identities found in Appendix~\ref{app:kronvec}.  Since $\mbf{A}^{-1}$ is symmetric (because $\mbf{A}$ is symmetric by assumption), we can remove the redundant entries by defining an {\em elimination} matrix, $\mbf{E}$, to pick the upper-half entries but then reconstitute them using a {\em duplication} matrix, $\mbf{D}$:\n\begin{equation}\n\mbf{D} \underbrace{\mbf{E} \vec(\mbf{A}^{-1})}_{\mbf{y}} = \vec(\mbf{A}^{-1}),\n\end{equation}\nwhich holds for any symmetric matrix.  See Appendix~\ref{app:elimdup} for more on elimination and duplication matrices and their properties.\nInserting this to~\eqref{eq:vecAAinv} and then premultiplying both sides of our equation by $\mbf{E}$ we have\n\begin{equation}\n\mbf{E} ( \mbf{1} \otimes \mbf{A} )\mbf{D} \, \mbf{y} = \mbf{E} \vec(\mbf{1}).\n\end{equation}\nThus, we can solve this system for the unique entries of $\mbf{A}^{-1}$ stored in $\mbf{y}$.  Unfortunately, the coefficient matrix $\mbf{E} ( \mbf{1} \otimes \mbf{A} )\mbf{D}$ does not have any obvious exploitable structure when expressed in this form.  The next section will discuss how to solve\n\beqn{flapogi}\n\mbf{A} \mbf{x} & = & \mbf{b}, \label{eq:flapogi1} \\\n\mbf{E} ( \mbf{1} \otimes \mbf{A} )\mbf{D} \, \mbf{y} & = & \mbf{E} \vec(\mbf{1}), \label{eq:flapogi2}\n\eeqn\nefficiently using a triangular factorization of $\mbf{A}$.  We will refer to~\eqref{eq:flapogi1} as the {\em primary problem} and~\eqref{eq:flapogi2} as the {\em secondary problem}.  We will see that the order in which place our variables in $\mbf{x}$ and $\mbf{y}$ is important; to avoid confusion, we will refer to the order of $\mbf{x}$ as the {\em primary variable order} and the order of $\mbf{y}$ as the {\em secondary variable order}.""","\\section{Conclusion and Future Work}\n\\label{sec:conclusion}\n\nWe have provided motivation for and a clear statement of the \\ac{FLAPOGI} problem, which is of course not new.  This problem is important in Gaussian estimation even when the full Bayesian posterior is not Gaussian (e.g., in the \\ac{ESGVI} framework).  Our novel contributions are twofold.  First, we provide a novel presentation of the \\citet{takahashi73} solution to the problem.  Second, we work out the details necessary to implement the method using a purely local message-passing scheme between a collection of agents computing in parallel.  The result is that we can guarantee convergence on both the primary (mean) and secondary (covariance) problems, even in the presence of loops in the associated factor graph, in a finite number of iterations for synchronous updates.\n\nThere are several avenues that could be explored in future work.  Foremost, we would like to implement the proposed local message-passing scheme on a large real-world inference problem to evaluate its performance; here our aim was merely to lay the foundation.  Also, we avoided discussion of how to best select the primary variable order, which influences the amount of fill-in in the global matrices and the local message-passing communication graph; we believe it may be possible to combine the result of \\citet{takahashi73} with the modern solvers used for inference in robotics and computer vision \\citep{kaess08,kaess11} to this aim.  It would also be interesting to look more deeply at the connection to Gaussian belief propagation; the methods perform the same on a tree graph, but what approximations are needed in the loopy case for our message-passing scheme to be similar/equivalent to \\ac{GaBP} (e.g., perhaps we simply need to delete the extra communication channels our method requires)?  We hope the ideas presented herein provide a starting point for further investigation."
,"""\section{Least squares problem}\n\nLet \( A \in \mathbb{R}^{m \times n} \), where \( m > n \). The system\n\[\nAx  = b\n\]\nfor a given vector \( b \in \mathbb{R}^m \) and solution \( x  \in \mathbb{R}^n \) is termed {\bf overdetermined} because it contains more equations than unknowns. This system essentially asks whether \( b \) can be expressed as a linear combination of the columns of \( A \):\n$$\nb = x_1a_{\cdot 1} + x_2a_{\cdot 2} + \cdots + x_na_{\cdot n}.\n$$\nFor the square system (the case \( m = n \)), the answer is ``yes'' provided that the column vectors \( \{a_{\cdot  1}, a_{\cdot  2}, \ldots, a_{\cdot  n}\} \) are linearly independent (or equivalently for a nonsingular matrix \( A \)). However, for overdetermined systems (\( m > n \)), the answer is usually ``no'' unless \( b \) happens to lie in the span of \( \{a_{\cdot  1}, a_{\cdot  2}, \ldots, a_{\cdot  n}\} \) (often denoted \( \text{span}(A) \) or \( \text{range}(A) \)), which is highly unlikely in most applications. Therefore, in general, such a system has no solution.\n\nTo illustrate this, consider the case where \( m = 3 \) and \( n = 2 \). In this scenario, \( a_{\cdot  1} \) and \( a_{\cdot  2} \) represent two vectors in \( \mathbb{R}^3 \). If \( a_{\cdot  1} \) and \( a_{\cdot  2} \) are linearly independent, then their span forms a plane (a \( 2 \)-dimensional subspace) in \( \mathbb{R}^3 \). The system \( Ax  = b \) has a solution if \( b \) lies in that plane; otherwise, the system has no solution. The probability of a vector \( b \in \mathbb{R}^3 \) lying in a plane is zero.\n\nIn such situations, one obvious alternative to ``solving the linear system exactly'' is to minimize the residual vector\n\[\nr = b - (x_1a_{\cdot  1} + x_2a_{\cdot  2} + \cdots + x_na_{\cdot  n}) = b - Ax .\n\]\nThe solution to the problem depends on how we measure the length of the residual vector. It is preferred to use the \( 2 \)-norm, although any norm could be used. The \( 2 \)-norm is induced by the inner product, thus is related to the notion of orthogonality, and it is smooth and strictly convex. These properties make the theory and computation with this norm much easier than with other norms. With the use of the \( 2 \)-norm, the solution is the vector \( x  \) that minimizes the sum of squares of differences between the components of \( b \) and \( Ax  \). This method is known as {\bf least squares}. In the least squares method, we seek to find an optimal vector that solves the minimization problem:\n\begin{equation}\label{leastsquaredef}\n\min_{x  \in \mathbb{R}^n} \|Ax  - b\|_2. \vsp\n\end{equation}\nAs we pointed out, the solution \( x  \) of this problem (which always exists) may not exactly satisfy \( Ax  = b \). To reflect the lack of exact equality, we may write the linear least squares problem as\n\[\nAx  \cong b,\n\]\nand approximation is understood in the least square sense.\n\vsp""",
"\\section{Introduction}\n\nThe notion of biderivations appeared in different areas. Maksa used biderivations to study real Hilbert space \\cite{Maksa}. Vukman investigated symmetric biderivations in the prime and semiprime rings \\cite{Vukman}. The well-known result that every biderivation on a noncommutative prime ring $A$ is of the form $\\lambda[x,y]$ for some $\\lambda$ belonging to the extended centroid of $A$, was discovered independently by Bresar et al \\cite{Bresar&Martindale&Miers}, Skosyrskii \\cite{Skosyrskii}, and Farkas and Letzter \\cite{Farkas&Letzter}, where biderivations were connected with noncommutative Jordan algebras by Skosyrskii and with Poisson algebras by Farkas and Letzter, respectively. Besides their wide applications, biderivations are interesting in their own right and have been introduced to Lie algebras \\cite{Wang&Yu&Chen}, which were studied by many authors recently. In particular, biderivations are closely related to the theory of commuting linear maps that has a long and rich history, and we refer to the survey \\cite{Bresar} for the development of commuting maps and their applications. It is worth mentioning that Bre\\v{s}ar and Zhao considered a general but simple approach for describing biderivations and commuting linear maps on a Lie algebra $L$ that having their ranges in an $L$-module \\cite{Bresar&Zhao}, which covered most of the results in \\cite{Chen, Liu&Guo&Zhao, Han&Wang&Xia, Tang, Wang&Yu}, and inspires us to generalize their method to Hom-Lie algebras.\n\nHom-Lie algebras are a generalization of Lie algebras, the notion of which was initially introduced in \\cite{Hartwig&Larsson&Silvestrov}, motivated by the study of quantum deformations or $q$-deformations of the Witt and the Virasoro algebras via twisted derivations, after several investigations of various quantum deformations of Lie algebras\\cite{Aizawa&Sato, Chaichian&Kulish, Curtright&Zachos, Hu, Kassel}. Due to the close relation to discrete and deformed vector fields and differential calculus, this kind of algebraic structures have been studied extensively during the last decade, which we mention here just a few: representations, (co)homology and deformations of Hom-Lie algebras \\cite{Agrebaoui&Benali&Makhlouf, Alvarez&Cartes, Makhlouf&Silvestrov, Sheng, Yau}; structure theory of simple and semisimple Hom-Lie algebras \\cite{Chen&Han, Jin&Li, Xie&Liu}; Hom-Lie structures on Kac-Moody algebras \\cite{Makhlouf&Zusmanovich}; extensions of Hom-Lie algebras \\cite{Casas&GarciaMartinez, Larsson&Silvestrov}; geometric generalization of Hom-Lie algebras \\cite{Laurent-Gengoux&Teles, Peyghan&Nourmohammadifar}; integration of Hom-Lie algebras \\cite{Laurent-Gengoux&Makhlouf&Teles}.\n\nHence it would be natural to generalize known theories from Lie algebras to Hom-Lie algebras, and we are interested in determining biderivations and commuting linear maps on a Hom-Lie algebra follows from \\cite{Bresar&Zhao}. The paper is organized as follows.\n\nIn Section 2, after recalling some preliminaries on Hom-Lie algebras, we introduce the notion of Hom-Lie algebra module homomorphisms, and give the Schur's lemma for adjoint Hom-Lie algebra modules.\n\nIn Section 3, we define biderivations on a Hom-Lie algebra $(L,\\alpha)$ having their ranges in an $(L,\\alpha)$-module $(V, \\rho, \\beta)$ as well as the centroid of $(V, \\rho, \\beta)$, and derive that\nskew-symmetric biderivations arise from the centroid provided that\n$(L,\\alpha)$ is perfect, $\\alpha$ is surjective, $\\beta$ is invertible and $Z_V(L)=\\{0\\}$ (see Theorem \\ref{BiderofperfectHomLiealg}). In particular, every skew-symmetric biderivation on a simple $(L,\\alpha)$ with $\\alpha$ invertible having its range in the $\\alpha^k$-adjoint $(L,\\alpha)$-module is of the form $\\lambda\\alpha^k([x,y])$ (see Theorem \\ref{BiderofsimpleHomLiealg}). We also give an algorithm to find all skew-symmetric biderivations and apply it to several examples.\n\nIn Section 4, we give the definition of commuting linear maps from a Hom-Lie algebra $(L,\\alpha)$ to an $(L,\\alpha)$-module $(V, \\rho, \\beta)$, which coincides with the centroid of $(V, \\rho, \\beta)$ if $\\alpha$ is surjective, $\\beta$ is invertible and $Z_V(L')=\\{0\\}$ (see Theorem \\ref{ComofHomLiealg}). An\nalgorithm to describe all commuting linear maps on $(L,\\alpha)$ is also provided.\n\nThroughout this paper, all the vector spaces are over a fixed field $\\F$ such that $\\char(\\F)\\neq 2$ unless otherwise stated.",,
"\\section{Introduction}\n\\label{section:introduction}\n\nLet $\\Omega \\subset \\R^d$ with $d \\ge 1$ be a bounded Lipschitz domain with polytopal boundary.\nGiven $f \\in L^2(\\Omega)$, we aim to numerically approximate the weak solution $u^\\star \\in H^1_0(\\Omega)$ of the nonlinear boundary value problem\n\\begin{align}\\label{eq:strongform}\n \\begin{split}\n  - \\div A(\\nabla u^\\star) &= f \\quad \\text{in } \\Omega,\\\\\n u^\\star &= 0 \\quad \\text{ on } \\partial\\Omega.\n \\end{split}\n\\end{align}\nTo this end, we propose an adaptive algorithm of the type\n\\begin{align}\\begin{split}\n & \\hspace*{2.7cm} \\boxed{\\text{ estimate total error and its components}}\\\\\n & \\hspace*{6.3cm} \\downarrow\\\\\n & \\boxed{\\text{advance algebra/advance linearization/mark and refine mesh elements}}\n\\end{split}\\end{align}\nwhich monitors and adequately stops the iterative linearization and the linear algebraic solver as well as steers the local mesh-refinement. The goal of this contribution is to perform a first rigorous mathematical analysis of this algorithm in terms of convergence and quasi-optimal computational costs.","""\section{Adaptive algorithm and main results}\n\label{section:main_results}\n\nIn this section, we introduce an abstract setting, in which all our results will be formulated, define the exact weak and finite elements solutions, introduce our requirements on mesh-refinement, error estimator, and algebraic solver, state our adaptive algorithm, and present our main results, including some discussions.\n\n\subsection{Abstract setting}\n\label{section:abstract}\nLet $\HH$ be a Hilbert space over $\K \in \{ \R, \C \}$ with scalar product $\product\cdot\cdot$, corresponding norm $\enorm\cdot$, and dual space $\HH'$ (with canonical operator norm $\enorm{\cdot}')$.\nLet $P: \HH \to \K$ be G\^ateaux-differentiable with derivative $\AA := {\rm d}P: \HH \to \HH'$, i.e.,\n\begin{align*}\n \dual{\AA w}{v}_{\HH'\times\HH} = \lim_{\substack{t \to 0 \\ t \in \R}} \frac{P(w+tv)-P(w)}{t}\n \quad \text{for all } v, w \in \HH.\n\end{align*}\nWe suppose that the operator $\AA$ is {\em strongly monotone} and {\em Lipschitz-continuous}, i.e.,\n\begin{align}\label{def:assumptions_operator}\n \alpha \, \enorm{w - v}^2 \le {\rm Re} \, \dual{\AA w - \AA v}{w - v}_{\HH'\times\HH}\n \quad \text{and} \quad\n \enorm{\AA w - \AA v}' \le L \, \enorm{w - v}\n\end{align}\nfor all $v, w \in \HH$, where $0 < \alpha \le L$ are generic real constants. \n\nGiven a linear and continuous functional $F \in \HH'$, the main theorem on monotone operators~\cite[Section~25.4]{zeidler} yields existence and uniqueness of the solution $u^\star \in \HH$ of\n\begin{align}\label{eq:exact_solution}\n \dual{\AA u^\star}{v}_{\HH'\times\HH} = F(v)\n \quad \text{for all } v \in \HH.\n\end{align}\nThe result actually holds true for any closed subspace $\XX_\coarse \subseteq \HH$, which also gives rise to a unique $u_\coarse^\star \in \XX_\coarse$ such that\n\begin{align}\label{eq:exact_solution_disc}\n \dual{\AA u_\coarse^\star}{v_\coarse}_{\HH'\times\HH} = F(v_\coarse)\n \quad \text{for all } v_\coarse \in \XX_\coarse.\n\end{align}\n\nFinally, with the {\em energy functional} $\EE := {\rm Re}\,(P - F)$, it holds that\n\begin{align}\label{eq:energy}\n \frac{\alpha}{2} \, \enorm{v_\coarse - u_\coarse^\star}^2\n \le \EE(v_\coarse) - \EE(u_\coarse^\star)\n \le \frac{L}{2} \, \enorm{v_\coarse - u_\coarse^\star}^2\n \quad \text{for all } v_\coarse \in \XX_\coarse;\n\end{align}\nsee, e.g.,~\cite[Lemma~5.1]{banach}.\nIn particular, $u^\star \in \HH$ (resp.\ $u_\coarse^\star \in \XX_\coarse^\star$) is the unique minimizer of the minimization problem\n\begin{align}\label{eq:minimization}\n \EE(u^\star) = \min_{v \in \HH} \EE(v)\n \quad \big( \text{resp.} \quad\n \EE(u_\coarse^\star) = \min_{v_\coarse \in \XX_\coarse} \EE(v_\coarse) \big).\n\end{align}\nAs for linear elliptic problems, it follows from \eqref{def:assumptions_operator}--\eqref{eq:exact_solution_disc} that the present setting guarantees the C\'ea lemma (see, e.g.,~\cite[Section~25.4]{zeidler})\n\begin{align}\label{eq:cea}\n \enorm{u^\star - u_\coarse^\star}\n \le \Ccea \, \enorm{u^\star - v_\coarse}\n \quad \text{for all } v_\coarse \in \XX_\coarse\n \quad \text{with} \quad\n \Ccea := L/\alpha.\n\end{align}""","\\section{Conclusion}\n\\label{section:conclusion}\n\nIn this work, we have proposed and analyzed a fully adaptive algorithm for the numerical approximation of nonlinear elliptic boundary value problems. The algorithm combines adaptive mesh-refinement, iterative linearization, and an algebraic solver. We have proved that the algorithm is linearly convergent, achieves optimal decay rates with respect to the number of degrees of freedom, and is also optimal with respect to the overall computational cost. Numerical experiments have confirmed our theoretical findings and demonstrated the robustness of the algorithm with respect to the choice of parameters."
"\\section{Introduction}\nLet $G$ be a group and $\\varphi$  an endomorphism of $G$. Two elements $x, y\\in G$ are said to be $\\varphi$-twisted conjugate, denoted by $x\\sim_{\\varphi} y$, if $y=gx\\varphi(g)^{-1}$ for some $g\\in G$. Clearly, $\\sim_{\\varphi}$ is an equivalence relation on $G$. The equivalence classes with respect to this relation are called \\textit{$\\varphi$-twisted conjugacy classes} or \\textit{Reidemeister classes} of $\\varphi$. If $\\varphi=\\Id$, then the $\\varphi$-twisted conjugacy classes are the usual conjugacy classes. Let $[x]_\\varphi$ denote the $\\varphi$-twisted conjugacy class containing $x\\in G$ and $\\mathcal{R}(\\varphi):=\\{[x]_\\varphi\\mid x\\in G\\}$. The cardinality of $\\mathcal{R}(\\varphi)$, denoted by $R(\\varphi)$, is called the \\emph{Reidemeister number} of $\\varphi$. A group $G$ is said to have  the \\textit{$R_{\\infty}$-property} if $R(\\varphi)$ is infinite for every automorphism $\\varphi$ of $G$.\n\nThe problem of determining  groups which  have the $R_{\\infty}$-property is an active area of research begun by Fel'shtyn and Hill \\cite{fh94} although the study of twisted conjugacy can be traced back to the works of Gantmakher in \\cite{gant}. The reader may refer to \\cite{FN} and the references therein for more literature. The $R_{\\infty}$-property of irreducible lattices in a connected semisimple Lie group of real rank at least $2$ has been studied by Mubeena and Sankaran (see \\cite[Theorem 1]{mstrans}). Nasybullov showed that if $K$ is an integral domain of zero characteristic and $\\Aut(K)$ is torsion then $\\GL_n(K)$ and $\\SL_n(K)$ (for $n> 2$) have the $R_{\\infty}$-property (see \\cite{nasy12}). \nA Chevalley group $G$ (resp. a twisted Chevalley group $G'$) over a field $K$ of characteristic zero possesses the $R_{\\infty}$-property if the transcendence degree of $K$ over $\\Q$ is finite, see \\cite[Theorem 3.2]{FN} (resp. \\cite[Theorem 1.2]{bdr}).\nIt is worth mentioning that a reductive linear algebraic group $G$ over an algebraically closed field $k$ of characteristic zero possesses the  $R_{\\infty}$-property if the transcendence degree of $k$ over $\\Q$ is finite and the radical of $G$ is a proper subgroup of $G$ (see \\cite[Theorem 4.1]{FN}). The converse of the latter  holds if the group $G$ is a Chevalley group of classical type ($A_n, B_n, C_n, D_n$) as shown in \\cite[Theorem 8]{nas20}, where the author proves that if $k$ has infinite transcendence degree over $\\Q$ then there exists an automorphism $\\varphi$ of $G$ such that $R(\\varphi)=1$. However, it turns out that the automorphism thus obtained is induced by a non-trivial automorphism of $k$ and therefore $\\varphi$ is not an algebraic group automorphism of $G$. This motivates the following consideration:\n\n\nLet $k$ be an algebraically closed field and  $G$ a linear algebraic group defined over $k$. Let $\\Aut(G)$ denote the group of all algebraic group automorphisms of $G$.  \nWe say that $G$ has the \\emph{algebraic $R_{\\infty}$-property} if $R(\\varphi)=\\infty$ for all $\\varphi\\in \\Aut(G)$. Since we shall deal with only algebraic automorphisms of algebraic groups in the sequel, we call the algebraic $R_\\infty$-property simply as the $R_\\infty$-property of $G$. A natural question that arises is the following:\n\\begin{question}\\label{question}\nUnder what conditions does $G$ have the $R_{\\infty}$-property ?\n\\end{question}\nMotivation for the present work comes from the results of Springer in \\cite{springer}, where he studied the twisted conjugacy classes in simply connected semisimple algebraic groups. \nA classical result by Steinberg says that for a connected linear algebraic group $G$ over an algebraically closed field $k$ and a surjective endomorphism $\\varphi$ of $G$, if $\\varphi$ has a finite set of fixed points then $G=\\{g\\varphi(g)^{-1}\\mid g\\in G\\}=[e]_{\\varphi}$, i.e., $R(\\varphi)=1$. Therefore, in this scenario, $G$ does not satisfy the $R_{\\infty}$-property (see \\cite[Theorem 10.1]{St}).  \nFor an endomorphism $\\varphi$ of a simple algebraic group $G$ there exists the following dichotomy:\n (1) $\\varphi$ is an automorphism. (2) $\\varphi$ has a finite set of fixed points (see  \\cite[Corollary 10.13]{St}). \nIn an attempt to answer Question \\ref{question}, we prove the following sufficient condition:\n\\begin{theorem*}[\\thmref{mainthm}]\nLet $G$ be a connected non-solvable linear algebraic group over an algebraically closed field $k$. Then $G$ possesses the $R_{\\infty}$-property. \n\\end{theorem*}\nHowever, the above condition is not necessary as is evident from Example \\ref{eg3} in \\secref{example} of this article. Also, see \\propref{solvable}.",,"\\section{Examples}\\label{example}\nIn this section we compute $R(\\varphi)$ for certain classes of algebraic groups. Example \\ref{eg3} shows that the sufficient condition proven in \\thmref{mainthm} is not necessary.\n\\begin{enumerate}\n\\item Let $\\mathrm{D}_n(k)=\\{\\diag(t_1,\\ldots, t_n)\\mid t_i\\in k^{\\times}\\}$ ($n\\geq 1$) be the diagonal subgroup of $\\GL_n(k)$. \n\\begin{enumerate}\\label{diag}\n\\item\\label{diag1} Let $\\varphi$ be an automorphism of $\\mathrm{D}_n(k)$ given by $$\\varphi(\\diag(t_1,\\ldots, t_n))=\\diag(t_1^{-1},\\ldots, t_n^{-1}).$$ Then $\\mathrm{D}_n(k)=[I_n]_{\\varphi}$. Thus $\\mathrm{D}_n(k)$ does not satisfy the $R_{\\infty}$-property.\n\\item For $r\\in \\mathbb{N}$, let $\\varphi_r$ be an automorphism of $\\mathrm{D}_n(k)$ given by $$\\varphi_r(\\diag(t_1, t_2, \\ldots, t_n))=\\diag(t_n, t_1, t_2, \\ldots, t_{n-2}, t_{n-1}t_n^{-r}).$$ Then $\\mathrm{D}_n(k)=[I_n]_{\\varphi_r}$ for all $r\\in \\mathbb{N}$.\n\\end{enumerate} \n\\item Let $\\varphi=\\varphi_1\\times \\varphi_2$ be the automorphism of $\\mathbb{G}_a\\times \\mathbb G_m$, where $\\varphi_1$ is an automorphism of $\\mathbb G_a$ given by $\\varphi_1(x)=\\alpha x$ for $\\alpha\\in k^{\\times}\\setminus \\{1\\}$ and $\\varphi_2$ is an automorphism of $\\mathbb{G}_m$ given by $\\varphi_2(x)=x^{-1}$. Then $R(\\varphi)=R(\\varphi_1)R(\\varphi_2)=1$. Therefore $\\mathbb{G}_a\\times \\mathbb G_m$ does not satisfy the $R_{\\infty}$-property. \n\\item\\label{unipotent} Let $G=\\U_n(k)$ ($n\\geq 1$) be the group of all upper triangular unipotent matrices of $\\GL_n(k)$, $d:=\\diag(t_1, t_2, \\ldots, t_n)\\in \\mathrm{D}_n(k)$ such that $t_i\\neq t_j$ for all $i\\neq j$ and  define an automorphism $\\varphi_d: \\U_n(k)\\longrightarrow \\U_n(k)$ by $\\varphi_d(g)=dgd^{-1}$ for all $g\\in \\U_n(k)$. \n\n\\noindent\n\\textbf{Claim:} $R(\\varphi_d)=1$, i.e. $g\\sim_{\\varphi_d}I_n$ for all $g\\in\\U_n(k)$.\n\n\\noindent\\emph{Proof of claim:} Let $g=(g_{ij})\\in \\U_n(k)$. It suffices to find some $y\\in \\U_n(k)$ such that $g=y^{-1}\\varphi_d(y)$ or equivalently,\n\\begin{equation}\\label{solution}\nyg=dyd^{-1}\n\\end{equation}\n For $1\\leq i<j\\leq n$ let us consider the following system of equations in the variables $x_{ij}$\n\\begin{equation}\\label{eq}\n(t_it_j^{-1}-1)x_{ij}=g_{ij}+\\displaystyle\\sum_{i< k<j}x_{ik}g_{kj}\n\\end{equation}\n\nNow, by the assumption on $d$, we note that $(t_it_j^{-1}-1)\\neq 0$ for all $i\\neq j$. Therefore, it is clear that Equation \\eqref{eq} admits a unique solution say $y_{ij}\\quad (1\\leq i<j\\leq n)$. If we set $y_{ii}=1$ for all $1\\leq i\\leq n$ and $y_{ij}=0$ for all $i>j$, then $y=(y_{ij})$ satisfies Equation \\eqref{solution}. Hence, the proof. \\qed \n\n(See \\cite[Theorem 2]{timur19}).\n\\item Let $G=\\mathrm{D}_l(k)\\times \\U_n(k)$ ($l, n\\geq 1$). The map $\\psi:G\\longrightarrow G$ defined by $\\psi(s,u)=(\\varphi(s),\\varphi_d(u))$  (for all $(s,u)\\in G$) is an automorphism (where $\\varphi$ is as in Example \\ref{diag1} and $d,\\varphi_d$ as in Example \\ref{unipotent}). Observe that $R(\\psi)=R(\\varphi)R(\\varphi_d)=1$. \n\\item\\label{eg3} The standard Borel subgroup $\\B_2(k)=\\Big\\{\\begin{pmatrix}a&b\\\\0&a^{-1}\\end{pmatrix}\\mid a\\in k^{\\times}, b\\in k\\Big\\}$ of $\\SL_2(k)$ satisfies $R_\\infty$-property. Note that $\\B_2(k)=U\\rtimes T\\cong \\mathbb{G}_a\\rtimes \\mathbb{G}_m$, where $T=\\Big\\{\\begin{pmatrix}a&0\\\\0&a^{-1}\\end{pmatrix}\\mid a\\in k^{\\times}\\Big\\}\\cong\\mathbb{G}_m$ and $U=\\Big\\{\\begin{pmatrix}\n1&y\\\\0&1\n\\end{pmatrix}\\mid y\\in k\\Big\\}\\cong \\mathbb{G}_a$. Let $\\varphi\\in \\Aut(\\B_2(k))$. Since $T$ and $\\varphi(T)$ are two maximal torus then there exists $g\\in \\B_2(k)$ such that $g\\varphi(T)g^{-1}=T$. Then the automorphism $\\psi:=\\Int_g\\circ \\varphi$ of $\\B_2(k)$ stabilizes $U$ as well as $T$. Hence there exists $\\alpha\\in k^\\times$ such that $\\psi\\left(\\begin{pmatrix}\n1&y\\\\0&1\n\\end{pmatrix}\\right)=\\begin{pmatrix}\n1&\\alpha y\\\\0&1\n\\end{pmatrix}$, for all $y\\in k$. Now the restriction of $\\psi$ on $T$ has two possibilities : either $\\psi(\\diag(t, t^{-1}))=\\diag(t, t^{-1})$ or $\\psi(\\diag(t, t^{-1}))=\\diag(t^{-1}, t)$ for all $t\\in k^{\\times}$.\n\n\\noindent\n\\textbf{Claim:} $\\psi(\\diag(t, t^{-1}))=\\diag(t, t^{-1})$ for all $t\\in k^{\\times}$.\n\n\\noindent \\emph{Proof of claim:} If possible let $\\psi(\\diag(t, t^{-1}))=\\diag(t^{-1}, t)$ for all $t\\in k^{\\times}$. Then we have $\\psi \\left(\\begin{pmatrix}a&b\\\\0&a^{-1}\\end{pmatrix}\\right)=\\psi\\left(\\begin{pmatrix}\na&0\\\\0&a^{-1}\n\\end{pmatrix}\\right) \\psi \\left(\\begin{pmatrix}\n1&ba^{-1}\\\\0&1\n\\end{pmatrix}\\right)$ $=\\begin{pmatrix}\na^{-1}&0\\\\0&a\n\\end{pmatrix}\\begin{pmatrix}\n1&\\alpha ba^{-1}\\\\0&1\n\\end{pmatrix}=\\begin{pmatrix}a^{-1}&\\alpha a^{-2}b\\\\0&a\\end{pmatrix}$ on $\\B_2(k)$. But clearly, this is not a homomorphism. Hence the claim. \n\nTherefore $\\psi=\\Id$ on $T$. \nThus, $R(\\psi|_{T})=R(\\Id)=\\infty$. Then, in view of  \\propref{solvable}, $R(\\psi)=\\infty$. By \\lemref{inner}, we have $R(\\varphi)=R(\\Int_g\\circ \\varphi)=R(\\psi)=\\infty$.\n Hence the group $\\B_2(k)$  has the $R_{\\infty}$-property although it is solvable. \n\\end{enumerate}"
"\\section{Introduction}\\label{sec:intro}\n\nEnergy efficiency is typically the most important challenge in advanced CMOS technology nodes. With the \\textit{dark silicon} problem~\\cite{Venkatesh:2010:CCR:1736020.1736044}, the vast majority of a large scale design is clock or power gated at any given point in time. Chip area becomes exponentially more available relative to power consumption, preferring ``a new class of architectural techniques that 'spend' area to 'buy' energy efficiency''~\\cite{taylor2012dark}. Memory architecture is often the most important concern, with 170-6400$\\times$ greater DRAM access energy versus arithmetic at 45 nm~\\cite{horowitz20141}.\n%Caches improve efficiency, but general purpose processors still face significant SRAM overheads relative to arithmetic energy.% when there is little local reuse of data in CPU registers.\nThis changes with the rise of machine learning, as heavily employed linear algebra primitives such as matrix/matrix product offer substantial local reuse of data by algorithmic tiling~\\cite{Wolfe:1989:MIS:76263.76337}: $\\mathcal{O}(n^3)$ arithmetic operations versus $\\mathcal{O}(n^2)$ DRAM accesses.\n%Other linear operations such as the form of convolution employed in convolutional neural networks (CNNs)~\\cite{LeCun:1989:BAH:1351079.1351090} provide even more local reuse.\nThis is a reason for the rise of dedicated neural network accelerators, as memory overheads can be substantially amortized over many arithmetic operations in a fixed function design, making arithmetic efficiency matter again.\n\nMany hardware efforts for linear algebra and machine learning tend towards low precision implementations, but here we concern ourselves with the opposite: enabling (arbitrarily) high precision yet energy efficient substitutes for floating point or long word length fixed point arithmetic. There are a variety of ML, computer vision and other algorithms where accelerators cannot easily apply precision reduction, such as hyperbolic embedding generation~\\cite{nickel2017poincare} or structure from motion via matrix factorization~\\cite{Tomasi:1992:SMI:144398.144403}, yet provide high local data reuse potential.\n\nThe logarithmic number system (LNS)~\\cite{swartzlander1975sign} can provide energy efficiency by eliminating hardware multipliers and dividers, yet maintains significant computational overhead with \\textit{Gaussian logarithm} functions needed for addition and subtraction. While reduced precision cases can limit themselves to relatively small LUTs/ROMs, high precision LNS require massive ROMs, linear interpolators and substantial MUXes. Pipelining is difficult, requiring resource duplication or handling variable latency corner cases as seen in~\\cite{coleman2008processor}. The ROMs are also exponential in LNS word size, so become impractical beyond a float32 equivalent. Chen et al.~\\cite{Chen2000} provide an alternative fully pipelined LNS add/sub with ROM size a $\\mathcal{O}(n^3)$ function of LNS word size, extended to float64 equivalent precision. However, in their words, ``[our] design of [a] large word-length LNS processor becomes impractical since the hardware cost and the pipeline latency of the proposed LNS unit are much larger.'' Their float64 equivalent requires 471 Kbits ROM and at least 22,479 full adder (FA) cells, and 53.5 Kbits ROM and 5,550 FA cells for float32, versus a traditional LNS implementation they cite~\\cite{Lewis1994} with 91 Kbits of ROM and only 586 FA cells.\n\nWhile there are energy benefits with LNS~\\cite{Popoff:2016:HLN:2971808.2972130}, we believe a better bargain can be had. Our main contribution is a trivially pipelined logarithmic arithmetic extendable to arbitrary precision, using no LUTs/ROMs, and a $\\mathcal{O}(n^2)$ precision to FA cell dependency. Unlike LNS, it is substantially more energy efficient than floating point at a 1:1 multiply/add ratio for linear algebra use cases. It is approximate in ways that an accurately designed LNS is not, though with parameters for tuning accuracy to match LNS as needed. It is based on the \\textit{ELMA} technique~\\cite{jhj}, extended to arbitrary precision with an energy efficient implementation of exp/log using restoring shift-and-add~\\cite{Muller:1985:DBC:4135.4987} and an ordinary differential equation integration step from Revol and Yakoubsohn\\cite{Revol1999AcceleratedSA} but with approximate multipliers and dividers. It is tailored for vector inner product, a foundation of much of linear algebra, but remains a general purpose arithmetic.\nWe will first describe our hardware exp/log implementations, then detail how they are used as a foundation for our arithmetic, and provide an accuracy analysis. Finally, hardware synthesis results are presented and compared with floating point.",,"\\section{Conclusion}\n\nModern applications of computer vision, graphics (Figure~\\ref{fig:raytrace}) and machine learning often need energy efficient high precision arithmetic. We present an novel dual-base logarithmic arithmetic applicable for linear algebra kernels found in these applications. This is built on efficient implementations of $e^x$ and $\\ln(x)$, useful in their own right, leveraging numerical integration with truncated mul/div.\nWhile the arithmetic is approximate and without strong relative error guarantees unlike LNS or floating point arithmetic, it is extendible to arbitrary precision, easily pipelinable and retains moderate to low relative error and low absolute error. The area/power tradeoff is certainly not appropriate for many designs, but can provide a useful alternative to high precision floating or fixed point arithmetic when aggressive quantization is impractical.\n\n\\textsc{Acknowledgments} We thank Synopsys for their permission to publish results on our research obtained by using their tools with a popular 7 nm semiconductor technology node."
"\\section{Introduction}\n\nMatrix and matrix-vector operations are fundamental operations in various scientific and engineering applications, including solving systems of linear equations \\cite{golub13}, signal processing \\cite{Ryan2019LinearAS}, scientific computing \\cite{Heath2018,Teukolski}, machine learning \\cite{aggarwal}, and control systems \\cite{Datta1994}. Various algorithms are available for inverting general matrices, such as Gaussian elimination, Gauss-Jordan elimination \\cite{Althoen1987}, Cholesky decomposition \\cite{Menon}, QR decomposition, and LU decomposition \\cite{Teukolski}. These algorithms, while common, are computationally demanding and typically involve cubic complexity in terms of the number of matrix-vector operations. State-of-the-art algorithms for matrix inversion will be reviewed in Sec. \\ref{sec:sota}.\n\nThese methods, while well-established, face significant challenges related to memory consumption, algorithmic complexity, and power efficiency, particularly as the size of the matrices increases. High-performance GPUs are known for their significant power consumption (300-350 W), which is a critical factor in designing energy-efficient computing systems \\cite{wang2021characterization} \\footnote{For instance, the NVIDIA GeForce RTX 3090 has an estimated power consumption of approximately 350 watts (W), while the NVIDIA GeForce RTX 3080 consumes around 320 W. Similarly, the AMD Radeon RX 6900 XT and AMD Radeon RX 6800 XT both have estimated power consumptions of approximately 300 W each.}. \n\nIn recent years, there has been growing interest in leveraging memristive crossbar arrays for computational tasks due to their potential for high-density integration, non-volatility, and low power consumption \\cite{yu2018}. Memristive devices (e.g. resistors with memory) are resistive switching devices that can be used to perform analog matrix-vector multiplications efficiently. These are promising candidates for implementing analog, neuromorphic, and other unconventional computing paradigms.\n\nOver the last few years, using the fact that Kirchhoff laws can be exploited to perform matrix-vector multiplication operations in one-shot,  memristive crossbars have been shown to be a promising platform to implement matrix operation algorithms. \n\nHowever, despite these promising developments, several challenges remain in realizing practical memristive crossbar-based matrix inversion. One major issue is the precision of the analog computations, which can be affected by device variability, non-idealities, and noise. Despite these challenges, research in crossbar arrays has flourished over the last decade \\cite{Alibart2013,Xia_2009,Jo_2010,Pi2018,Christensen2022,Xia2019,WanGeeKim2014,Ohba2018,Woo2018,Wang2018}, and state-of-the-art cross-point memristive memory has reached effectively 1024 (10 bits) states using a variety of noise-reduction techniques \\cite{Song_2024}. Although this precision is not enough yet for scientific computing, the methods currently implemented for noise reduction are scalable and promising for error mitigation in future technology. \n\nCrossbars are typically thought of as accelerators of matrix-matrix and matrix-vector multiplication \\cite{mehonic}. However, \\cite{Sun2019} present an innovative approach for matrix inversion utilizing cross-point resistive arrays. The process involves implementing the target matrix in a cross-point array circuit, where the input currents are applied, and the resulting output potentials are measured. This physical realization leverages the properties of resistive memory devices (RRAM) to perform matrix-vector multiplication (MVM) efficiently. The matrix inversion is achieved through a feedback mechanism with operational amplifiers that forces the output voltage to satisfy the equation $A \\cdot V + I = 0$, thus obtaining $V = -A^{-1} \\cdot I$. In practice, the authors solve linear equations $A \\vec x=\\vec b$ in one step, but this can be used to solve for matrix inverses in $N$ steps by carefully choosing $\\vec b$ at each iteration, where $N$ is the size of the matrix. The authors also demonstrated this method's accuracy and stability by comparing the experimentally measured inverse matrix with small matrices. \n\nWe ask whether it is possible to use a different method to solve a variety of problems at the same time. It is also crucial to note that the method in \\cite{Sun2019}, while very fast, requires a cross-point array whose junction conductances are already driven to represent $A$. Our method addresses this requirement by instead driving the physical state of the cross-point array to $A^{-1}$ using recursive iteration, which provides the desired answer but also yields an analog resource for $O(1)$ linear transformations of input signals/vectors using the resulting inverse matrix.\n\nMoreover, the scalability of memristive crossbar arrays is a critical factor that influences their applicability in large-scale matrix computations. As the size of the crossbar array increases, issues related to interconnect resistance, crosstalk, and power distribution become more pronounced. Crosstalk and sneak paths can be reduced dramatically using the 1T1R approach, standing for 1 transistor 1 resistor. Although there are other techniques (1 selector 1 resistor 1S1R, and 1 diode 1 resistor 1D1R), the 1T1R is the standard approach to avoid crosstalk in the experimental setup. We will use this technique in our study.\n\nIn Sec. \\ref{sec:sota} we provide a summary of previous works and a summary of the results obtained in this paper.  In Sec. \\ref{sec:results} introduce all the paper's main results. In Sec. \\ref{eq:algorithm} we introduce the algorithm we will be basing our results on as a dynamical system, and introduce the different variations to obtain different types of (pseudo-)inverses. In Sec. \\ref{sec:spice} we discuss the simulation scheme and SPICE that we use to implement the inverses by simulating the crossbar. In Sec. \\ref{sec:anres} we discuss analytical results and power consumption of the algorithm. In Sec. \\ref{sec:applic} we discuss applications, and in particular we consider parameter learning of Gaussian distributions, and online learning for reservoir computing. Conclusions follow. All the derivations of the results of this manuscript are provided in the Appendices.",,"\\section{Discussion}\n\nThe advent of parallel computing has shifted the focus towards developing efficient algorithms for large-scale, distributed matrix inversion. In this paper, we present several key results that highlight the advantages of utilizing memristive crossbar arrays for matrix inversion.\n\nOur main algorithm emulates a differential equation to iteratively update the resistance values in a memristive crossbar array, effectively solving the system of linear equations. We demonstrate that our algorithm converges reliably to the correct matrix inverse, leveraging the inherent properties of memristive devices. Moreover, we have provided error estimates in the presence of noise, and shown that the algorithm is stable in the present of delays, relevant to the case of volatile devices.\n\nWe validated our theoretical results through extensive simulations, showcasing the practical applicability and robustness of our approach. The empirical data supports our claims, showing that the memristive array-based method not only performs accurately but also offers considerable advantages in terms of speed and energy efficiency. Our simulations indicate that the cumulative power consumption converges exponentially to an upper bound, demonstrating the energy efficiency of our approach. The simulations were conducted using SPICE, ensuring realistic device behavior and providing a strong foundation for the practical implementation of the algorithm. \n\nOne of the key advantages of our method is its scalability. The number of operations required for convergence scales linearly with the matrix size, which is a significant improvement over traditional methods that often exhibit quadratic or higher-order complexity. This makes our approach particularly suitable for large-scale problems where conventional methods become computationally prohibitive.\n\nWe also highlight that our algorithm can handle both positive and negative entries in the matrix, which is a crucial capability for real-world applications. Theoretical results with noise show that our algorithm maintains stability and converges to the true inverse of the matrix under certain conditions. We provide upper bounds on the error, demonstrating that noise can be mitigated by adjusting the algorithm's pace (e.g. the applied voltage).\n\nFurthermore, we provide analytical results that underpin the convergence and stability of our algorithm, reinforcing the reliability of our approach. The ability to handle observational and process noise with analytical guarantees adds to the robustness of our method.\n\nOur analysis extends to non-invertible matrices, where we successfully apply our method to compute the Moore-Penrose inverse and the Drazin inverse. This flexibility underscores the robustness of our approach and its applicability to a wide range of linear algebra problems.\n\nMoreover, in terms of implementation schemes, we demonstrate that our algorithm can be adapted to solve both for $A^{-1}$ and for $N$ parallel instances of the type $A^{-1} \\vec b$\n when the matrix is invertible, offering versatility in its application. This dual capability allows for broader use in various computational contexts.\n\nThe numerical simulations we provided corroborate our theoretical findings, showing that our method performs well even with realistic device imperfections. The results indicate that memristive crossbar arrays can be effectively used for matrix inversion, providing a scalable, energy-efficient alternative to traditional methods.\n\nMost importantly, the present manuscript shows that the use of crossbar arrays with memristive devices can be used both for online and offline matrix inversion. We have tested these schemes on two applications, i.e. parameter learning of multi-variate Gaussian distributions and reservoir computing. Specifically, we have shown that our method rapidly converges to the optimal solution.  \n\nIn summary, our study demonstrates that memristive crossbar arrays offer a promising approach for efficient matrix inversion, with potential applications in various fields requiring large-scale linear algebra computations. The results presented in this paper provide a strong foundation for further research and development in this area. In our future work, we will focus on the physical implementation of the algorithm, exploring its practical deployment in more realistic case studies, and on-chip."
,,
,"""Recall that the invariant verification problem checks whether the algebraic set defined by a given set of input polynomials is an invariant for an input loop, though it may not be an inductive invariant. The study and motivation behind such non-inductive invariants have been  explored previously~\cite{humenberger2020algebra, kincaid2017reasoning, kovacs2008psolvable, bayarmagnai2024algebraic,  Muller-OlmS04, rodriguez2004inv}.\n\nAs discussed in~\cref{sec-overview}, a standard backward algorithm, used in similar settings in~\cite{benedikt2017polynomial,bayarmagnai2024algebraic,kauers2007equivalence,kauers2008solving}, provides  \na conceptually simple procedure for  invariant verification. \n%\nLet   $\langle M,\bm{\alpha}\rangle$ be a simple linear loop. Let \n $S\subseteq \QQ[\bm{x}]$ be a set of polynomials, written in the dense representation, with the  description size~$s$.\nDefine the sequence of nested ideals~$(I_i)_{i\in \NN}$ by\n\t$I_0= \langle S \rangle$ and $\n\tI_i = \langle P(M^j\boldsymbol{x}): P\in S, j\leq i \rangle$, as defined in~\eqref{eq:chain1}. \n%\nSince $\QQ[\boldsymbol{x}]$ is Noetherian,\nthis ascending sequence $I_0 \subseteq I_1  \subseteq I_2 \subseteq \cdots $\nof  ideals stabilises: there exists~$k$ such that $I_k= I_{k+j}$ for all\n$j\in \mathbb{N}$. Denote by $I_{\infty}$ the stabilising value of the\nsequence.\nThe algorithm tests whether $\bm{\alpha} \in  V(I_\infty)$. \nIf yes, then $V(S)$ is an algebraic invariant for $\langle M,\bm{\alpha}\rangle$; otherwise,  it is not. \n\n\n\n%\n%\n%\n\nSince $M\boldsymbol{x}$ is a linear transformation, the degree of\n$P(M^i\boldsymbol{x})$ is at most the degree of $P(\boldsymbol{x})$. \nFrom this observation we prove that \nif $\boldsymbol{\alpha}\not\in V(I_{\infty})$ holds then\nthere exists $k=  O(2^{s})$ such that  \n$\boldsymbol{\alpha}\not\in V(I_k)$.\nBelow, we use this bound to obtain a $\coNP$\nupper bound for invariant verification with the input ideal given in dense  representation.\n \n\lemweakverifycoNP*\n\begin{proof}\n\nRecall that $s$ denotes the description size of~$S$, and\nrecall \nthe chain of nested ideals  $(I_i)_{i\in \NN}$, defined above (and in \eqref{eq:chain1}). \nTo show the $\coNP$ membership,\n it suffices to provide a polynomial-time verifiable  certificate  \nshowing that\n $\boldsymbol{\alpha} \not \in V(I_{\infty})$ for negative instances of the problem. We first prove that\n \n\begin{restatable}{claim}{lengthchain}\n\label{claim:length}\nSuppose that $\boldsymbol{\alpha} \not \in V(I_{\infty})$. There exists $k =O(2^{s})$\nsuch that $\boldsymbol{\alpha} \not \in V(I_{k})$.\n\end{restatable}\n\begin{proof}\nBy definition,  the maximal degree~$D$  of polynomials in $S$ and the number~$d$ of variables  are both bounded by~$s$.  \nFor all $n \in \N$ and all polynomials~$P \in S$,  the degree of  $ P(M^n \boldsymbol{x})$ is at most~$D$, meaning that  \nthe ideal $I_n$ is generated by polynomials of degree at most $D$.\n\nLet $N\in \NN$ be such that $I_N = I_{N +1}$. This implies that $Q(M^{N +1}\boldsymbol{x}) \in \langle P(M^j\boldsymbol{x}) : P\in S, \, j \leq N \rangle$\nfor all~$Q\in S$.\nBut then $Q(M^{N +2}\boldsymbol{x}) \in \{ P(M^j\boldsymbol{x}) : P\in S,\,  j \leq N + 1\} = I_{N +1}=I_N$\nfor all~$Q\in S$.\nHence, using an inductive argument we can show $I_{N + j} = I_N$ for all~$j \in \N$. \n    Let $k$ be the smallest integer such that $I_{k} = I_{k +1}$, which implies $I_{k} = I_\infty$. Since $I_0 \subsetneq I_1 \subsetneq \cdots \subsetneq I_k$,  at each step $i$ we introduce at least one generator of $I_i$ that can not be expressed as a linear combination of the  generators of $I_{i -1}$. The generators of $I_i$ lie in the vector space of polynomials of degree at most $D$ with dimension~$\binom{D + d}{d}$. Therefore, $k \leq \binom{D + d}{d}$ which completes the proof of the claim.\n\end{proof}\n\n\t\n\t\nThis claim implies that \n  there exists $P\in S$ such that \n  $P(M^k\boldsymbol{x}) (\boldsymbol{\alpha})\neq 0$.  \nA $\coNP$ algorithm  guesses $P(\bm{x})\in S$,  an index $k =O(2^{s})$  and a prime $p$\nwith bitsize~$s$ such that $P(M^k\boldsymbol{x}) (\boldsymbol{\alpha})\neq 0$.\nTaking the binary representation of~$k$ into account and, by standard doubling techniques, \nthe algorithm constructs a small circuit \nfor~$P(M^k\boldsymbol{x})$  and checks whether $P(M^k\boldsymbol{x})\not \equiv 0$ (mod~$p$) in polynomial time.\nWe borrow the correctness of the latter test from the well-known fingerprinting procedure for the ACIT problem~\cite{allender2009complexity}.\n\n\nThe proof of {\coNP}-hardness is by a  reduction from  \(\mathsf{3SAT}\)  to the complement of the invariant verification problem. \nGiven a  \(\mathsf{3SAT}\) formula~$\Phi$, we construct a  loop~$\langle M, \bm{\alpha} \rangle$ with orbit~$\orbit$  and a polynomial~$Q(\bm{x})$ such that \n$\Phi$ is satisfiable if and only if $\orbit \not \subseteq V(\langle Q \rangle)$. \n\n\n\nLet $\Phi = \bigwedge_{i=1}^m C_m$ be in CNF  over \nvariables~$\{y_1, \dots, y_k\}$. Let $p_1 <  \dots < p_k$ be the first~$k$ primes. Define $D(i) \coloneqq  1+\sum_{j=1}^{i-1} p_j$ for $i\in \{1,\ldots,k\}$ and \n $d\coloneqq  \sum_{j=1}^{k} p_j$. \nConstruct $M \in \{0,1\}^{d\times d}$ and~$\bm{\alpha} \in \{0,1\}^d$ as follows:\n the entry $\alpha_{\ell}$  of $\bm{\alpha}$  is $1$ if, and only if, \n\t$\ell = D(i)$ for some $i \in \{1, \dots, k\}$. \nThe matrix~$M$ is a \n block-diagonal matrix $ M_1 \oplus \dots \oplus M_k$,\n\twhere $M_j$ is a permutation matrix of size~$p_j$.\nIntuitively speaking, for any $n \in \NN$ the vector $M^n\bm{\alpha}$ splits into $k$ blocks of prime size, and  exactly one entry in each block is set to~$1$. By the Chinese remainder theorem, for every $\bm{\beta} \in \{0,1\}^d$ with exactly one~1 entry in each block,\n\tthere exists~$n\in \NN$ such that $\bm{\beta} = M^n\bm{\alpha}$.\n\n We construct the polynomial~$Q$ over the vector $\bm{x}=(x_1,\cdots, x_d)$ of variables. \n For each $1 \leq i \leq k$, we interpret the variables $x_{D(i)}$ and $x_{D(i)+1}$ as \nliterals $y_i$ and $\neg y_i$. The other variables are called \emph{non-literal}.\nDefine $Q\in \QQ[\bm{x}]$ as follows \n\t\[Q (\bm{x}) \coloneqq  \left(\prod_{x_i\text{ non-literal}} (1-x_i)\right) \cdot \prod_{i=1}^m Q_i(\bm{x}),\] \n\twhere $Q_i \coloneqq  \sum_{j=1}^k t_{ij}^2$ with \n\[t_{ij} = \begin{cases}\n\t\tx_{D(j)+1} & \text{if $y_j$ appears in $C_i$,}\\\n\t\tx_{D(j)+2} & \text{if $\neg y_j$ appears in $C_i$,}\\\n\t\t0 & \text{otherwise.}\n\t\end{cases}\n\n\nThe correctness of reduction follows from   two simple observations:  \n$Q(\bm{x})$ vanishes on~$\bm{\beta}\in \{0,1\}^{d}$  if $\beta_i = 1$ for some non-literal entry $i$. Furthermore,  \nprovided that  all  non-literal entries are zero, the point~$\bm{\beta}$ is a zero of $Q(\bm{x})$ if and only if it corresponds to an unsatisfying assignment of~$\Phi$. \n\end{proof}""",
,,"The complexity of our algorithm for computing the orbit closure of simple linear loops primarily depends on finding a basis for the lattice of multiplicity relations between the eigenvalues of matrices. In particular, as argued in the proof of \\cref{theo-inver-pspace}, all other steps in our algorithm run in polynomial time. Ge's algorithm~\\cite{ge1993thesis} computes such a basis in polynomial time in the degree of the splitting field of the input algebraic numbers (i.e., the update matrix eigenvalues). For rational matrices, while the degree of the eigenvalues is bounded by the matrix dimension, the degree of the splitting field can grow exponentially with the dimension. We observed this in the earlier extension of \\cref{ex:splitting} to a family of $2k$-dimensional matrices, where the degree of the splitting field becomes $2^k$. Therefore, Ge's algorithm, in its current form, does not allow us to reduce the complexity from {\\PSPACE} to polynomial time."
"\\section{Introduction}\n\nFollowing Erd\\H os \\cite{Erdos}, Davenport and Roth \\cite{dav} we prove:  \n\\begin{Theorem}\n\\label{hanluc}\n   Let\n  $\\{a_n\\}_{n=1}^\\infty$ \n  be a non-decreasing sequence of positive integers such that\n    $\\lim\\limits_{n\\rightarrow\\infty} a_n^{\\frac{1}{3^n}} =\n    \\infty$ and $\\{ p_n\\}_{n=1}^\\infty$ be the increasing sequence of all primes. \n Then the continued fractions \n $$ \\Big[0;\\Big(1+\\frac {p_1}1\\Big)\\sqrt2,\\dots ,\\Big(1+\\frac {p_n}n\\Big)\\sqrt 2,\\dots\\Big], \\  \\  \\Big[0;a_1\\frac {p_1}1\\sqrt 2,\\dots ,a_n\\frac {p_n}n\\sqrt 2,\\dots\\Big]$$\n and number $1$ are linearly independent over $\\mathbb Q(\\sqrt 2)$ particularly  over $\\mathbb Q$ . \n\\end{Theorem}\nThis is an immediate consequence of Theorem \\ref{hanclJitu1.T1}, which will be introduced in the chapter Main Results. \nThe results presented in this paper have some history. Forty years ago Davenport and Roth in \\cite{dav} proved that the continued fraction $[a_1; a_2,\\dots]$, where $a_1,a_2,\\dots$ are positive integers satisfying \n$\\limsup_{n\\to\\infty}\\left((\\log \\log {a_n})\\frac{\\sqrt{\\log n}}{n}\\right)=\\infty,$\nis a transcedental number. Han\\v cl \\cite{hs2} found some criteria for continued fractions to be linearly independent. Andersen and Kristensen \\cite{and}  come with special conditions on continued fractions consisting of algebraic integers to be irrational or transcedental numbers. The generalization of transcendence is algebraic independence and there are several results concerning the algebraic independence of continued fractions, see, for instance, \\cite{bun}, \\cite{hs3}  or \\cite{hkn}. In 1975, Erd\\H os \\cite{Erdos} proved that if  $\\{a_n\\}_{n=1}^\\infty$ is a non-decreasing sequence of positive integers such that \n$\\lim_{n\\to\\infty} a_n^{\\frac 1{2^n}}=\\infty$ then the number $\\sum_{n=1}^\\infty \\frac 1{a_n}$ is irrational. Later, in 1991, Han\\v cl \\cite{han0} proved that if $\\{a_n\\}_{n=1}^\\infty$ is a sequence of positive real numbers such that $a_n\\leq 2^{\\frac 1{n^2}2^n}$ holds for any positive integer $n$, then there exists a sequence $\\{c_n\\}_{n=1}^\\infty$ of positive integers such that the number $\\sum_{n=1}^\\infty \\frac 1{c_na_n}$ is rational. Rucki \\cite{ruc} established a criterion for the sums of reciprocals of a sequence of natural numbers to be irrational. Gen\\v cev \\cite{gen1} obtained some irrationality results with the help of special transformations. Then Han\\v cl and Sobkov\\' a  \\cite{hs1} established the linear independence of the sums of certain infinite series.  Using Pad\\' e approximation Matala-aho and Zudilin \\cite{mz} obtained some interesting results in irrationality of infinite series. Recently, Han\\v cl and Kolouch \\cite{hk} gave a criterion for infinite products of rational numbers to be irrational. A nice review of these results can be found in \\cite{karf} and \\cite{kn}. \n\nOur results are of a quite general character and written in the spirit of Erd\\\",,
,"""\section{A very brief introduction to TDA}\n\nGiven a data set in the form of a point cloud, perhaps the scatterplot for a spreadsheet of numerical values, we're often interested in describing the perceived geometry of this point cloud. If the dimension of our point cloud (i.e. perhaps the number of columns of the spreadsheet) is at most three, then we can simply look at a scatter plot of the data points in $\mathbb{R}^n$ to visualize our point cloud and try to qualitatively assign geometric features. For instance, does it form clusters? Does it have a void or hole? These questions are not easily answered by the traditional statistical methods in an undergraduate curriculum. Using topology to study point clouds gives us new tools. Topology is an area of mathematics which is uniquely qualified to study the general shape of a point cloud without being too concerned with details such as angles, lengths, or linearity. In this paper, we explicitly share one of the topological tools developed to study data: \emph{persistent homology}. It has  proven itself useful in various applications, including the study of data in materials science of glass \cite{glass} and in neuroscience \cite{Be}, among others.\n\nTo illustrate how topology can be helpful, consider some examples of $2$-dimensional point clouds in Figure \ref{Fig:RegressionPlots} below.\n\n\begin{figure}[H]\n   \begin{minipage}{0.45\textwidth}\n     \centering\n     \includegraphics[width=\textwidth]{PlotA.png}\n   \end{minipage}\n   \begin{minipage}{0.45\textwidth}\n     \centering\n     \includegraphics[width=\textwidth]{PlotB.png}\n   \end{minipage}\n      \begin{minipage}{0.45\textwidth}\n     \centering\n     \includegraphics[width=\textwidth]{PlotC.png}\n   \end{minipage}\n        \caption{Scatterplots $A$, $B$, and $C$ with $R^2$ values of $0$, $0$, and $0.8447$, respectively}\label{Fig:RegressionPlots}\n\end{figure}\nThrough the lens of any usual form of regression (one of the main tools usually taught in undergraduate statistics or linear algebra), there would be no difference between plots $A$ and $B$, despite the fact that a quick glance would leave no doubt that these two data sets have very different structures. Studying these plots' {persistent homology} \cite{CZ} (or persistence modules) fails to distinguish plots $B$ and $C$. We mention this failure, where standard regression analysis  succeeds, in order to make it clear that topological data analysis is not a replacement of regression techniques but an additional tool. \n\n\nWe offer a friendly introduction to persistent homology through \v{Cech} cohomology. In Sections \ref{PC} and \ref{advanced}, we discuss how to extract topological information from a data set without requiring prior knowledge of topology concepts. Section \ref{computation} details the main ``by hand'' computational example.""",
"\\section{Introduction}\n\nAn approach to studying a $\\bG$-module $M$ for a connected affine group scheme $\\bG$\nover a field $k$ of characteristic $p > 0$ is to investigate the restriction of $M$ to Frobenius \nkernels $\\bG_{(r)}$ of $\\bG$.   From some\npoints of view, the representation theory of finite group schemes such as $\\bG_{(r)}$ resembles\nthe representation theory of finite groups and thus shares many useful properties.   The technique\nof restricting $\\bG$-modules to Frobenius kernels has been effective in studying irreducible modules\nand standard finite dimensional modules for reductive groups $\\bG$ (see, for example, \\cite{J}). \nOne method for studying the representations of finite group schemes $G$ involves constructing a \nsuitable theory of ``supports\",,
,"""\subsection{Gradient Sketch}\n\label{sec:sgd}\n\nWith SGD, we estimate the first-order (gradient) information by subsampling the components $\psi_i$ of the objective, and using those to construct an unbiased estimate $\hat\g_t$:\n    \begin{align*}\n      \x_{t+1} = \x_t - \eta_t\hat\g_t,\qquad\text{where}\quad\n      \E[\hat\g_t] = \nabla f(\x_t).\n    \end{align*}\nDespite its popularity, SGD has a number of limitations, e.g., large variance $\E\big[\|\hat\g_t-\g_t\|^2\big]$, which slows the convergence near the optimum, as well as\nsensitivity to hyper-parameters (such as step size, mini-batch size, etc.). \nRandNLA offers a number of techniques for addressing these\nlimitations, such as using weighted gradient sampling based on\n  leverage scores or other importance scores, as well as\n  using a sketching-based preconditioner.\nWe consider the following.\n\n\vspace{1mm}\noindent\textbf{Preconditioned Weighted SGD.}\nConsider the Preconditioned Weighted SGD (PW-SGD) method\n(which is shown here for the LS problem, $f(\x)= \frac1n\|\A\x-\b\|^2$, but which is applicable far beyond this setting \cite{durfee2018ell_1,agarwal2017second,gonen2016solving,svrn}):  \n\n   \begin{algorithmic}[1]\n    \State Compute $SA$ with some sketching operator $S$\n    \State Compute $R$ such that  $SA =QR^{-1}$ for orthogonal $Q$\n    \State Compute leverage score estimates $\tilde l_i$ for $A$\n    \For{$t=0$ to $T-1$}\n    \State Compute $g_t \leftarrow \frac1s\sum_{i=1}^s\frac Z{\tilde l_{I_i}}\nabla \psi_{I_i}(\x_t),\quad \Pr(I_i)\propto  \tilde l_{I_i}.$\n    \State Compute $x_{t+1}\leftarrow x_t - \eta_t RR^\top g_t$ \n    \EndFor\n  \end{algorithmic}\nThis algorithm uses a sketching operator $S$ to construct the $R$, which is the preconditioner of the problem, since its spectrum approximates the spectrum of (the inverse of) $\A$. \nMoreover, the algorithm uses leverage score estimates of $\A$,   %as the weights,\nwhich are used\nfor\nsubsampling the gradients (but this can be replaced by other sketching/sub-sampling schemes).\n\nThanks to a combination of preconditioning and importance sampling,\nthis version of PW-SGD can completely avoid any condition number\ndependence in its convergence rate \cite{chenakkod2023optimal}.\nIn particular, \n    suppose that the \textit{leverage score} estimates satisfy:\n    $\tilde l_i\geq l_i(\A)/\alpha$ for all $i$.\n    Then, letting $\eta_t:= \frac{\beta}{1+\beta t/8}$ for \n    $\beta = \frac{k/8}{k+4\alpha d}$, this version of PW-SGD %\ref{alg:least-squares}\n    satisfies: % the following:\n    \begin{align*}\n      \E[f(x_t) - f(x^*)] \leq \frac{f(x_0)}{1+st/(c\alpha d)}\qquad\forall_{t\geq 1}. \n    \end{align*}\n\n\noindent\nThe key advantage here is that the resulting iteration complexity $t=O(d/s\epsilon)$\nis entirely independent of the number of data points $n$ or of the\ncondition number $\kappa$ of matrix $\A$ \cite{chenakkod2023optimal},\nwhereas classical SGD may require as many as $O(n\kappa^2/s\epsilon)$\niterations. \nFor reaching a \textit{moderate precision} solution (the regime of greatest interest in ML and data science applications), the computational cost comparison shows that this method is faster than Sketch-and-Solve or Sketch-and-Precondition \cite{chenakkod2023optimal}.\n\nThere are also other RandNLA-based approaches for sketching the gradient\ninformation of the objective: \nin non-finite-sum settings, e.g., Sega \cite{hanzely2018sega};\nfor distributed/federated learning, e.g., FetchSGD \cite{rothchild2020fetchsgd}; \nmethods inspired by randomized coordinate descent \cite{needell2014stochastic,lee2013efficient};\nand randomized preconditioning for other stochastic gradient methods,\ne.g., Preconditioned SVRG and SVRN \cite{gonen2016solving,svrn}, and others \cite{gower2018tracking,liu2019acceleration}.\n\n\n\subsection{Hessian Sketch}\n\nRandNLA methods have proven particularly effective at efficiently extracting second-order information about the optimization objective \cite{subsampled-newton,pilanci2017newton,newton-less}. \nThis has led to many sketching-based Newton-type optimization algorithms. \nRecall that Newton's method represents the paradigmic second-order optimization algorithm, which minimizes a local quadratic approximation of the objective using gradient $\g_t=\nabla f(\x_t)$ and Hessian $\H_t=\nabla^2 f(\x_t)$:\n\begin{align*}\n  \x_{t+1} = \x_t + \argmin_\v\Big\{\g_t^\top\v +\n  \frac{\eta_t}2\v^\top\H_t\v\Big\} = \x_t-\eta_t\H_t^{-1}\g_t  .\n\end{align*}\n\n\noindent\nAs an example, consider a standard Generalized Linear Model~(GLM),\n%  \begin{align*}\n    $f(\x) = \frac1n\sum_{i=1}^nl_i(\a_i^\top\x) +\n    \frac\gamma 2\|\x\|^2$,\n%  \end{align*}\n  where $\a_i^\top\x$ represents a linear prediction associated with a data\n  point $\a_i\in\R^d$, and loss $l_i$ encodes the prediction error, dependent\n  on a label $y_i$. For instance, in logistic regression, we have $y_i=\pm 1$\n  and $l_i(\a_i^\top\x) = \log(1 + e^{-y_i\a_i^\top\x})$. Here, the\n  Hessian at $\x_t$ is given by:\n    \begin{align*}\n      \nabla^2 f(\x_t) = \frac1n\A^\top\D_t\A+\gamma\I,\n      \qquad \D_t := \diag(l_1''(\a_1^\top\x_t),...,l_n''(\a_n^\top\x_t)),\n    \end{align*}\nwhere the dominant cost is the $O(nd^2)$\nmatrix multiplication $\A^\top\D_t\A$. \nIt is natural to reduce this cost with\nRandNLA sketching or sub-sampling, by repacing $\A^\top\D_t\A$ with\n$\tilde\A_t^\top\tilde\A_t$ where $\tilde\A_t=\S_t\D_t^{1/2}\A$ for some\nsketching matrix $\S_t$.\nOne version of this leads to the\nNewton Sketch \cite{pilanci2017newton}:\n\begin{align*}\n  \x_{t+1} \n   = \x_t-\eta_t \hat\H_t^{-1}\g_t,\quad \hat\H_t = \frac1n\tilde\A_t^\top\tilde\A_t+\gamma\I.\n\end{align*}\nStandard RandNLA guarantees such as the subspace embedding are\nsufficient (although not necessary) to ensure that $\hat \H_t$ provides a good enough\napproximation to enable accelerated local convergence in time $\tilde O(nd)$.\n\nThese approaches have also been extended to distributed settings via\nRMT-based model averaging, \nwith\napplications in ensemble methods, distributed optimization, and federated learning\n\cite{lacotte2022adaptive,lacotte2020optimal,hessian-averaging,newton-less,gupta2021localnewton}.  \nFurther RandNLA-based Newton-type methods include:\nSubsampled Newton\n\cite{erdogdu2015convergence,subsampled-newton,bollapragada2018exact,berahas2020investigation};\nHessian approximations via randomized Taylor expansion\n\cite{agarwal2017second} and low-rank approximation \cite{frangella2023promise,derezinski2024faster}; Hessian\ndiagonal/trace estimates via Hutchinson's\nmethod \cite{meyer2021hutch++} and Stochastic Lanczos Quadrature, particularly for\nnon-convex problems,\ne.g., PyHessian \cite{yao2020pyhessian}, AdaHessian\n\cite{yao2021adahessian}; and finally\nStochastic Quasi-Newton type methods \cite{kovalev2019stochastic,mokhtari2018iqn}.\n\n\n\subsection{Sketch-and-Project}\n\n\nThe Sketch-and-Project framework has gained\nattention as a powerful methodology within the Iterative Sketching\nparadigm of RandNLA. While this framework has found applications across\nthe stochastic optimization landscape, it originally arose from\nrandomized algorithms for solving systems of linear\nequations.  \n\nSolving an $m\times n$ linear system $\A\x=\b$ can be viewed as an instance of the LS problem, i.e., minimizing the objective $f(\x) = \|\A\x-\b\|_2^2$. \nAs discussed in Sec.~\ref{sxn:least-squares}, Classical RandNLA has addressed this problem in the \emph{highly} over- or under-determined settings (i.e., when $\A$ is very tall or wide).\nHowever, in many applications, particularly high-dimensional\nsettings that arise in ML,\n$m$ and $n$ are (equal or) of comparable size.\n\nFor these nearly-square matrix problems, randomization can still be beneficial, in particular when using Modern RandNLA methods. \nFor instance, consider the classical Kaczmarz algorithm, which solves\na linear system of $m$ equations $\a_i^\top\x=b_i$ via the following\niterative procedure, starting from some $\x_0$:\n\begin{algorithmic}[1]\n    \For{$t=0$ to $T-1$}\n    \State Select index $I_t\in\{1,...,m\}$\n    \State $\x_{t+1}\leftarrow$ Project $\x_t$ onto the solutions of equation $\a_{I_t}^\top\x=b_{I_t}$\n    \EndFor\n  \end{algorithmic}\nThis simple procedure has been known for a long time \cite{kaczmarz37}, but only with the use of randomization are we able to characterize its convergence \cite{strohmer2009randomized}:\n    if the Kaczmarz method selects index $I_t$ randomly, with probability proportional to $\|\a_{I_t}\|_2^2$, then:\n    \begin{align*}\n      \E\,\|\x_t - \x^*\|_2^2 \leq \bigg(1 - \frac{\sigma_{\min}^2(\A)}{\|\A\|_F^2}\bigg)^t\|\x_0-\x^*\|_2^2.\n    \end{align*}\n  We can interpret this Randomized Kaczmarz (RK) method as a Weighted SGD algorithm \n  solving the finite-sum minimization problem \eqref{eq:finite-sum} with\n  $\psi_i(\x)=(\a_i^\top\x-b_i)^2$ \cite{needell2014stochastic}, \n  drawing a parallel with PW-SGD as described in Sec.~\ref{sec:sgd}. \n\nHowever, this paradigm becomes quite different from PW-SGD once we\nselect more than one equation at a time, giving rise to\nSketch-and-Project \cite{gower2015randomized}: \n\emph{Sample a random $k\times m$ matrix $S=S(t)$,\n  and project $\x_t$ onto the solutions of $\S\A\x=\S\b$:}\n  \begin{align*}\n    \x_{t+1} = \argmin_\x\|\x_t-\x\|_2^2\quad\text{subject to}\quad \S\A\x=\S\b.\n  \end{align*}\nWe recover RK if the matrix $\S$ is chosen to be the indicator vector of the equation $I_t$.\nHowever, this general framework (with Modern RandNLA tools from Sec.~\ref{sxn:modern-randnla}) allows for other natural choices,\nsuch as selecting blocks of equations (Block Kaczmarz, e.g., \cite{needell2013two,needell2014paved,rebrova2021block}) \nor sketching the input matrix $\A$ using any of the methods described earlier. \nThe Sketch-and-Project framework has been used to capture and extend other stochastic optimization methods, including Coordinate Descent\n\cite{leventhal2010randomized, lee2013efficient, gower2018accelerated}, and to develop more general purpose\nfirst- and second-order optimization algorithms \cite{gower2019rsn,kovalev2019stochastic,hanzely2020stochastic,yuan2022sketched}.\n      \nGoing beyond RK, the convergence analysis of Sketch-and-Project has proven much more challenging, largely due to the complex interdependence between the distribution of the sketching matrix $S$ and the spectrum of the input matrix $\A$. \nHowever, by using tools from Modern RandNLA theory \cite{rk20,randomized-newton,precise-expressions},\nwe can relate this convergence to the quality of the sketch $\S\A$ as\na low-rank approximation of the data \cite{derezinski2022sharp}:\n    \begin{align}\n      \E\,\|\x_t-\x^*\|_2^2 \ \lesssim\  \bigg(1 - \frac{k\sigma_{\min}^2(\A)}\n      {\E\|\A- \A\n      \P_{\S\A}\|_F^2}\bigg)^t\,\|\x_0-\x^*\|_2^2.\label{eq:implicit-preconditioning}\n    \end{align}\nHere, $\E\|\A - \A \P_{\S\A}\|_F^2$ is the projection error of\nthe low-rank approximation of $\A$ produced from the sketch $\S\A$,\nsame as in equation \eqref{eqn:LR_structural_result} from  Sec.~\ref{sxn:low-rank} (except sketched on\nthe other side). This analysis has revealed new classes of problems\n(often overlapping with ML domains that exhibit low-rank structure)\nfor which Sketch-and-Project methods offer dramatically improved peformance over\nthe more traditional Sketch-and-Precondition paradigm\n\cite{derezinski2023solving,derezinski2024fine}, thanks to the \n\textit{implicit preconditioning} phenomenon that is described by \eqref{eq:implicit-preconditioning}.""",
,,
\\section{Introduction}\nThe computation of Gr\\\,,
"Introduction\n\n%hey gave a new charaterization of rank oneo endomorphisms and also gave a new formula for the trace $\\tr(\\tau)$ of an endomorphism $\\tau$ in terms of the Galois-theoretic trace $\\tr$ as defined in \\cite{GQ2008}.  \n\nLet $K$ be a field (with arbitrary characteristic) admitting a Galois extension $L$ of degree $n$ and let $V$ be an $n$-dimensional $K$-space. Then $L$ may serve as a model for $V$. This allows for a new look on the fundamental notions and concepts of linear algebra in the light of Galois theory. Such was the goal of the study undertaken in \\cite{GQ2008} where there was a special focus on endomorphisms and their properties as well certain determinants that can be formed using the elements of the Galois group. Some of the results below require $L$ to be cyclic.\n\nWe begin by noting as in \\cite{GQ2008} that any linear functional $f \\in L^\\ast$ may be written in the form $ f = f_b$ for suitable $ b \\in L$, where $f_b(x) = \\tr (bx)$ and $\\tr$ is the trace map of $L/K$ defined as \n\\[ \\tr (x)= \\displaystyle \\sum_{\\sigma \\in \\gal(L/K)} \\sigma(x). \\] \nIndeed\n$L^\\ast = \\hom_K(L,K)$ is an  $L$-space with the action $f \\mapsto f.b$ where   \n\\[ f.b(x) = f(bx). \\]\nMoreover, it is clear that $\\dim_L(L^\\ast) = 1$ and as $\\tr$ is in general a nonzero functional in $L^\\ast$, therefore $L^\\ast = \\tr .L$.  Let $\\theta : L \\rightarrow L^\\ast$ be the map defined by $b \\rightarrow \\tr.b \\ (b \\in L)$. As the $\\tr$ bilinear form of a  Galois extension is non-degenerate it follows that $\\theta$ is an isomorphism of $K$-spaces. \nIn particular, $\\{\\tr .b_1, \\cdots, \\tr .b_n \\}$ is a K-basis of $L^\\ast$ whenever $b_1, \\cdots, b_n$ is a K-basis of $L$.  \n \n\nLet $V$ and $W$ be $K$-spaces. Recall that an element $z \\in V \\otimes_K W$ is called \\emph{decomposable} if it decomposes as \\[ z = x \\otimes y, \\quad x\\in V,y \\in W.\\]\nIn the following $\\otimes$ always means  $\\otimes_K$.  Recall that the space  $$ \\displaystyle\\mathcal{T}_{p}^{q}(V) =( \\otimes^p V) \\otimes ( \\otimes^q V^{\\ast})$$ is called the space of tensors of type $(p,q)$ on $V$. Given $a \\in V$ and $f \\in V^{\\ast}$, the \\emph{contraction} of the $(1,1)$-tensor $(a \\otimes f) \\in V \\otimes V^{\\ast}$ means $f(a)$.",,
"\\section{Introduction}\n\nModern computational science and engineering problems are often based on parametric partial differential equations (PDEs). The lack of analytical solutions for realistic engineering problems (heat transfer, fluid flow, structural mechanics, etc.) leads researchers to exploit advances in numerical analysis. The basic numerical methods for solving PDEs, such as finite element, finite difference, finite volume and meshless methods (e.g., smoothed particle hydrodynamics), result in a system of linear equations $Ax = b, \\,\\, A \\in \\mathbb{R}^{n \\times n}$, ~$x \\in \\mathbb{R}^n$, and $b \\in \\mathbb{R}^n$. These systems are usually sparse, i.e. the number of non-zero elements is $\\ll n^2$. Furthermore, some classes of parametric PDEs are characterized by a very large dimension of the parametric space and by a high variation of the parameters for a given sample.\n\nTypically, the application of parametric PDEs produces large linear systems, often with entries of varying scale, and therefore poses significant computational challenges. Projection Krylov subspace iterative methods are widely used to solve such systems. They rely on finding an optimal solution in a subspace constructed as follows: $\\mathcal{K}_r(A, b) = \\text{span}\\{ b, ~Ab, ~A^2b, \\dots, ~A^{r-1}\\}$.\n\nThe conjugate gradient (CG) method is used to solve large sparse systems with symmetric and positive definite (SPD) matrices~\\citep{saad2003iterative, axelsson1996iterative}. CG has a well-established convergence theory and convergence guarantees for any symmetric matrix. However, the convergence rate of CG is determined by $\\sqrt{\\kappa(A)}$, which is typically large. The condition number $\\kappa(A)$ of an SPD matrix $A$, defined in $2\\text{-norm}$, is a ratio between the maximum and minimum eigenvalues $\\kappa (A) = |\\lambda_{\\max}| \\big/ |\\lambda_{\\min}|$.\n\nReal-world applications with non-smooth high-contrast coefficient functions and high-dimensional linear systems separate eigenvalues and results into ill-conditioned problems with high $\\kappa(A)$. Decades of research in numerical linear algebra have been devoted to constructing preconditioners $P$ for ill-conditioned $A$ to improve the condition number in the form (for left-preconditioned systems) $\\kappa(P^{-1}A) \\ll \\kappa(A)$.\n\nThe well-designed preconditioner should tend to approximate $A$, be easily invertible and be sparse or admit an efficient matrix-vector product. The construction of a preconditioner is typically a trade-off between the quality of the approximation and the cost of storage/inversion of the preconditioner \\citep{saad2003iterative}.\n\nIn this paper, we propose a novel neural method of preconditioner design for symmetric positive definite matrices called PreCorrector (short for Preconditioner Corrector). Preconditioners constructed with PreCorrector have better effect on the spectrum than classical preconditioners by learning correction for the latter. Our contributions are as follows:\n\n\\begin{itemize}\n    \\item[1.] We propose a novel scheme for preconditioner design based on learning correction for well-established preconditioners from linear algebra with the GNN, which has better effect on spectrum than classical preconditioners.\n    \\item[2.] We propose an understanding of the loss function used with emphasis on low frequencies. We also provide experimental justification for the understanding of learning with such an objective.\n    \\item[3.] We propose a novel dataset generation approach with a measurable complexity metric that addresses real-world problems.\n    \\item[4.] We provide extensive experiments with varying matrix sizes and dataset complexities to demonstrate the superiority of the proposed approach and loss function over classical preconditioners.\n\\end{itemize}","""\section{Neural design of preconditioner}\n\label{sec:neural_prec_design}\n\n\paragraph{Related work}\nWhile there are a dozen different preconditioners in linear algebra, for example \citep{saad2003iterative, axelsson1996iterative}: block Jacobi preconditioner, Gauss-Seidel preconditioner, sparse approximate inverse preconditioner, algebraic multigrid methods, etc., the choice of preconditioner depends on the specific problem, and practitioners often rely on a combination of theoretical understanding and numerical experimentation to select the most effective preconditioner. Even a brief description of all of them is beyond the scope of a single research paper. One can refer to the related literature for more details\n\nThe growing popularity of neural operators for learning mappings between infinite dimensional spaces (e.g., \citep{hao2024newton, cao2024laplace, raonic2024convolutional}) is also present in recent work on using neural networks to speed up iterative solvers. The FCG-NO~\citep{rudikov2024neural} approach combines neural operators with the conjugate gradient method to act as a nonlinear preconditioner for the flexible conjugate gradient method~\citep{notay2000flexible}. This method uses a proven convergence bound as a training loss. A novel class of hybrid preconditioners~\citep{kopanivcakova2024deeponet} combines DeepONet with standard iterative methods to solve parametric linear systems. This framework uses DeepONet for low-frequency error components and conventional methods for high-frequency components. The HINTS~\citep{zhang2022hybrid} method integrates traditional relaxation techniques with DeepONet. It targets different spectral regions, ensuring a uniform convergence rate. It is also possible to use convolutional neural networks to speed up multigrid method~\citep{azulay2022multigrid, li2024machine}, which require materialization of sparse matrices into dense format. However, these approaches can suffer from the curse of dimensionality when applied to large linear systems and can be too expensive to apply at each iteration step.\n\nThe authors of \citep{li2023learning, hausner2023neural} present a novel approach to preconditioner design using GNNs that aim to approximate the matrix factorization and use it as a preconditioner. These approaches use shallow GNNs and typically require a single inference before the iteration process. GNNs take the initial left hand side matrix and right hand side vector as input and construct preconditioners in the form of a Cholesky decomposition. However, these GNNs cannot produce preconditioners that have a better effect on the condition number of the solving system than their classical analogues.\n\n\paragraph{Problem statement}\nWe consider systems of linear algebraic equations from the discretization of differential operators $Ax = b$ formed with a symmetric positive definite (SPD) matrix $A \succ 0$. One can use Gaussian elimination of complexity $\mathcal{O}(n^3)$ to solve small linear systems, but not real-world problems, which produce large and ill-conditioned systems.\n\n% The CG iterative solver is well suited for solving large linear systems with SPD matrices, but the CG converges poorly on ill-conditioned systems when $\kappa(A)$ is large.\n\n\paragraph{Preconditioned linear systems}\nBefore solving initial systems by iterative methods, we want to obtain a preconditioner $P$ such that the preconditioned linear system $P^{-1}Ax = P^{-1}b$ has a lower condition number than the initial system. If one knows the sparsity pattern of $A$, then possible options are incomplete LU decompositions (ILU)~\citep{saad2003iterative}: (i) with $p$-level of fill-in denoted as ILU($p$) and (ii) ILU decomposition with threshold with $p$-level of fill-in denoted as ILUt($p$). Additional information about these preconditioners can be found in the Appendix~\ref{app:ilu}.\n\nIn this paper we focus on the SPD matrices so instead of ILU, ILU($p$) and ILUt($p$) we use the incomplete Cholesky factorization IC, IC($p$) and ICt($p$). Further, we will form the preconditioners in the form of Cholesky decomposition~\citep{trefethen2022numerical} $P = LL^\top$ with sparse $L$ obtained by different methods. \n\n\paragraph{Preconditioners with neural networks} \nOur ultimate goal is to find such a decomposition that $\kappa((L(\theta)L(\theta)^\top)^{-1}A) \ll \kappa((LL^\top)^{-1}A) \ll \kappa(A)$, where $L$ is the classical numerical IC decomposition and $L(\theta) = \mathcal{F}(A)$ is an approximate decomposition with some function $\mathcal{F}$. Several papers~\citep{li2023learning, hausner2023neural} suggest using GNN as a function $\mathcal{F}$ to minimize certain loss function:\n\begin{equation}\n    \label{eq:F_as_GNN}\n    L(\theta) = \text{GNN}(\theta, A, b)\, .\n\end{equation}\n\paragraph{Loss function}\n\nThe key question is which objective function to minimize in order to construct a preconditioner. A natural choice, which is also used in \citep{hausner2023neural}, is: \n\begin{equation}\n    \label{eq:loss_minus_A}\n     \min \big \Vert P - A \big \Vert_F^2.\n\end{equation}\n\nBy design, this objective minimizes high frequency components (large eigenvalues), which is not desired. Low frequency components (small eigenvalues) are the most important because they correspond to the simulated phenomenon, when high frequency comes from discretization methods. It is also known that CG eliminates errors corresponding to high frequencies first and struggles the most with low frequencies. We suggest using $A^{-1}$ as the weight for the previous optimization objective to take into account low frequency since $\lambda(A) = \lambda^{-1}(A^{-1})$:\n\begin{equation}\n    \label{eq:loss_minus_A_invA}\n    \min \big \Vert (P - A) A^{-1} \big \Vert^2_F~.\n\end{equation}\n\nLet us rewrite this objective using Hutchinson's estimator \citep{hutchinson1989stochastic}:\n\begin{multline}\n    \label{eq:deriv_loss}\n    \big \Vert \big(P - A\big) A^{-1} \big \Vert^2_F = \big \Vert P A^{-1} - I \big \Vert_F^2 = \\\n    \Tr \Big( (PA^{-1} - I)^\top (PA^{-1} - I) \Big) = \\\n    \mathbb{E}_\varepsilon \Big[ \varepsilon^\top(PA^{-1} - I)^\top (PA^{-1} - I)\varepsilon \Big] = \\ \n    \mathbb{E}_\varepsilon \big\Vert (PA^{-1} - I)\varepsilon \big\Vert_2^2, \quad \varepsilon \sim \mathcal{N}(0,1).\n\end{multline}\n\nSuppose we have a dataset of linear systems $A_i x_i = b_i$, then the training objective with $\varepsilon = b_i, ~P = L(\theta)L(\theta)^\top$ and $A^{-1}_ib_i = x_i$ will be:\n\begin{equation}\n    \label{eq:loss_with_rhs}\n    \mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \big \Vert L(\theta)L(\theta)^\top x_i - b_i \big\Vert_2^2~.\n\end{equation}\n\nThis loss function has appeared previously in related research~\citep{li2023learning} but with an understanding of the inductive bias from the PDE data distribution. We claim that training with loss~(\ref{eq:loss_minus_A_invA}) allows to obtain better preconditioners than with loss~(\ref{eq:loss_minus_A}). In the Section~\ref{sec:Experiments}, we demonstrate that loss~(\ref{eq:loss_minus_A_invA}) does indeed mitigate low-frequency components.""","\\section{Discussion}\n\nIn our work, we propose a neural design of preconditioners for the CG iterative method that can outperform analogous classical preconditioners with the same sparsity pattern of the ILU family in terms of both effect on the spectrum and total time-to-solution. Using the classical preconditioners as a starting point and learning corrections for them, we achieve stable and fast training convergence that can handle parametric PDEs with contrast coefficients. We also propose a complexity metric to measure the complexity of PDEs with random coefficients.\n\nWe observe that training a GNN from scratch can be unstable, resulting in preconditioners that have a weaker effect on the spectrum than their classical analogues. In addition, as the matrix size increases, the very first step of training, when GNN is initialized with random weights, leads to an loss overflow since the residual with random $P$ is huge. Learning corrections to classical methods mitigates these problems.\n\nWe provide numerical evidence for our observation of low-frequency cancellation with the loss function used. However, we found no trace of this relationship in the numerical analysis literature. We believe that there exists a learnable transformation that will be universal for different sparse matrices to construct the ILU decomposition that will significantly reduce $\\kappa(A)$. We propose that this loss analysis is the key ingredient for successful learning of the general form transformation.\n\n\\section{Limitations}\n\nThe limitations of the proposed approach are as follows:\n\n\\begin{itemize}\n    \\item[1.] Theoretical study of the loss function used. We provide only a heuristic understanding with experimental justification for the loss function. A theoretical analysis of the loss function is the subject of future research.\n    \\item[2.] The target objective in norms other than $\\Vert\\cdot \\Vert_F$ may provide a tighter bound on the spectrum. Investigating the possible use of target values in other norms is a logical next step.\n    \\item[3.] Experiments on other meshes and sparsity patterns of the resulting left hand side matrices $A$. Generalization of the PreCorrector to transformation in the space of sparse matrices with general sparsity patterns.\n    \\item[4.] While the PreCorrector has only been tested on systems with SPD matrices from the discretization of elliptic equations, further work will require generalization to irregular grids, non-symmetric problems, hyperbolic PDEs, nonlinear problems, to other iterative solvers such as GMRES and BiCGSTAB and modification of the preconditioner design accordingly.\n    \\item[5.] The forcing term $f(x)$ is sampled from the standard normal distribution, but the case of complex forcing terms needs to be studied separately as it can also affect the complexity of solving parametric PDEs.\n\\end{itemize}"
"\\section{Introduction}\n\nIn this paper we propose a quantum algorithm for approximating multivariate\ntraces, i.e. traces of the products of matrices. Formally, the multivariate\ntrace of matrices $\\matA_{1},\\matA_{2},\\dots,\\matA_{k}$ of compatible\ndimensions is defined as:\n\\begin{equation}\n\\MTrace{\\matA_{1},\\dots,\\matA_{k}}k\\coloneqq\\Trace{\\matA_{1}\\matA_{2}\\cdots\\matA_{k}}.\\label{eq:multivariate-trace}\n\\end{equation}\nApproximating multivariate traces is motivated as follows. For a square\n$\\matA\\in\\R^{n\\times n}$, its matrix moments $\\STrace{\\matA^{k}}=\\MTrace{\\matA,\\dots,\\matA}k$\nfor $k=1,2,\\dots$, which reveal useful spectral properties of $\\matA$\nwith applications in scientific computing~\\cite{golub2009matrices}\nand other fields, are multivariate traces. Furthermore, by introducing\na set of shifts we have $\\MTrace{\\matA-\\alpha_{1}\\matI,\\dots,\\matA-\\alpha_{k}\\matI}k=\\Trace{p(\\matA)}$\nfor some polynomial $p(x)$ of degree $k$ and leading coefficient\n1. The right hand side $\\Trace{p(\\matA)}$ is equal to $\\sum_{i=1}^{n}p(\\lambda_{i})$\nwhere $\\lambda_{1},\\dots,\\lambda_{n}$ are the eigenvalues of $\\matA$,\nwhich is a spectral sum. Since many smooth functions $f(x)$ can be\napproximated well using polynomials~\\cite{trefethen2013approximation},\nit is a common practice to approximate spectral sums of the general\nform $\\sum_{i=1}^{n}f(\\lambda_{i})$ using restricted forms $\\Trace{p(\\matA)}$.\nA well known example is $\\log\\det\\matA=\\sum_{i=1}^{n}\\log(\\lambda_{i}$).\nMany machine learning techniques estimate spectral properties of various\nmatrices by approximating appropriate spectral sums via polynomial\nspectral sums $\\Trace{p(\\matA)}$, e.g. Gaussian Processes~\\cite{Rasmussen2006},\nkernel learning~\\cite{davis2007information}, Bayesian learning~\\cite{mackay2003information},\nmatrix completion~\\cite{candes2009exact}, differential privacy problems~\\cite{hardt2012simple},\ngraph analysis~\\cite{estrada2000characterization}, Hessian and neural\nnetwork property analysis~\\cite{ramesh2018backpropagation,ghorbani2019investigation}\nand many more~\\cite{ubaru2018applications,chen2021analysis}. Other\napplications of multivariate traces appear in computational physics,\ne.g. entanglement estimation~\\cite{horodecki2002method,leifer2004measuring,brydges2019probing,feldman-2022-entan-estim,quek2024multivariate},\nquantum error mitigation~\\cite{liang2023unified}, quantum distinguishability\nmeasures~\\cite{buhrman2001quantum}.\n\nDue to the ubiquity of matrix moments and spectral sums (and more\ngenerally multivariate traces), there is an extensive literature on\nefficient algorithms for spectral sum approximation. Example of classical\n(conventional) algorithms for spectral sum approximation\\footnote{In the context of this paper, whenever we talk about ``classical\nalgorithms'' we mean algorithms that use only classical computing,\nas opposed to ``quantum algorithm'' that also use quantum computing\n(but may have a classical component).} are~\\cite{han2017approximating,ubaru-2017-fast-estim}. Many of\nthese classical methods are based on (a) estimating multivariate traces\nusing randomized trace estimation techniques (Hutchinson's method\n\\cite{hutchinson1990stochastic} and related methods \\cite{bai1996some,avron2011randomized,roosta2015improved,meyer2021hutchpp,cortinovis2022randomized}),\nand (b) approximating the function or quadratic form using polynomial\napproximations, Lanczos Gaussian quadrature, histogram or some other\nrational approximation. While these methods are powerful and useful,\nwith the constant increase in the size of matrices encountered in\napplications, and ever increasing accuracy requirements (which lead\nto larger degrees in approximating polynomials), developing novel\nalgorithms for trace estimation is an active research topic.\n\nQuantum computers can efficiently perform certain linear-algebraic\noperations on large computational spaces (exponential in the number\nof qubits), and offer the potential to achieve significant speedups\nover classical computations. In recent years, a variety of quantum\numerical linear algebra (qNLA)~\\cite{prakash2014quantum,luongo2020quantum,gilyen2019quantum,cade2017quantum}\nand quantum machine learning (QML)~\\cite{schuld2019quantum,schuld2015introduction,biamonte2017quantum}\nmethods have been proposed to harness the computational power of quantum\ncomputing to achieve polynomial to exponential speedups over the best-known\nclassical methods. In this paper, we consider the problem of estimating\nmultivariate traces.",,
"\\section{Introduction}\n\n% About PDAEs\nPartial differential-algebraic equations (PDAEs), also  known  as infinite-dimensional descriptors \\cite{reis2008controllability},  singular distributed  parameter systems \\cite{ge2009exact, ge2013well}, and abstract DAEs \\cite{lamour2013differential} arise in many applications. Examples include fluid dynamics specifically  divergence-free fluid dynamics \\cite{lin1997sequential},  electrical networks \\cite{gunther2000joint}, nano-electronics \\cite{bartel2008concept}, biology \\cite{ahn2019global} and chemical engineering \\cite{leung2013systems}. The well-posedness of linear PDAEs has received  attention, for example, \\cite{sviridyuk2003linear, jacob2022solvability,erbay2024index,mehrmann2023abstract}  and the references therein. %A key aspect of this research is the decomposition of the state space to obtain a Weierstrass form  analogous to that for (finite-dimensional)  differential-algebraic equations (DAEs) \\cite{kunkel2006differential}. \n\n%Concept of index to make it clear for the reader when mentioning consistent initial condition admissible controls avoid distributions.\n\nVarious  definitions for the index of a PDAE exist in the literature\\cite{campbell1999index, martinson2000differentiation, wagner2000further, rang2005perturbation}. Unlike (finite-dimensional) DAEs \\cite{kunkel2006differential}, the different definitions of indices for PDAEs are not equivalent \\cite{erbay2024index}. The concept of the index in PDAEs is crucial, serving as a key indicator of the numerical challenges expected when solving such equations. Moreover, it plays a  role in the assignment of initial and boundary conditions to ensure the consistency relation that results from the algebraic component of the system\\cite{martinson2000differentiation}.  \nFailure to meet this relation leads to distributions in the solution of the PDAE \\cite{ge2019approximate}. \n%Motivation and discussion of the existing literature for (finite-dimensional) DAEs \n%\\todo{rewrite lit review. You need to mention all author names or none.}\n\nController synthesis methods for PDAEs are needed in order stabilize their dynamics  and/or to  achieve   desired performance. One  valuable technique in  controller design is linear quadratic (LQ) control. Researchers have made progress in establishing LQ controller design for DAEs in finite-dimensional spaces, addressing index-1 \\cite{bender1987linear,mehrmann1991autonomous} as well as  general higher-index \\cite{reis2019linear,polezhaev2003spatial}. Using a singular value decomposition and calculus of variations, Bender and Laub \\cite{1104694} studied the optimization problem for index-1 DAEs on a finite and infinite horizons. Moreover, to solve for the optimal control,  Bender and Laub \\cite{1104694} have derived several differential  Riccati equations.  Mehrmann \\cite{mehrmann1991autonomous} studied the finite-time LQ control of the same problem and showed that the existence of a unique continuous optimal control depends on the solvability of a two-point boundary value problem. This optimal control was assumed to satisfy  the  consistency relation on the initial condition.  On the other hand, Reis and Voigt \\cite{reis2019linear}  used a behavior-based approach to study  optimal control for  DAEs with arbitrary index. Petreczky and Zhuk \\cite{petreczkysolutions} also used behaviors to study optimal control for linear DAEs that are not regular.  \n\nOptimal control for DAEs on infinite-dimensional spaces, that is, PDAEs, contains many open problems. In this context, we mention the work done by  Grenkin et al.  \\cite{grenkin2016boundary} where they tackled the boundary optimal control problem  for a heat transfer model consisting of a coupled transient and steady-state heat equations. Grenkin et al.  \\cite{grenkin2016boundary} showed the existence of weak solutions of this optimization problem under certain assumptions but without taking into consideration  the initial condition's consistency.   More recently,  Gernandt and Reis \\cite{gernandt2024linear} studied   LQ optimal control for a class of PDAEs with resolvent index-one in the pseudo-resolvent sense by considering the mild solutions of the system.   Under specific conditions on the Popov operator, and provided that the initial algebraic sub-state of the system adheres to a certain algebraic constraint, Gernandt and Reis \\cite{gernandt2024linear} showed the existence of unique minimizing control in the space of square integrable functions.    The optimal cost was shown to  be determined  by a bounded Riccati operator.\n\n\n%Contibutions\nThis paper studies  the LQ control problem over a finite-horizon for a class of linear PDAEs, those with  {\\em radiality-index 0}.  (The radiality index will be defined in the next section.)  Many  equations arising in applications, such   a parabolic-elliptic systems and also the equation used to model the free surface of seepage liquid have  {\\em radiality-index } 0 \\cite{jacob2022solvability} have index zero.\nWith  proofs different from those for finite-dimensional systems  \\cite{mehrmann1991autonomous, reis2019linear}, a fixed-point argument is used to show that there is a unique continuous optimal control, and this control can  written in feedback form.  We do not impose assumption on the initial values such as been done in \\cite{gernandt2024linear}. Instead, by defining a set of admissible controls for the optimization problem, we demand that the control signal maintain the consistency of the initial conditions, thereby preventing distributions in the solutions \\cite{ge2019approximate}. Next,  we derive a coupled system consisting of a differential Riccati-like equation and an algebraic equation that leads to the optimal control. It is important to note that specific projections are involved in the  derivation's proof process. However, after the state weight's decomposition through these projections is established, there is no  need to use these projections in computing the optimal control. This approach draws inspiration from the works of Heinkenschloss \\cite{heinkenschloss2008balanced} and Stykel \\cite{stykel2006balanced}, where they developed a Lyapunov equation within the study of balanced truncation model reduction for specific finite-dimensional systems. Similarly, Duan \\cite{duan2010analysis}  derived a Lyapunov equation for a particular class of index-1 DAEs.  Breiten et al. \\cite{breiten2019feedback} studied optimal control of the Navier-Stokes equations and derived a certain Riccati equation.  In fact, our work marks a first effort towards deriving differential Riccati-like equations for a general class of differential-algebraic equations, without needing to know the projection explicitly. We demonstrate that although  projections can be a valuable tool in deriving the desired equations, they may not be needed for computation of the control. Finally,  we illustrate the approach  by design of an LQ-optimal control for  an unstable coupled parabolic-elliptic system \\cite{ParadaCerpaMorris,Alalabi&Morris}. \n\nThe paper is structured as follows: In Section \\ref{section1}, we formulate the problem, and define the class of PDAEs. Section \\ref{section2} establishes the existence of an optimal control for the finite-horizon optimization problem. The derivation of a differential Riccati equation, essential for determining this optimal control, is elaborated  in Section \\ref{section3}.  Finally, in Section \\ref{section5}, numerical simulations are given to illustrate the theoretical results.",,"\\section{Conclusions}\n\nThis paper  extends the classical finite-time linear quadratic control problem for finite-dimensional DAEs into the  infinite-dimensional case. We showed the existence of  a continuous optimal control that ensures the consistency of the initial conditions while  minimizing the cost functional. Decomposing  the PDAE into a  Weierstra$\\beta$  canonical form was crucial in the proofs. \nHowever, the optimal control can be calculated  derived differential Riccati equation without projecting of any operators other than the state weight.\nFuture work will consider the minimization problem on an infinite-time horizon. \n%On an infinite-horizon,  with the assumption that the system is exponentially stabilizable,   we derived couples algebraic equations whose  minimal non-negative solution determines the minimal cost. We show that exponential detectability   ensures the existence of a unique  solution for coupled AREs, and that the optimal control stabilizes the dynamical part  in the long-term.  Decomposing  the PDAE into a structure similar to the Weierstra$\\beta$  canonical form for DAEs, was crucial in our research outcomes.  \n\nThe class of PDAEs  considered here was restricted to those with radiality-index  zero. We aim to extend the results to higher-index PDAEs in future research. This already been done \\cite{ACC} for finite-dimensional DAEs without the use of behaviors.\n%\\cite{reis2019linear}.\nAnother prospective research problem is to examine the linear quadratic control problem for linear PDAEs in situations where the operator associated with the penalization on the $L_2$-norm of the control input is not necessarily invertible, since as illustrated in \\cite{reis2019linear} a unique solution can exist for the optimization problem in the finite-dimensional situation.  Finally, since  the derived differential Riccati equation is an  operator equation on an infinite-dimensional Hilbert space, numerical treatment for solving needs further attention."
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
,,
"\\section{Introduction}\nThis article is about doing old things in a new way. It is aimed at mathematicians who teach linear algebra, and it is laid out in a fairly formal way, for efficiency sake.\\footnote{It is a useful feature of the arguments that they are short and are easily converted into a concrete and visual form that students find pleasantly comprehensible and convincing, despite the absence of full generality and rigorous formalism. This is how the material is presented in our forthcoming textbook ``Not Your Grandpa's First Introduction To Linear Algebra.''} The reader who decides to embark on this trip is advised to exercise a touch of amnesia; otherwise the tenets of the standard approach will not let go of the mind.\n\nModern developments suggest that initial focus in introductory linear algebra should be placed on $\\R^{n}$ and $\\C^{n}$, with concepts of orthogonality and least squares playing an earlier and more central role, and Singular Value Decomposition done justice within the first course. To be able to implement this without skipping arguments that justify claims, or replacing proper development with commandments, something has to go overboard. \n\nOur approach establishes basic fundamental results quickly, bypassing traditional stumbling blocks and time sinks. \nThe gained efficiency is due to an early introduction of a combinatorial concept of right/left dimension of a subspace, requiring no prior exposure to linear independence, linear systems or row-reduction.\n\nIn a course, the material that should precede our point of departure, would include an introduction to the operations on $\\R^{n}$ (or $\\C^{n}$, or $\\F^{n}$), the concept of a subspace, and the fact that spans and ortho-complements generate subspaces. No  more than that.\n\n\nThe article is organized as follows. In section 2 we introduce right/left-standard indices. This is a new concept that serves as a trailhead for the shortcut. Section 3 deals with the right/left-basic elements of subspaces and canonical coordinate systems. Section 4 treats dimensionality.  In section 5 we connect the canonical coordinate systems of a subspace with those of its ortho-complement.\n\nThe material in sections 6 and 7 assumes some rudimentary familiarity with matrix algebra, including transposition. Again, only the basics are required. In these sections we apply the new methods to establish the equality of row and column ranks, Rank-Nullity theorem, full rank factorization, and existence/uniqueness of the RREF, all as immediate by-products. All of this happens quickly and at a low cost.\n\nSection 8 describes a recursive form of Gauss-Jordan elimination reminiscent of Gram-Schmidt process, that can be used to construct the RREF of a matrix.\n\nSection 9 of the article deals with a combinatorial question that is not usually a part of the course. If one were to mark all of the positions where the elements of a subspace have their first non-zero entry and their last non-zero entry, what configurations can be thus produced?",,"\\section{Concluding Remarks}\n\nAccording to John Baez' insight \\cite{Baez}, a good mathematical article should not end abruptly, and we concur. \n\nOriginally we were motivated by a search for a palatable argument showing the uniqueness of RREF for the students in the first linear algebra course at our home institution. At the same time we had grown weary of  our students fixating on row reduction and the RREF, making these a knee-jerk response to almost any linear-algebraic question. Given the  centrality of the orthogonality-related notions in contemporary applications, this just won't do.\n\nFurthermore, it is counterproductive to let students think that proficiency in carrying out Gauss-Jordan elimination or a determinant calculation by hand is a sign of valuable intellectual accomplishment. Present day professionals perform such procedures by hand just about as often as they extract square roots by hand or graph complicated functions with pencils on paper. Proficiency in reducing a quest to Gauss-Jordan elimination (or a determinant calculation), along with an ability to use computing systems, and having the skills to interpret the results, is of course a different matter.\n\nIt has to be mentioned that we have only tested ``in a jiffy'' approach at our home institution, which benefits from having strong and motivated students. It works well and is indeed efficient. The approach is most effective when the emphasis is placed  on comprehension of the ideass, and the general proofs are scaled down to more visual and concrete arguments that can be played with in real time. \n\n\nIt is our hope that the right/left-standard bases tack can be adopted more broadly, and that it finds a regular place in the toolbox of modern mathematical educators.\\Vm{3}\n\n\\textit{Acknowledgements: We are very grateful to Professors Fernando Gouvea and Scott Taylor, who had kindly read the first version of the article. Their insightful suggestions prompted a significant improvement, as did the blog post by John Baez which Fernando Gouvea had brought to my attention. We would also be remiss not to thank those Colby students whom we had subjected to early versions of the ideas in this article. Their questions and comments contributed greatly to a refinement of the approach.}"
,,
"\\section{Introduction} \nIn this note, we consider abstract differential-algebraic systems in Hilbert spaces of the form\n\\begin{equation}\n\\begin{split}\n\\label{system}\n\\tfrac{{\\rm d}}{{\\rm d}t}Ex(t)&=Ax(t)+Bu(t),\\quad Ex(0)=Ex_0,\\\\ y(t)&=Cx(t),\\quad t\\geq 0,\n\\end{split}\n\\end{equation}\nwhere $E\\in L(X,Z)$, $C\\in L(X,Y)$ and $B\\in L(U,Z)$ are bounded linear operators and $A: X\\supseteq D(A)\\rightarrow Z$ is closed and densely defined and $X$, $Z$, $U$ and $Y$ denote the Hilbert spaces of states, images of states, inputs and outputs, respectively. \n\nThe specific challenge arises from the fact that, in numerous practically motivated instances, as observed in \\cite{MehrZwar23,Reis07,Reis06,ReisTisc05}, the operator $E$ possesses non-trivial kernel and co-kernel. Examples of such scenarios include abstract Cauchy problems incorporating additional closure relations, physical problems characterized by vanishing material parameters or densities, or systems of coupled partial differential equations (PDEs) like heat-wave couplings formulated in an abstract differential-algebraic form.\nSubject to the dynamics \\eqref{system}, our objective is to minimize the quadratic cost on some time horizon $t_f>0$ given by \n\\begin{align}\n\\label{quadcost}\nJ(u, y) = \\int_0^{t_f} \\left<\\begin{pmatrix}y(t)\\\\ u(t)\\end{pmatrix},\\begin{bmatrix}\\mathcal{Q}(t) & \\mathcal{N}^*(t) \\\\ \\mathcal{N}(t) & \\mathcal{R}(t)\\end{bmatrix}\\begin{pmatrix}y(t)\\\\ u(t)\\end{pmatrix} \\right>_{Y\\times U} {\\rm d}t\n\\end{align}\nwhere $\\mathcal{Q}\\in L^\\infty([0,t_f],L(Y))$, $\\mathcal{R}\\in L^\\infty([0,t_f],L(U))$, $\\mathcal{N}\\in L^\\infty([0,t_f],L(Y,U))$ (that is, $\\mathcal{Q}$, $\\mathcal{R}$ and $\\mathcal{N}$ are bounded functions with values in the space of linear operators), and additionally, $Q$ and $R$ are pointwise self-adjoint. This minimization is performed over all solutions of \\eqref{system} with $u\\in L^2([0,t_f],U)$ and $y \\in L^2([0,t_f],Y)$.\n\nAs a main assumption, we use that the abstract differential-algebraic system has index at most one and is stable in the pseudo-resolvent sense, meaning that the following holds\n\\begin{equation}\\label{eq:resgrowth}\n\\exists\\, M, \\omega>0:\\quad\\|(A-\\lambda E)^{-1}E\\|\\leq \\frac{M}{\\lambda-\\omega} \\quad \\text{ $\\forall\\,\\lambda>\\omega$.}    \n\\end{equation}\nThe incorporation of this index condition, alongside an additional stability assumption, enables us to relate our setup to the optimal control framework for standard infinite-dimensional systems presented in \\cite{WeisWeis97}.\n\nFurther, we assume that the {\\em transfer function} $G(s)=C(sE-A)^{-1}B$ is bounded in the complex right half-plane, that is\n\\begin{equation}\n    C(\\cdot E-A)^{-1}B\\in H^{\\infty}.\\label{eq:GHinf}\n\\end{equation}\nThis condition guarantees that the output of the system \\eqref{system} depends continuously on the input in the $L^2$-sense. We will further discuss how to eliminate these assumptions towards the conclusion of this article.\n\nOur findings are inspired by previous results for finite-dimensional differential-algebraic equations \\cite{ReisVoig19}, see also \\cite{Mehr91} for an overview in the ODE case. Furthermore, we use the solution theory that is developed for inhomogeneous ADAEs of finite pseudo-resolvent index \\cite{GernReis23} to define Popov operators for ADAEs. These operators can be used so solve the LQ problem on finite-time and infinite time-horizons. Recently, a different approach to LQ optimal control of ADAEs with radiality index was presented in \\cite{AlaM24} using the solutions of differential Riccati equations, see also \\cite{ErbJMRT24,GernReis23} for a comparison of the different index concepts.\n\nIn this note, we consider also the infinite horizon LQ problem, and here it is necessary to have exponential stability of the underlying degenerate semi-groups. Therefore, we also provide Lyapunov-like characterizations of exponential stability in terms of the pseudo-resolvents and in terms of the coefficients of the ADAE. \n\nThe paper is organized as follows: in section~\\ref{sec:prelim} we recall some preliminaries on pseudo-resolvents, mild solutions and degenerate semigroups. In section~\\ref{sec:stability} we characterize the exponential stability of abstract differential-algebraic equations having pseudo-resolvent index at most one. The previous results are used to provide a solution to the linear quadratic optimal control problem in section~\\ref{sec:lqr} using Popov operators.",,
"\\section{Introduction}\\label{sect:intro}\\noindent\n\nThroughout $A$ and $B$ shall denote complex unital Banach algebras, where we will always use $\\mathbf{1}$ to denote the identity of an algebra under consideration. The group of invertible elements of a Banach algebra $A$ shall be denoted by $G(A)$. For any element $x$ in a unital Banach algebra $A$, recall that its spectrum is given by $$\\sigma(x) = \\{\\lambda \\in \\mathbb{C}: \\lambda \\mathbf{1} - x \\notin G(A)\\}.$$ \nMoreover, we use $\\sigma'(x) = \\sigma(x)\\setminus\\{0\\}$ and $\\rho(x) = \\sup_{\\lambda \\in \\sigma(x)}|\\lambda|$ to denote its nonzero spectrum and spectral radius, respectively. For the Jacobson radical of a Banach algebra $A$ we shall write $\\rad(A)$. We say that $A$ is semisimple if $\\rad(A) = \\{0\\}$. If $A$ is semisimple, then it has a smallest two-sided ideal containing all minimal left and right ideals, called the socle of $A$ which we denote by $\\soc(A)$. Lastly we mention that if $A$ is semisimple, then $\\soc(A)$ is essential if and only if its left annihilator is trivial.\n\nThe study of linear mappings preserving the invertibility or eigenvalues of matrices can be traced back to Dieudonn\\'{e} in \\cite{dieudonne1948generalisation}. Let $M_n(\\mathbb{C})$ denote the Banach algebra of all complex $n \\times n$ matrices. Building on the work of Dieudonn\\'{e}, Marcus and Purves showed in \\cite{marcus1959linear} that if a linear map $\\varphi: M_n(\\mathbb{C}) \\to M_n(\\mathbb{C})$ preserves invertibility (that is, $x \\in G(M_n(\\mathbb{C})) \\implies \\varphi(x) \\in G(M_n(\\mathbb{C}))$), then there exist $u, v \\in G(M_n(\\mathbb{C}))$ such that\n\\begin{equation}\n\t\\varphi(x) = uxv \\mbox{ for all } x \\in M_n(\\mathbb{C})\\;\\; \\mbox{ or }\\;\\; \\varphi(x) = ux^tv \\mbox{ for all } x \\in M_n(\\mathbb{C}),\n\t\\label{eqform}\n\\end{equation}\nwhere $x^t$ denotes the transpose of $x$. Of course, if the mapping $\\varphi$ above is unital, that is, $\\varphi(\\mathbf{1}) = \\mathbf{1}$, then $u = v^{-1}$. Notice also that one does not need to assume here that $\\varphi$ is surjective (since injectivity can be established from the hypotheses). Motivated by the preceding result of Marcus and Purves, and by Kaplansky's famous problem, Sourour classified in \\cite{Sourour1996} the form of any bijective linear invertibility preserving map between $\\mathcal{L}(X)$ and $\\mathcal{L}(Y)$, where $\\mathcal{L}(X)$ and $\\mathcal{L}(Y)$ are the Banach algebras of all bounded linear operators acting on the Banach spaces $X$ and $Y$, respectively. This was further generalized by Bre\\v{s}ar, Fo\\v{s}ner, and \\v{S}emrl in \\cite{bfs2003}, where it is shown that if $\\varphi: A \\to B$ is a unital bijective linear mapping that preserves invertibility, with $A$ and $B$ semisimple and $\\soc(A)$ essential, it follows that $\\varphi$ is a Jordan isomorphism; that is, a linear bijective map satisfying $\\varphi(x^2) = \\varphi(x)^2$ for all $x \\in A$. In more recent years there has been much interest in general preserver problems, where the condition of linearity is removed from the hypotheses and the preserving condition is replaced by something which can potentially be used to recover linearity (or semi-linearity) in the conclusion. For example, it is easy to see that a linear spectrum preserving map $\\varphi:A \\to B$ has the property that $\\sigma(\\lambda x + y) = \\sigma(\\lambda \\varphi(x) + \\varphi(y))$ for any $x, y \\in A$ and $\\lambda \\in \\mathbb{C}$. However, if $\\varphi$ is not assumed to be linear, can one recover linearity from the preserving property alone? For recent work in this direction see \\cite{Askes2022} and \\cite{Benjamin2023}.\n\nIn this note we shall study pairs of surjective mappings $\\varphi, \\psi: A \\to B$ preserving the invertibility of linear pencils in both directions, that is, which satisfy the property that\n\\begin{equation}\n\t\\lambda x+y \\in G(A) \\iff \\lambda \\varphi(x) + \\psi(y) \\in G(B) \\mbox{ for all }x, y \\in A \\mbox{ and }\\lambda \\in \\mathbb{C}.\n\t\\label{eqcond}\n\\end{equation}\nOur work is motivated by a recent paper of Costara, \\cite{COSTARA2020216}, which generalizes a result on determinant preserving mappings by Dolinar and \\v{S}emrl in \\cite{DOLINAR2002189}, and deals with our situation in the setting of complex matrices, i.e. the case where $\\varphi, \\psi$ satisfy \\eqref{eqcond} and $A = B = M_n(\\mathbb{C})$. Remarkably, if only \\textit{one} of the mappings in this case is either surjective or continuous, then \\cite[Theorem 1]{COSTARA2020216} says that $\\varphi = \\psi$ and $\\varphi$ takes one of the forms in \\eqref{eqform}. Our aim here is to show that if a pair of surjective mappings $\\varphi, \\psi: A \\to B$ satisfy \\eqref{eqcond}, where  $A$ is semisimple with $\\soc(A)$ essential, then $\\varphi = \\psi$ and $\\varphi: x \\mapsto uJ(x)$, where $u \\in G(B)$ and $J: A \\to B$ is a Jordan isomorphism. Of course, since our consideration includes infinite-dimensional Banach algebras (in particular, $\\mathcal{L}(X)$ with $X$ of any dimension), and since we do not place any restrictions on $B$, our assumption that \\textit{both} mappings are surjective seems reasonable.",,
"\\section{INTRODUCTION} \nSucceeding in college mathematics requires developing many skills students may not anticipate they will need. As college math instructors, we desire to help students learn such important skills as how to collaborate, how to\nexplore and read math by themselves, and how to link together different ways of understanding a concept. We also want our students to develop a positive relationship with math. On the other hand, college instructors in introductory classes often face demands from their institution to cover a certain curriculum in a certain amount of time, and these restrictions can make it hard to teach students the skills we believe they need to succeed in college. To make this task even more challenging, students have widely varying backgrounds as they enter college, even as they take the same few introductory classes.\n\nIn this paper we share activities from a linear algebra class we designed for a summer bridge-to-college program for incoming first-year students. We focus on innovative, engaging tasks designed to go beyond the dichotomy between addressing students' needs and dealing with institutional constraints. Our discussion of each activity includes a reflection of how students engaged with it. We expect that the tasks we designed will be useful to instructors teaching introductory linear algebra classes and make all the activities available in this footnote\\footnote{All activities designed for the class are available at \\url{https://github.com/sfushidahardy/SSEA-Linear-Algebra-22/blob/main/README.md}. The website includes linear algebra and non-linear algebra activities in addition to those described in this paper.}. While the tasks we have created are flexible, we also share our activity design process because we recognize that these tasks may need to be modified to address the constraints of various curricula.","""\section{ACTIVITY DESIGN}\n\nOur design process started with a couple of basic assumptions, which we believe many instructors will share.\n\vspace{-4mm}\n\paragraph{\RE} Every activity we designed had to engage the students and help them develop mathematical confidence.\n\vspace{-4mm}\n\paragraph{\TE} Every activity was based on the curriculum we were constrained to, and the concepts we needed to cover.\n\nThe first step in our activity design process was to come up with a rough idea for a task related to the concept we wanted to cover. For some concepts we quickly came up with an initial idea on our own. Other concepts lent themselves to finding an activity online that we could build from. In this case, we would often have to modify the activity to suit the topic we needed to cover. For example, one of the activities we considered was the ‘Carpet Ride’ problem series from the Inquiry Oriented Linear Algebra (IOLA) curriculum \cite{wawro2012inquiry}. For us however, an important topic in the curriculum was the discussion of various ways of describing planes. We thus had to modify the problem series as we discuss later in the paper.\n\nIn either case, task development for us was an iterative process. After we had an initial idea in mind, we considered the questions below to repeatedly modify the activity to better fit our goals.\n\n\addtolength{\leftmargini}{0.6cm}\n\begin{enumerate}\n\item[\CO] Is this activity collaborative? \n\item[\EX] Is there a step in the activity where I'm just telling students something?\n\item[\LI] Is there a step of the activity that involves only abstract definitions? Is there some visual component to this task? Is the activity about a real-world application?\n\item[\TI] How long do I think this activity will take?\n\item[\BA] Does the activity have a simple entry point while simultaneously challenging my students?  Is there a step of the activity that seems like a big leap?\n\end{enumerate}\n\addtolength{\leftmargini}{-0.6cm}\n\nAfter every lesson, we discussed how effective activities were and what issues they had. Driving our activity design is our belief that effective instruction is fundamentally student-centered.  We taught this course three consecutive summers, and asked students what aspects of the course they found most helpful.  We revised the overall curriculum and individual activities based on this feedback, which was sought through anonymous surveys and one-on-one interviews. An example of how one activity we discuss in this paper has recently been modified is detailed later in this paper.\n\nIn the next section we share a sampling of the activities we developed, and how each activity connects with our goals and the questions listed above. We also discuss how students engaged with each activity.""","\\section{CONCLUSION}\nTo understand student perspectives, we interviewed 10 of the 70 students who took the course in Summer 2021. In analyzing these interviews, we see ways the activities met the goals we felt were important as instructors, and some of the limitations of our work.   \n\nMultiple students reported that they found the activities enjoyable, and one student mentioned how they encouraged creativity. Several students also mentioned that they left the course feeling better about math. One student shared how he uses visuals in a new way because of his experience in the class.  Reviews of student work from the course show multiple examples of student drawings next to algebraic representations.  \n\nStudents reported that they still struggled to risk being wrong, which limited some of their mathematical exploration. At the same time, we saw many students making mistakes in class, and we sought to foster an environment where mistakes were encouraged. Indeed, students grew more comfortable making mistakes and working in unfamiliar situations, as described in \\emph{Very Sneaky Mathematicians}. \n\nOne goal of the activities was to encourage student collaboration. The overwhelming majority of students reported feeling positively about their experience in group work. Several students mentioned how group work helped them build community, while one reported that group work was intimidating, but not in a bad way. \n\nHowever, some students questioned our emphasis on group work, reporting that in their classes during the academic year, they were expected to work alone. They suggested that our course should have more activities that aren't centered around group work. This disconnect points to a need to communicate more to students the value of collaboration, not just for classes in university, but for however they choose to use mathematics afterwards. \n\nOur interviews revealed that students found our actvities more challenging than their courses during the academic year, which we expected, given the discomfort inquiry-based tasks can cause \\cite{deslauriers2019measuring}. Despite this, students also shared how the material covered was helpful in their transition to the academic year, even for the students who were not immediately taking a linear algebra course. \n\nFor most instructors, using more active learning is better for students, and this paper offers ready-to-use activities for teachers of linear algebra. The reflective questions we discussed may spark ideas for designing additional activities. Every instructor faces different constraints, and instructors should feel empowered to design and adapt activities to their curricula and goals."
"\\section{Introduction}\n\\subsection{Problem setting and main results}\nUndoubtedly, adaptive finite element methods (AFEMs) are in the canon of reliable numerical methods for the\nsolution of partial differential equations (PDEs). Some of the seminal contributions in this still very\nactive area are~\\cite{bv1984, doerfler1996, mns2000, bdd2004, stevenson2007, ckns2008, ks2011, cn2012,\nffp2014} for linear problems,~\\cite{veeser2002, dk2008, bdk2012, gmz2012, ghps2021} for nonlinear problems,\nand~\\cite{axioms} for an abstract framework.\n\nBy means of conforming finite elements, this paper is concerned with the cost-optimal computation of the\nsolution $u^\\star \\in H_0^1(\\Omega)$ to the \\emph{semilinear} elliptic model problem\n\\begin{equation}\\label{eq:modelproblem}\n  -\\operatorname{div}(\\boldsymbol{A} \\nabla u^{\\star}) + b(u^{\\star}) = F \\quad \\text{ in } \\Omega \\quad\n  \\text{ subject to }\\quad\n  u^{\\star}=0 \\quad \\text{ on } \\partial \\Omega,\n\\end{equation}\nwith a Lipschitz domain $\\Omega \\subset \\mathbb{R}^d$ for $d \\in \\{1,2,3\\}$, an elliptic diffusion coefficient\n$\\boldsymbol{A}\\colon \\Omega \\to \\mathbb{R}_{\\textup{sym}}^{d\\times d}$, a monotone nonlinearity $b \\colon\n\\Omega \\to \\mathbb{R}$, and\nsufficiently regular data $F$. The assumptions are such that the Browder--Minty theorem ensures existence and\nuniqueness.\n\nMoreover, the model problem~\\eqref{eq:modelproblem} can be recast into the framework of strongly monotone and\nlocally Lipschitz continuous operators such that the abstract model problem reads: For $\\mathcal{X} = H_0^1(\\Omega)$\nwith topological dual space $\\mathcal{X}' = H^{-1}(\\Omega)$ and duality bracket $\\langle \\, \\cdot \\, , \\,\n\\cdot \\, \\rangle$, a nonlinear\noperator $\\mathcal{A}\\colon \\mathcal{X} \\to \\mathcal{X}'$, and given data $F\\in \\mathcal{X}'$, we aim to\napproximate the solution $u^\\star \\in \\mathcal{X}$ to\n\\begin{equation}\\label{eq:weakform}\n  $\\langle \\, \\mathcal{A} u^\\star \\, , \\, v \\, \\rangle = \\langle \\, F \\, , \\, v \\, \\rangle \\quad \\text{ for\n  all } v \\in \\mathcal{X}.\n\\end{equation}\nTo this end, we employ conforming piecewise polynomial finite element spaces $\\mathcal{X}_H \\subset \\mathcal{X}$ with\nthe corresponding discrete solution $u^\\star_H \\in \\mathcal{X}_H$ to\n\\begin{equation}\\label{eq:weakform:discrete}\n  $\\langle \\, \\mathcal{A} u^\\star_H \\, , \\, v_H \\, \\rangle = \\langle \\, F \\, , \\, v_H \\, \\rangle \\quad \\text{\n  for all } v_{H} \\in \\mathcal{X}_H,\n\\end{equation}\nwhich, however, can hardly be computed exactly, since~\\eqref{eq:weakform:discrete} is still a discrete\nnonlinear system of equations.\n\nThe major difficulty of such problems is that the Lipschitz constant of $\\mathcal{A}$ depends on the considered\nfunctions $v$ and $w$ in the sense that for $\\vartheta> 0$, it holds that\n\\begin{equation}%\\label{eq:locally-lipschitz}\n  $\\| \\mathcal{A} v -\\mathcal{A} w \\|_{\\mathcal{X}'} \\le  L[\\vartheta]\\, \\vvvert v- w \\vvvert \\quad  \\text{\n  for all } v, w \\in \\mathcal{X}\n  \\text{ with } \\max\\big\\{ \\vvvert v \\vvvert, \\vvvert w \\vvvert \\big\\} \\le \\vartheta. \\tag{LIP$'$}\n\\end{equation}\nMoreover, this dependence also appears in the stability constant of the residual-based \\textsl{a~posteriori}\nerror estimator~\\cite{v2013, bbimp2022}.\n\nHence, for such a problem class, any approximate numerical scheme must ensure uniform boundedness of all\ncomputed approximations $u_H^\\star \\approx u_H \\in \\mathcal{X}_H$ throughout the algorithm. This\nconstitutes the first main result: The developed \\emph{adaptive iteratively linearized FEM}  (AILFEM)\nalgorithm (more detailed in Algorithm~\\ref{algorithm:AILFEM} below) guarantees a uniform upper bound on all\niterates (see Theorem~\\ref{theorem:uniformBoundedness} below). In particular, the algorithm steers the\ndecision whether it is more preferable to refine the mesh adaptively or to do an additional step of\nlinearization or a further algebraic solver step instead.\n\nOnce uniform boundedness is established, we prove full R-linear convergence\n(Theorem~\\ref{theorem:RLinearConvergence} below) as the second main result. Full R-linear convergence\nestablishes contraction in each step of the algorithm regardless of the algorithmic decision. At the expense\nof a more challenging analysis that links energy arguments with the energy norm of the algebraic solver, full\nR-linear convergence is guaranteed for all mesh levels $\\ell \\ge \\ell_0=0$ while prior\nworks~\\cite{bbimp2022, bhimps2023} used compactness arguments which only guaranteed the existence of the index\n$\\ell_0\\in \\mathbb{N}_0$ (and not necessarily $\\ell_0=0$). As a consequence of uniform boundedness and full R-linear\nconvergence, the third main result proves optimal rates both understood with respect to the degrees of\nfreedom and with respect to the overall computational cost (Corollary~\\ref{cor:rates=complexity} and\nTheorem~\\ref{th:optimal_complexity}) of the proposed algorithm.\n\nCompared to existing results in the literature~\\cite{ghps2021, hpsv2021, hpw2021, fps2023},\nall three main results require a suitable adaptation of the stopping criteria of the linearization loop as\nwell as sufficiently many iterations in the algebra loop, together with subtle technical challenges, in\nparticular, for the proof of full R-linear convergence.\n\n\\subsection{From AFEM to AILFEM}\nOn each mesh level (with mesh index $\\ell$), the arising discrete nonlinear problems cannot be solved exactly\nin practice as supposed in classical AFEM~\\cite{veeser2002, dk2008, bdk2012, gmz2012}. To deal with this\nissue, we follow~\\cite{cw2017, ghps2018, hw2020:ailfem} and consider the so-called \\emph{Zarantonello\niteration} from~\\cite{zarantonello1960} as a linearization method (with index $k$). The Zarantonello\niteration is a Richardson-type iteration where only a Laplace-type problem has to be solved in each\niteration. Since the arising large SPD systems are still expensive to solve exactly, we employ a contractive\nalgebraic solver as a nested loop to solve the Zarantonello system inexactly (with iteration index $i$). The\nloops thus come with a natural nestedness (see Figure~\\ref{fig:solveAndEstimate}), where the overall\nschematic loop of the algorithm reads\n\\medskip\n\\begin{center}\n  \\includegraphics[width = 0.7\\textwidth]{afem_loop}\n\\end{center}\n\\medskip\n%\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width = \\textwidth]{ailfem_overview}\n  \\end{center}\n  \\caption{Depiction of the nested loops of the AILFEM algorithm~\\ref{algorithm:AILFEM} below.}\n  \\label{fig:solveAndEstimate}\n\\end{figure}\nSince the proposed adaptive loop depends on all previous computations, optimal convergence rates should\nbe understood with respect to the overall computational cost. This idea of \\emph{optimal complexity}\noriginates from the wavelet community~\\cite{cdd2001, cdd2003} and was last used in the context of AFEM\nin~\\cite{stevenson2007} for the Poisson model problem and~\\cite{cg2012} for the Poisson eigenvalue problem,\nboth under realistic assumptions on generic iterative solvers.\n\nAILFEMs with iterative and/or inexact solver with \\textsl{a~posteriori} error estimators are found in,\ne.g.,~\\cite{bms2010, aev2011, agl2013, ev2013, aw2015, cw2017}  and references therein. Besides the\nZarantonello iteration, for globally Lipschitz continuous nonlinearities, the works~\\cite{hw2020:convergence,\nhw2020:ailfem, hpw2021} analyze also other linearizations such as the Ka\\v{c}anov iteration or damped Newton\nschemes. Optimal complexity of the Zarantonello loop that is coupled with an algebraic loop is analyzed\nin~\\cite{bhimps2023} for nonsymmetric second-order linear elliptic PDEs and for strongly monotone (and\nglobally Lipschitz continuous) model problems in~\\cite{ghps2018, ghps2021, hpsv2021,  hpw2021, fps2023}.\n\nThe literature on AILFEMs for locally Lipschitz continuous problems is scarce and closing this gap is the aim\nof this work. The semilinear model problem is treated in, e.g.,~\\cite{aw2015} by a damped Newton iteration\nand in~\\cite{ahw2022} by an energy-based approach with experimentally observed optimal rates. We also refer\nto the own work~\\cite{bbimp2022cost} for an AILFEM with optimal rates with respect to the overall\ncomputational cost, however, under the assumption that the arising linear systems can be solved\nat linear cost.\nMore precisely, compared to the previous work~\\cite{bbimp2022cost}, in this paper we also take an optimal\n  algebraic solver for the linearized problem into account and propose an adaptive algorithm\n  ensuring optimal convergence\n  rates with respect to the computation time. Moreover, compared to~\\cite{bbimp2022cost} that elaborates the\n  proof of full R-linear convergence\n  along the lines of~\\cite{ghps2021}, we provide a much simpler proof inspired by~\\cite{fps2023}.\n  However, the work~\\cite{fps2023}\n  employs a general quasi-orthogonality from~\\cite{feischl2022} that is not available for nonlinear problems\n  in general since\n  the proof relies on a stable LU-decomposition of the linear problem. Therefore, to avoid compactness\n  arguments like in~\\cite{bhimps2023}, we employ orthogonality in the underlying energy.\n\n\\subsection{Outline}\nThis paper is structured as follows: Section~\\ref{section:monlip} introduces the abstract framework on\nlocally Lipschitz continuous operators. In Section~\\ref{section:algorithm}, we formulate the (idealized)\nAILFEM algorithm (Algorithm~\\ref{algorithm:AILFEM}). We prove uniform boundedness for the final iterates of\nthe algebraic solver (Theorem~\\ref{theorem:uniformBoundedness}).  Section~\\ref{sec:Rlinear} presents the\nsecond main result: Full R-linear convergence (Theorem~\\ref{theorem:RLinearConvergence}). In particular,\nrates with respect to the degrees of freedom coincide with rates with respect to the computational cost\n(Corollary~\\ref{cor:rates=complexity}). In Section~\\ref{section:optimality}, we prove the main result on\noptimal complexity of the proposed AILFEM algorithm (Theorem~\\ref{th:optimal_complexity}). In\nSection~\\ref{section:numerics}, we present numerical experiments of the proposed AILFEM strategy and\ninvestigate its optimal complexity for various choices of the adaptivity parameters.",,
,"""%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\section{Fully adaptive algorithm} \label{section:algorithm}\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\nIn this section, we present the adaptive iterative linearized finite element method (AILFEM). As a first main\nresult, we prove that the iterates from the proposed algorithm are uniformly bounded.\n\n%%%%%%%%%%%%%%%%%%%%%\n\subsection{Fully adaptive algorithm}\n%%%%%%%%%%%%%%%%%%%%%%\n\nIn this section, we introduce a fully adaptive algorithm that steers mesh refinement ($\ell$),  linearization\n($k$) and the algebraic solver ($i$). The algorithm utilizes specific stopping indices denoted by an\nunderline, namely \(\underline{\ell},\underline{k}[\ell], \underline{i}[\ell,k]\). However, we may omit the\ndependence when it is apparent\nfrom the context, such as in the abbreviation \(u_\ell^{k, \underline{i}}\coloneqq u_\ell^{k,\underline{i}[\ell,k]}\).\n\n\begin{algorithm}[adaptive iterative linearized FEM (AILFEM)]\n  \label{algorithm:AILFEM}\n  \phantom{2}\\  {\bfseries Input:} Initial mesh $ \mathcal{T}_0$, marking parameters $0 < \theta \le 1$, $C_{\rm\n  mark} \ge 1$, solver parameters $\lambda_{\textup{lin}},\lambda_{\textup{alg}} > 0$, minimal number of\n  algebraic solver steps $i_{\textup{min}}\n  \in \mathbb{N}$, initial guess $u_0^{0,0}  \coloneqq u_0^{0,\star}\coloneqq u_{0}^{0, \underline{i}} \in\n  \mathcal{X}_0$ with\n  $\enorm{u_0^{0,0}} \le 2 M$, and Zarantonello damping parameter \(\delta > 0\).\n\n  \noindent\n  {\bfseries Adaptive loop:} For all $\ell = 0, 1, 2, \dots$, repeat the following steps\n  \eqref{alg:solve+estimate}--\eqref{alg:refine}:\n  \begin{enumerate}[label = (\Roman*), ref=\Roman*, font=\upshape]\n    \item\label{alg:solve+estimate} {\tt SOLVE \& ESTIMATE.} For all $k = 1, 2, 3, \dots$, repeat steps\n      \eqref{alg:phi}--\eqref{alg:terminate_k}:\n      \begin{enumerate}[label = (\alph*), ref=\alph*, font=\upshape]\n        \item\label{alg:phi} Define $u_\ell^{k,0} \coloneqq u_\ell^{k-1, \underline{i}}$ and, for only\n          theoretical reasons,\n          $u_\ell^{k,\star} \coloneqq \Phi_\ell(\delta; u_\ell^{k-1,\underline{i}})$.\n        \item For all $i=1,2,3, \dots$ repeat steps \eqref{alg:solve}--\eqref{alg:terminate_i}:\n          \begin{enumerate}[label = (\roman*), ref=\roman*, font=\upshape]\n            \item\label{alg:solve} Compute $u_\ell^{k, i} \coloneqq \Psi_\ell(u_\ell^{k,\star};\n              u_\ell^{k, i-1})$ and error estimator $\eta_\ell(u_\ell^{k,i})$.\n            \item\label{alg:terminate_i} Terminate the $i$-loop and define $\underline{i}[\ell, k] \coloneqq i$ if\n              \begin{equation}\label{eq:i_stopping_criterion}\n                \enorm{u_\ell^{k,i-1} - u_\ell^{k, i}} \le \lambda_{\textup{alg}} \, \bigl[\lambda_{\textup{lin}}\n                \,\eta_{\ell}(u_\ell^{k,i})  + \enorm{u_\ell^{k, i} - u_\ell^{k,0}} \bigr]  \quad\n                \mathtt{AND} \quad i_{\textup{min}} \le i.\n              \end{equation}\n          \end{enumerate}\n        \item\label{alg:terminate_k} Terminate the $k$-loop and define \(\underline{k}[\ell] \coloneqq k\) if\n          \begin{equation}\label{eq:k_stopping_criterion}\n            \mathcal{E}(u_\ell^{k, 0})- \mathcal{E}(u_\ell^{k, \underline{i}}) \le \lambda_{\textup{lin}}^2\n            \, \eta_\ell(u_\ell^{k, \underline{i}})^2\n            \quad  \mathtt{ AND } \quad  \enorm{u_\ell^{k, \underline{i}}} \le 2M.\n          \end{equation}\n      \end{enumerate}\n\n    \item  \texttt{MARK.} Find a set $\mathcal{M}_\ell \in \mathbb{M}_\ell[\theta,\n      u_\ell^{\underline{k},\underline{i}}] \coloneqq\n      \{\mathcal{U}_\ell \! \subseteq \! \mathcal{T}_\ell \colon \theta \, \eta_\ell(u_\ell^{\underline{k},\n        \underline{i}})^2 \le\n      \eta_\ell(\mathcal{U}_\ell, u_\ell^{\underline{k}, \underline{i}})^2\}$ such that\n      \begin{equation}\label{eq:doerfler}\n        \# \mathcal{M}_\ell \le C_{\textup{mark}}  \min_{\mathcal{U}_\ell \in \mathbb{M}_\ell[\theta,\n            u_\ell^{\underline{k},\n        \underline{i}}]} \# \mathcal{U}_\ell.\n      \end{equation}\n\n    \item\label{alg:refine}  \texttt{REFINE.} Generate the new mesh $\mathcal{T}_{\ell+1} \coloneqq  {\tt refine}\n      (\mathcal{M}_\ell, \mathcal{T}_\ell)$ by\n      employing NVB and define $u_{\ell+1}^{0,0} \coloneqq u_{\ell+1}^{0,\underline{i}} \coloneqq u_{\ell+1}^{0,\n      \star} \coloneqq u_\ell^{\underline{k},\underline{i}}$ (nested iteration).\n  \end{enumerate}\n\n  \noindent\n  {\bfseries Output:}  Sequences of successively refined triangulations $\mathcal{T}_\ell$, discrete approximations\n  $u_\ell^{k, i}$ and corresponding error estimators $\eta_\ell(u_\ell^{k, i})$.\n\end{algorithm}\n\nFor the analysis of Algorithm~\ref{algorithm:AILFEM}, we define the countably infinite index set\n\begin{equation*}%\label{eq:indexSet}\n  \mathcal{Q} \coloneqq \{(\ell, k, i) \in \mathbb{N}_0^3 \colon u_\ell^{k, i} \text{\n  is used in Algorithm~\ref{algorithm:AILFEM}}\},\n\end{equation*}\nwhere, for any $(\ell, 0,0)\in \mathcal{Q}$, the final indices are defined as\n%\begin{subequations*}%\label{eq:indices}\n\begin{alignat*}{2}\n  \underline{\ell} &\coloneqq \sup \{\ell \in \mathbb{N}_0 \colon (\ell, 0, 0) \in \mathcal{Q}\} &&\in\n  \mathbb{N}_0 \cup \{ \infty\},\n  \\\n  \underline{k}[\ell] &\coloneqq \sup \{k \in \mathbb{N} \colon (\ell, k, 0) \in\n  \mathcal{Q}\}&&\in \mathbb{N} \cup \{ \infty\},\n  \\\n  \underline{i}[\ell, k] &\coloneqq \sup \{i \in \mathbb{N} \colon (\ell, k, i) \in\n  \mathcal{Q}\}&&\in \mathbb{N} \cup \{ \infty\}.\n\end{alignat*}\n%\end{subequations*}\nWe note, first, that these definitions are consistent with those of Algorithm~\ref{algorithm:AILFEM}, second,\nthat Lemma~\ref{lemma:termination-i} below proves that $\underline{i}[\ell, k] < \infty$, and, third, that hence\neither \(\underline{\ell} =\n\infty\) or $\underline{\ell} < \infty$ with\n\(\underline{k}[\underline{\ell}] = \infty\). For all $(\ell, k, i) \in \mathcal{Q}$, we introduce the total\nstep counter $\vert \cdot, \cdot,\n\cdot \vert$ defined by\n\begin{equation*}\n  |\ell, k, i| \coloneqq \#\n  \{(\ell^\prime, k^\prime, i^\prime) \in \mathcal{Q} \colon \! (\ell^\prime, k^\prime, i^\prime) < (\ell,k, i) \} \n  = \sum_{\ell' = 0}^{\ell-1} \sum_{k' = 1}^{\underline{k}[\ell']}\n  \sum_{i'=1}^{\underline{i}[\ell', k']} 1\n  + \sum_{k' = 1}^{k-1} \sum_{i'=1}^{\underline{i}[\ell, k']} 1\n  + \sum_{i'=1}^{i-1} 1.\n\end{equation*}\nWe note that this definition provides a lexicographic ordering on \(\mathcal{Q}\).\n\nIn the later application to AILFEM for semilinear elliptic PDEs, every step of\nAlgorithm~\ref{algorithm:AILFEM} can be performed\nin linear\ncomplexity as the following arguments show.\n\begin{itemize}\n  \item[$\triangleright$] \texttt{SOLVE.} The employed algebraic solver is an $hp$-robust\n    multigrid~\cite{imps2022} and hence each algebraic solver step requires only $\mathcal{O}(\#\n    \mathcal{T}_\ell)$ operations.\n  \item[$\triangleright$] \texttt{ESTIMATE.} The simultaneous computation of the standard error indicators\n    $\eta_\ell(T, u_\ell^{k,i})$ for all $T \in \mathcal{T}_\ell$ can be done at the cost of $\mathcal{O}(\#\n    \mathcal{T}_\ell)$.\n  \item[$\triangleright$] \texttt{MARK.} The employed Dörfler marking (and the involved determination of\n    $\mathcal{M}_\ell$) is indeed a linear complexity problem; see~\cite{stevenson2007} for $C_{\textup{mark}} = 2$\n    and~\cite{pp2020} for $C_{\textup{mark}} = 1$.\n  \item[$\triangleright$] \texttt{REFINE.} The refinement of $\mathcal{T}_\ell$ is based on NVB and, owing to the\n    mesh-closure estimate~\cite{bdd2004, stevenson2008}, requires only linear cost $\mathcal{O}(\# \mathcal{T}_\ell)$.\n\end{itemize}\nThus, the total work until and including the computation of $u_\ell^{k,i}$ is proportional to\n\begin{equation}\label{eq:cost}\n  \mathtt{cost}(\ell, k, i)\n  \coloneqq\! \! \! \! \! \!\sum_{\substack{(\ell', k', i') \in \mathcal{Q} \\ |\ell', k', i'|\le |\ell, k,\n  i|}}\! \! \! \!\! \! \# \mathcal{T}_{\ell'} =\n  \sum_{\ell^\prime = 0}^{\ell-1} \sum_{k' = 1}^{\underline{k}[\ell^\prime]} \sum_{i'\n  = 1}^{\underline{i}[\ell', k']} \# \mathcal{T}_{\ell^\prime}\n  + \sum_{k' = 1}^{k-1} \sum_{i'=1}^{\underline{i}[\ell, k']} \# \mathcal{T}_{\ell}\n  + \sum_{i'=1}^{i} \# \mathcal{T}_{\ell}.\n\end{equation}""",
,,"Given full R-linear convergence from Theorem~\\ref{theorem:RLinearConvergence}, then convergence rates with respect to the degrees of freedom coincide with rates with respect to the overall computational cost, where we recall $\\mathtt{cost}(\\ell,k,i)$ from~\\eqref{eq:cost}. Since all essential arguments are provided, the proof follows verbatim from~\\cite[Corollary~16]{fps2023}.\n\n\\begin{corollary}[rates $\\,\\boldsymbol{\\widehat{=}}\\,$ complexity]\\label{cor:rates=complexity}\n  Suppose full R-linear convergence~\\eqref{eq:RLinearConvergence}. Recall $\\mathtt{cost}(\\ell, k, i)$ from~\\eqref{eq:cost}. Then, for any $s > 0$, it holds that\n  \\begin{equation}\\label{eq:single:complexity}\n    M(s)\n    \\coloneqq\n    \\sup_{(\\ell,k,i) \\in \\mathcal{Q}} (\\#\\mathcal{T}_\\ell)^s \\, \\textup{H}_\\ell^{k,i}\n    \\le \\sup_{(\\ell,k, i) \\in \\mathcal{Q}} \\mathtt{cost}(\\ell, k, i)^s \\, \\textup{H}_\\ell^{k,i}\n    \\le C_{\\rm cost} \\, M(s),\n  \\end{equation}\n  where the constant $C_{\\rm cost} > 0$ depends only on $C_{\\textup{lin}}$, $q_{\\textup{lin}}$, and $s$.\n  Moreover, there exists $s_0 > 0$ such that $M(s) < \\infty$ for all $0 < s \\le s_0$. \\hfill \\qed\n\\end{corollary}"
,,
,,
"\\section{Introduction}\n\nIn this article we  take a closer look at the index terms for infinite-dimensional differential-algebraic systems (DAE) of the form\n    \\begin{equation}\\label{eqn:dae}\n        \\frac{{\\rm d}}{{\\rm d}t}Ex(t)=Ax(t)+f(t), \\quad t\\geq 0,\n    \\end{equation}\nwhere $E\\colon \\X\\to\\Z$ is a bounded linear operator (denoted by $E\\in \\L(\\X,\\Z)$), $(A,\\dom(A))$ is a closed and densely defined linear operator from $\\X$ to $\\Z$ and $f\\colon[0,\\infty) \\to\\Z$. Throughout this article, $\\X$ and $\\Z$ are  Banach spaces and the DAE \\eqref{eqn:dae} is assumed to be regular. That is,\n    \\begin{equation*}\n        \\rho(E,A)\\coloneqq \\left\\{\\lambda \\in \\C \\,\\middle|\\, (sE-A)\\inv \\in \\L(\\Z,\\X)\\right\\}\\not=\\emptyset.\n    \\end{equation*}\n By a \\textit{solution} of \\eqref{eqn:dae} we mean a \\textit{classical solution}, that is, a function $x\\colon [0,\\infty)\\rightarrow \\dom(A)$ such that $Ex(\\cdot)$ is continuously differentiable as a~function with values in $\\Z$, and \\eqref{eqn:dae} is satisfied for every $t\\ge 0$.\n\nThe index of a DAE can be defined in a number of various ways. Examples include the differentiation index, the nilpotency index, the resolvent index and the radiality index \\cite{GernandHallerReis, Kunkelmehrmann, fedorov-sviridyuk, STrostorff}. Not all indices are defined in the infinite-dimensional case. For instance, the nilpotency index of a DAE demands a Weierstraß form (defined formally below), which is not always available. %Even if such a form would exist at all times, it is still open if it would be unique up to isomorphism, such that the nilpotency index would also be uniquely defined.\n\nOur aim in writing this paper is to 'collect' all the index terms that are applicable in the infinite-dimensional case, and to characterize and compare them to each other. In particular, we investigate the resolvent index $\\pres^{(E,A)}$, the chain index $p_{\\rm chain}^{(E,A)}$, the radiality index $p_{\\rm rad}^{(E,A)}$, the nilpotency index $p_{\\rm nilp}^{(E,A)}$, the differentiation index $p_{\\rm diff}^{(E,A)}$ and the perturbation index $p_{\\rm pert}^{(E,A)}$. Several of these indices have not previously been defined for infinite-dimensional systems.\n\n\nOne of our main results is  that if  all the  indices mentioned in the previous paragraph exist then\n%\\todo[inline]{To add $p_{\\rm pert}^{(E,A)}$ we also need to assume, that $A_1$ in the Weierstraß form generates a $C_0$-semigroup.}\n    \\begin{align*}\n        p_{\\rm rad}^{(E,A)}+1 \\geq \\pres^{(E,A)} \\geq p_{\\rm nilp}^{(E,A)} = p_{\\rm diff}^{(E,A)} =p_{\\rm chain}^{(E,A)}.\n    \\end{align*}\n    If in addition the operator $A_1$ in the Weierstraß form generates a $C_0$-semigroup, then\n  \\begin{align*}\n        p_{\\rm rad}^{(E,A)}+1 \\geq \\pres^{(E,A)} \\geq p_{\\rm nilp}^{(E,A)} = p_{\\rm diff}^{(E,A)} =p_{\\rm chain}^{(E,A)}=p_{\\rm pert}^{(E,A)}.\n    \\end{align*}\nFurthermore, Proposition \\ref{nilp>=rad+1}  implies that in the finite-dimensional case, equality holds in all these bounds.\n\n\n%\\section{Index concepts for differential-algebraic equations}\nWe conclude the introduction with some notation.\n%The \\textit{resolvent set} of the operator pencil $(E,A)$ is\n  %  \\begin{equation*}\n  %      \\rho(E,A)\\coloneqq \\left\\{\\lambda \\in \\C \\,\\middle|\\, (sE-A)\\inv \\in \\L(\\Z,\\X)\\right\\}.\n%    \\end{equation*}\n % A DAE  \\eqref{eqn:dae} is said to be \\textit{regular} if $\\rho(E,A)$ is not empty.\n For a $\\lambda \\in \\rho(E,A)$ we call $(\\lambda E-A)\\inv$, $R^E(\\lambda,A)\\coloneqq (\\lambda E-A)\\inv E$ and $L^E(\\lambda,A)\\coloneqq E(\\lambda E-A)\\inv$ the \\textit{resolvent}, \\textit{right-$E$ resolvent} and \\textit{left-$E$ resolvent} of $A$ respectively.",,"\\section{Summary}\n The purpose of this paper was, as explained in the introduction, to review the common indices for DAEs and to extend them to infinite-dimensional DAEs where this has not already been done. Rigorous definitions were given for the nilpotency, resolvent, radiality, chain, differentiation and perturbation indices.  The definitions of the differentiation and perturbation indices are as far as we know, new in the Banach space setting. Unlike finite-dimensional DAEs, the existence of an index is not guaranteed, and furthermore the various indices  are not equivalent for general DAEs. Each index reveals different information about the system. We  were able to show  that in some cases existence of a particular index will imply existence of some other other indices. One implication is that the chain index is the least restrictive index since existence of any of the  other indices implies existence of the chain index, as well as a bound on the chain index.\n\n This work has revealed a number of open questions for DAEs on Banach spaces. Under what conditions does a particular index exist? If two indices exist, when are they equal? Which indices are more useful for the study of DAEs? It is hoped that this paper will inspire research on these and related questions."
"\\section{Introduction}\n\nCorona problems are relevant in linear infinite-dimensional control theory, especially for delay equations see \\cite{YamamotoWillems,chitour:hal-03827918}. Exact controllability in finite time is often characterized in terms of a B\\'ezout identity over appropriate functional algebras and hence obtaining an exact controllability criterion is tantamount to the resolution of a corona problem for measures or distributions compactly supported algebras. \n\nSince the resolution of the corona problem in one dimension for holomorphic bounded functions in the unit disk by the celebrated paper \\cite{Carleson1962}, the corona problem received a large attention. Carleson's result has been extended in various way, as for more general domains or algebras, see for instance a matrix version in the polydisk \\cite{treil2005matrix} or in a multiply connected domains \\cite{brudnyi2000matrix}, for some functions algebra on planar domains \\cite{mortini2014corona} or for the algebra of almost periodic function with a Bohr--Fourier negatively supported \\cite{frentz2014subalgebra}. The most closely corona theorem  related to the controllability of difference delay equations is stated in \\cite[Corollary 3.3]{maad2011generators} for distributions positively compactly supported, but at the current state of the literature, it does not apply directly \nto the exact controllability of linear controlled delayed difference equations (LCDDE).\n\n% Some recent papers undertook the purpose to give corona theorems for the algebra of almost periodic functions, see for instance \\cite{frentz2014subalgebra,bottcher1999corona}, \n% but at the current state of the literature, they do not apply directly \n% to the exact controllability of linear controlled difference equations (LCDE). \n\nIn this paper, we establish two results. The first one consists in the resolution of a corona theorem for a subalgebra of $M(\\mathbb{R}_-)$, the commutative Banach algebra made of Radon measures compactly supported in $\\mathbb{R}_-$. More precisely, for a finite number of $f_1,...,f_N$, each of them being a finite sum of Dirac measures supported in $\\mathbb{R}_-$, we give a necessary and sufficient condition on the Laplace transform of the measures $f_1,...,f_N$ to obtain the existence of $g_1,...,g_N \\in M(\\mathbb{R}_-)$ such that\n\\begin{equation}\n\\label{eq_intro}\n    f_1*g_1+...+f_N*g_N=\\delta_0,\n\\end{equation}\nwhere $*$ denotes the convolution product and $\\delta_0$ the Dirac distribution at zero.\nThat result is then used to derive an $L^1$ exact controllability criterion (in finite time) for LCDDE expressed in the frequency domain, thus solving an open question raised in \\cite{chitour:hal-03827918}. We emphasize that LCDDE can sometimes be used to address some control theoretic questions for 1-D hyperbolic partial differential equations \\cite{CoNg,baratchart}.\n%and it follows that the result of this paper might be pertinent in the study of the controllability of PDE's.\n\nThe strategy of proof for the corona problem goes as follows : in a first step, we reduce the corona problem~\\eqref{eq_intro} to a corona problem in a quotient Banach algebra. The second step goes by contradiction and relies on  Gelfand representation theory characterizing maximal ideal as the kernel of homomorphisms, in the spirit of \\cite{frentz2014subalgebra,bottcher1999corona}. It is not immediate how to deduce our corona theorem from these references and we include a proof of it for sake of clarity (yet very similar to that of \\cite{frentz2014subalgebra}). \n% However, the corona theorem for $\\mathrm{APW}_-$  is not enough to solve our controllability problem and we crucially need to achieve a corona theorem for the smaller algebra $\\mathrm{APW}_-^{\\rm{bd}}$. This is done in a second step, relying on a result of \\cite{kamen1976module}.\nAs for our second main result, it answers a question raised in \\cite{chitour:hal-03827918}\nwhere the sufficiency of a frequency domain criterium for $L^1$ exact controllability of a LCDDE was reduced to establishing the  corona theorem established previously. \n\n% The proof of the first result follows a standard strategy: in a first step, we establish a corona theorem for $\\mathrm{APW}_-$ the commutative Banach algebra made of almost periodic functions with summable Fourier series, which is not a truly original result at the light of the \\cite{frentz2014subalgebra,bottcher1999corona}. It is not immediate how to deduce that particular corona theorem from these references and we included a proof of it for sake of clarity (yet very similar to that of \\cite{bottcher1999corona}). However, the corona theorem for $\\mathrm{APW}_-$  is not enough to solve our controllability problem and we crucially need to achieve a corona theorem for the smaller algebra $\\mathrm{APW}_-^{\\rm{bd}}$. This is done in a second step, relying on a result of \\cite{kamen1976module}. As for our second main result, it answers a question raised in \\cite{chitour:hal-03827918}\n% where the sufficiency of a frequency domain criterium for $L^1$ exact controllability of an LCDE was reduced to establishing the  corona theorem in $\\mathrm{APW}_-^{\\rm{bd}}$ established previously. \n\n",,
"\\section{Introduction.}\nLet  $X$ be a projective variety over a field $F$. The automorphism group functor $\\AAut(X)$ is represented by a group scheme, locally of finite type over $F$. This  is due to Grothendieck (see also \\cite{MO}, Theorem 3.7). Note that the sub-group scheme $\\AAut^0(X) \\subset \\AAut(X)$, defined as the connected component of the identity, is then a group scheme of finite type over $F$- that is to say, an algebraic group over $F$.\\\\ Conversely, it is natural to ask:\n\n\\begin{qu}\n Let  $G$ be an algebraic group over a field $F$.\\\\\nDoes there exists a smooth projective $F$-variety $X$, such that $G \\simeq \\AAut(X)$? \n\\end{qu}\n\nWhen $G=A$ is an abelian variety, the answer was found independently by several authors:  it is positive, if and only if $\\Aut_{gp}(A)$ is  finite.  See \\cite{LM}, \\cite{BB} and \\cite{F}. \\\\ In the recent  paper \\cite{BS} (to which we refer for an overview of the rich history of Question 1.1), Brion and Schr\\\",,
,"""\section{Spectral symbol and spectral theory} \label{sec:spectral}\n    The present section is devoted to the spectral and structure analysis for fixed dimension and for the related sequences of matrices as the fineness parameter tends to zero.\n    To begin with, \n    we explicitly construct the symbol, following the assembly procedure of the stiffness matrix, which is depicted in Figure~\ref{fig:Ablocks}. \n    In our setting weigths dof's are just evaluation at points, so let $\{\xi_i\}_{i=1}^{r+1}$ be a set of reference positions in $\Ihat=[0,1]$.\n    We recall that the presence of both extrema of the interval is needed for defining continuous elements, so we assume that $\xi_1=0$ and $\xi_{r+1}=1$.\n\n\begin{figure}[h]\n\begin{center}\n\begin{tikzpicture}[scale=0.5]\n  [pin distance=10mm,\n   pin edge={stealth-,shorten <=-4pt,decorate,decoration=bent}]\n  \foreach \x in {0,1,2,3,4,5,6,7,8,9}{\n    \fill[green!50]  (\x,-\x) rectangle +(1,-1);\n  }\n  \foreach \x in {0,1,2,3,4,5,6,7,8}{\n    \fill[yellow!30] (\x+1,-\x) rectangle +(1,-1);\n    \fill[yellow!30] (\x,-\x-1) rectangle +(1,-1);\n  }\n\n  \begin{scope}\n    \clip (0.3,-0.3) rectangle (10,-10);\n    \foreach \x in {0,2,4,6,8}{\n     \filldraw[draw=black,pattern=north east lines]\n         (\x,-\x)  rectangle +(1.3,-1.3);\n     \filldraw[draw=black,pattern=north west lines]\n         (\x+1,-\x-1)  rectangle +(1.3,-1.3);\n    }\n  \end{scope}\n  \node[pin=above right:{\large $A^{(I_k)}$}] at (5.3,-4) {};\n  \node[pin=above right:{\large $A^{(I_{k+1})}$}] at (6.3,-5) {};\n\n  \node[pin=left:{\large $\alpha_\xi$}] at (0,-0.5) {};\n  \node[pin=left:{\large $\beta_\xi$}] at (0,-1.5) {};\n  \node[pin=right:{\large $\beta_\xi^T$}] at (2,-0.5) {};\n\n  \draw[very thick,xshift=0.3cm] (0.3,-0.3) -- (0.,-0.3) -- (0.,-10) -- (0.3,-10);\n  \draw[very thick,xshift=10cm]  (-0.3,-0.3) -- (0.,-0.3) -- (0.,-10) -- (-0.3,-10);\n  \node[anchor=east] at (0,-5) {\huge $A_n=$};\n\end{tikzpicture}\n\end{center}\n    \caption{Structure of the stiffness matrix and of the Toeplitz matrix.\n    Element contributions are hatched in black: note the overlap of nearby blocks and the smaller first and last block. The blocks of the Toeplitx matrix are shaded in colour.}\n    \label{fig:Ablocks}\n\end{figure}\n\n    Let then $A^{(I_k)}$ be the $(r+1)\times(r+1)$ local contribution of element $I_k$ to the global stiffness matrix, defined as in \eqref{eq:Aloc}. In the simple case of unit diffusion coefficient ($b(x)=1$) and uniform mesh size, then $A^{(I_k)}=A^{\loc}$ is independent of $k$ and the stiffness matrix has the general structure of a block-tridiagonal Toeplitz matrix with $r\times r$ blocks (see Figure~\ref{fig:Ablocks}).\n    We are thus able to write the spectral symbol\n    $f_{r}^{\xi}: [-\pi, \pi]  \to \mathbb{C}^{r \times r}$\n    of the stiffness matrix as a function depending on the $ \xi_i $'s.\n\n    Let us consider a uniform grid with $n$ elements and let $A_n$ be the corresponding stiffness matrix \eqref{eq:stiffness}. \n    Consider the set of global indices $\mathcal{I}_k\doteq\{\LTG(i,k):i=0,\ldots,r\}$ associated with a generic $k$-th element (with $k\neq1,n$ since the first and last elements are influenced by the boundary conditions). \n    Since the contribution of the shape function associated with $\xi_{r+1}=1$ of each element will overlap in $A_n$ with those of of the shape function associated with $\xi_1=0$ of the next element, the submatrix of $A_n$ for the rows and columns in $\mathcal{I}_k$ is\n    $$ (A_\xi)_{i,j} \doteq\n    \begin{cases}\n      A_{1, 1}^{\loc} + A_{r+1, r+1}^{\loc} &\text{ if } (i,j)=(1,1) \text{ or } (r+1,r+1) , \\\n      A_{i,j}^{\loc} &\text{ otherwise }\n    \end{cases}\n    $$\n    The structure of the $(r+1)\times (r+1)$ matrix $A_\xi$ is\n    $$ A_{\xi} =\left[\n    \begin{array}{c|c}\n      \alpha_\xi & \widetilde{\beta}_\xi^T \\ \hline\n      \widetilde{\beta}_\xi & b\n    \end{array}\right],$$\n    where $ \alpha_\xi \in \mathbb{R}^{r \times r}$, $ \widetilde{\beta}_\xi \in \mathbb{R}^{1 \times r}$, $ b \in \mathbb{R} $ and\n    $b=(\alpha_\xi)_{1,1}$. Following \cite{Rahla}, define \n    \begin{equation*}\n      \beta_\xi \doteq \left[ \begin{array}{c}\n        \widetilde{\beta}_\xi \\ \hline\n        \boldsymbol{0}\n      \end{array}\n      \right] ,\n    \end{equation*}\n    with $ \boldsymbol{0} $ being the zero matrix of size $(r-1)\times r$, so that $ \beta_\xi \in \mathbb{R}^{r \times r}$.\n\n    Recalling (\ref{eq:fourier_coefficients})-(\ref{eq:toeplitz_kron}),\n    we observe that the stiffness matrix $A_n$ is a principal submatrix of size $nr-1$ of the Toeplitz matrix $T_n(f_{r}^{\xi})$ with generating function\n    \begin{align} \label{eq:symboldegr}\n      f_{r}^{\xi}: \quad [-\pi, \pi] & \to \mathbb{C}^{r \times r},\n      \\\n      \theta & \mapsto \alpha_\xi + \beta_\xi e^{i \theta} + \beta_\xi^T e^{-i \theta}. \nonumber\n    \end{align}\n\n    Furthermore, according to the first item of axiom {\bf GLT2} and to axiom {\bf GLT1}, we deduce that $f_{r}^{\xi}$ is also the spectral and the singular value symbol of $\{T_{n}(f_{r}^{\xi})\}_{{n}}$, in the sense of Definition \ref{def-distribution}. As a first conclusion we infer that $f_{r}^{\xi}$ is the spectral and the singular value symbol of the stiffness matrix-sequence $\{A_{n}\}_{{n}}$. This is proven by applying Theorem \ref{extradimensional} to $X_n=T_{n}(f_{r}^{\xi})$, $A_n=P_n^*X_nP_n$, $d_n=nr$, $\delta_n=nr-1$, so that $\delta_n/d_n\to1$ as $n\to\infty$, and $P_n$ rectangular matrix obtained by the identity of size $d_n$ by eliminating only the last column.\n\n    As a consequence, the union of the ranges of the $r$ eigenvalues $\lambda_1(f_{r}^{\xi})\le \lambda_2(f_{r}^{\xi}) \le \cdots \le \lambda_r(f_{r}^{\xi})$\n    is a weak cluster of the eigenvalues of both $\{T_{n}(f_{r}^{\xi})\}_{{n}}$ and $\{A_{n}\}_{{n}}$, in accordance with Definition \ref{def-cluster} and the subsequent Remark \ref{rem:clustering vs distribution}.""",
,"""\section{Linear Operations as Graph Network Layers}\label{sec:lin_gnn}\nThis section demonstrates how linear algebra computations can be\ncarried out utilizing the language and structure of graph neural networks.\n%he motivation is to demonstrate that many basic linear operations can be\n%easily represented as graph neural networks.\nThese familiar algorithms are selected to make the\ngraph neural network framework described above concrete.\n%This serves to make graph neural networks more approachable,\n%by explaining how to express familiar algorithms in the form of a\n%graph neural network.  \nWe note that the representation of these\noperations as graph neural networks are not unique, and multiple\nrepresentations may exist for a given calculation. As such, this\nis not meant to be an exhaustive list, but instead a general guide\nto setting up and representing different linear algebra operations as\ngraph neural networks.\n\nWe begin by briefly illustrating the natural relationship between sparse matrices\nand graphs. A weighted graph with $n$ vertices \ncan represent a square $n \times n $ matrix $A$. Each \nnonzero $A_{ij}$ defines a weight for a directed edge that\nemanates from vertex $j$ and terminates at vertex $i$.\nNotice that this definition uses self edges to represent \nentries on the matrix diagonal. Alternatively, one can \nomit self edges and instead store $A_{ii}$ at the $i^{th}$ vertex. \nThe inclusion or omission of self edges affects how linear algebra \noperations are represented and how data  propagates through\nthe network as different information is available during specific \nupdate/aggregation phases.  The algorithms in this section assume that\nthe graph neural network includes self edges, except for \nthe matrix-vector example in Section~\ref{sec:algo_matvec}.\nAdditionally, our edge orientation choice\nis natural for matrix-vector products\n$ y = A x $ where the $i^{th}$ entry of \n$y$ is defined by\n\begin{equation*}\n  y_{i} = \sum_{j \in A_{i*}} A_{ij}x_{j} \n\end{equation*}\nand $A_{i*}$ denotes the set of all nonzero entries in the $i^{th}$ matrix row.\nHere, information from neighboring edges must be gathered at the $i^{th}$ vertex.\nThis is accomplished by an aggregation function as the edges in the\n$i^{th}$ matrix row terminate at the $i^{th}$ vertex.\nMore generally, our edge orientation choice emphasizes that information \nthat directly influences vertex $i$ flows into vertex $i$.\n%, and thus defines the input to aggregation functions.\nFinally, we note that we only consider square matrices. However, \nnon-square matrices can be represented as bipartite graphs where rows \nand columns each have their own distinct set of vertices. \nBipartite graphs extensions of Algorithm~\ref{algo:GNBlock} are possible, but\nare beyond the scope of this educational survey.  \n%such as defining piecewise aggregation functions for rows and columns, or\n%perhaps having two separate aggregation phases for rows and columns.  \n\nWe start with foundational linear algebra algorithms:\nsparse matrix-vector product and a matrix-weighted norm. \nNext,\nwe describe three simple iterative methods: a weighted Jacobi linear solver,\na Chebyshev linear solver, and a power method eigensolver. We conclude\nwith some kernels used within an advanced algebraic multigrid\nlinear solver. \nAll of these examples follow\nAlgorithm~\ref{algo:GNBlock} for the structure of the graph neural\nnetwork. The details of each operation specific component, such as the\nupdate functions ($\phi_v$, $\phi_e$, $\phi_g$) and aggregation\nfunctions ($\rho_{e\rightarrow v}$, $\rho_{e\rightarrow g}$,\n$\rho_{v\rightarrow g}$), are summarized in\nTables~\ref{algo:spmv}-\ref{algo:soc_classic}.""",
"\\section{Introduction}\nArtificial intelligence (AI) and machine learning (ML) have drawn a great deal\nof media attention %in the early 2020s\n --- deep fake audio \\cite{Khanjani2023},\ntransformer-based AI chatbots \\cite{huggingfacetransformers}, stable diffusion AI\nimage generation and even AI Elvis \\cite{AIElvis} singing a\nsong have all inserted themselves into popular culture.  In \nscientific realms, medical image identification (e.g. detection of cancer) has\nalso generated media attention exposing the potential of ML technologies.  But in the\nrealm of applied mathematics the impact of AI/ML has been far less\nvisible to the general public. The goal of this paper is to provide an introduction to graph neural networks (GNNs) and\nto show how this specific  class of AI/ML algorithms can be used to represent \n(and enhance) traditional algorithms in numerical linear algebra. Associated code which implements the example GNNs is provided at \\url{https://github.com/sandialabs/gnn-applied-linear-algebra/}.\n\nIntroductions to neural network models in ML and AI typically focus\non deep neural networks (DNNs) or convolutional neural networks (CNNs).\n%GNNs tend to get less exposure in introductory AI texts than deep\n%neural networks (DNNs) or convolutional neural networks (CNNs), but\nHowever, GNNs are can be notably more appropriate \nfor many computational science tasks~\\cite{shukla2022scalable}.  DNNs and CNNs generally\nassume \\textit{structured} input data --- a vector that is a\n fixed size, or an image where pixels are aligned along Cartesian directions. \nWhile structured applications do exist in\ncomputational science (uniformly meshed problems for example), many other applications are \\textit{unstructured}, \nagain often associated with meshes.\nConsider a drawing of an object that an engineer wishes to\nsimulate on a computer.  After pre-processing, the geometry of the object will be subdivided a \nmesh. % up the object in the drawing. % and that mesh will not be %structured.  \nThe object may have holes, protrusions or a\ndisparity of features scales such that a structured, Cartesian mesh\ncannot be generated. As a result, an unstructured mesh is constructed.\nApplication of traditional DNN and CNN tools would likely not be possible for this mesh due to the \nthe unstructured nature of the data.  \nHowever, GNN models could still be employed on this geometry for associated AI/ML tasks. \n%that might be needed for this particular project.\n\nSo what is a GNN?  As suggested by the name, graph neural networks are\nbuilt on the concept of associating data with edges and vertices of a\n\\textit{graph}. For the above engineered object, the mesh itself can\ndefine the graph.  \n%For that matter, \nGraphs can also represent\nstructured data ---  for an image, pixels can be vertices and\nedges can be used to represent \nneighboring pixels.  \nThe general nature\nof unstructured graphs make them highly appropriate for a wide range\nof science and engineering applications.   Beyond the meshing example,\n another example of unstructured data arises from social network graphs where\neach edge represents a connection between two people. In these type of\ngraph networks, eigenvector \ncalculations provide useful information for determining the influence\nthat a vertex (e.g., a person) has on the rest of the network. \nGNNs were first proposed in~\\cite{gori_new_2005}\nand~\\cite{scarselli_graph_2009} as a means of adapting a convolutional\nneural\nnetwork to graph problems.  \n\nThe issue with basic neural networks (e.g. DNNs and CNNs) that GNNs\naddress is that they require a fixed input/feature size. \nThat is, a basic neural network can be viewed as a type of function\napproximation where the number of input/feature values \nto the function is always the same when either training the network or when using the network for inference.\nIf a graph has a simple repeatable interaction pattern, then \na basic neural network can be effectively applied to a fixed window (i.e., a\nsubset of the graph), which can be moved to address different portions\nof the network.  However, general graphs have no \nsuch repeatable pattern.  The key difficulty lies in the fact that the number of edges adjacent to each vertex can \nvary significantly throughout the network. In the case of a social\nnetwork graph, for instance, a popular person has many \nfriends while a loner might have few interactions.\nTo address this variability, GNNs include the notion of general aggregation functions that allow for a variable \nnumber of inputs. Simple aggregation function examples are summation or maximum which are well-defined\nregardless of the number of inputs. Aggregation functions are used to combine information from edges\nadjacent to a vertex. These functions can include a fixed number of learnable parameters. For example, \nan aggregation function might include both a summation and a maximum function \nwhose results are combined in a weighted fashion using a learnable\nweight. These are then combined with transformation functions\nassociated with edges and vertices, which can also include learnable parameters.\n\nIt is well known that many numerical linear algebra algorithms also\npossess a graph structure. For instance, in Section~\\ref{sec:algo_matvec} we detail how \nsparse matrix-vector multiplication transforms matrix entries as graph edge attributes and vector values as graph vertex\nattributes, into a new set of graph vertex attributes representing the product. This algorithm can be written as a GNN with\nprecise choices of aggregation and transformation functions. We\nprovide additional descriptions of standard iterative methods and\ncomponents of algebraic multigrid (AMG) preconditioners\nthat can be re-formulated using GNNs.  Some of the considered multigrid components include matrix-vector\nproducts, Jacobi relaxation, AMG strength-of-connection measures, and AMG interpolation operators. While the associated\nGNNs do not contain trainable parameters, they illustrate the flexibility of GNNs in representing traditional\nnumerical linear algebra components.\nThe potential for advances in AMG using GNN architectures is made apparent by\nincluding trainable parameters in the transformations. These parameters will enable learning complex nonlinear\nrelationships between the feature spaces, implicitly encoding the topological and numerical\nproperties of a sparse matrix. We explore some prototypical use cases in Section~\\ref{sec:iter-methods}.\nWe note that more complicated\napplications of GNNs for multigrid have been considered in the\nliterature and encourage the interested reader to look more deeply\n\\cite{Moore2021, luz2020,taghibakhshi2023_MGGNN,taghibakhshi2022_OptimizationBasedAMG}. As this paper is educational in nature, we are not\nproposing new AI/ML powered methods that significantly outperform\nexisting methods, but rather demonstrating how GNNs can allow us to\nmodify existing numerical methods while simultaneously highlighting some of the challenges that must be\naddressed.\n\n\\subsection{Developing Intuition: A Viral Example}\n\nTo make the discussion %and notation \nmore accessible, we present\nan example based on the spread of disease (a subject that we are all\nunfortunately familiar with). The intent is to help the reader develop\n%a better \nintuition about %each of the \ncomponents of the GNN algorithms\nand data structure. In the text that follows we call out explicitly where we\nuse the example. % to help develop intuition about the notation and the\n%ideas being presented. \nIf the notation and ideas are clear to the\nreader, these \\emph{Viral Interludes} can be skipped.  \n\nHere, we briefly describe the setup of the viral example. We consider\na community where each individual interacts on a weekly basis with\n a fixed set of community members. \n%In the context of graphs, the individuals\n% are represented by vertices, and their interaction by edges. \n The one-week period over which the set of interactions occurs is \n referred to as a cycle. Initially, within the community each individual\n has a probability of being infected by the disease. Simultaneously, a\n cure, which also (conveniently) diffuses through individual interactions,\nhas been distributed randomly through the population\\footnote{Perhaps additional individuals seek treatment after learning about it through peer\ninteractions.}.\nThe specific rate\n of diffusion for the disease and cure, as well as the effectiveness of the\n intervention is not known in advance, and must be estimated based on observations.\nThe goal then is to develop a model to\nassess each individual's risk of carrying the infection or carrying\nthe cure after a particular number of cycles using the information\nabout the interactions between individuals. \n\n%%%\\Eric{\n%%%Consider a\n%%%graph representation of a hypothetical viral outbreak where spread of\n%%%the virus is counteracted by graph diffusion of a cure. We would like\n%%%to track the progression of the virus and cure over several cycles\n%%%(weeks of time). The vertices of the graph are individuals in a\n%%%community, while edges between vertices represent interactions between\n%%%individuals. In this case, both the vertices and edges contain\n%%%information. Initially, before the first cycle, a vertex (an\n%%%individual) contains the probability an individual is currently\n%%%infected or not (denoted with values between 0 and 1) and the\n%%%probability the individual is carrying the cure or not (again denoted\n%%%with values between 0 and 1). In addition, in our hypothetical example\n%%%both the cure and the disease can spread from individual to individual\n%%%in a similar fashion. Within each cycle, brief interactions with an\n%%%individual with the cure (disease) are less likely to spread the cure\n%%%(disease), while longer interactions generate a much better chance of\n%%%spreading. This implies that initially the edges contain a unit of\n%%%time that describes the length of the interaction. We would like to\n%%%assess each individual's risk of carrying the infection or carrying\n%%%the cure after a particular number of cycles using the information\n%%%about the interactions between individuals. This model has\n%%%parameterized dynamics that define the expected impact of an\n%%%interaction of some duration on both individuals. Determining these\n%%%parameters may provide insight into the most effective disease spread\n%%%mitigation strategies. Finally, the state of the system, the\n%%%probability of possessing the disease and cure and interaction\n%%%duration, may change with each cycle, based on policy decisions or\n%%%natural evolution (mortality or immunity). These metrics may augment\n%%%the initial state of the graph with additional information. }",,
,,
"\\section{Introduction}\nAs educators, we aim to aid students' learning, promoting knowledge retention, problem-solving, and critical thinking skills beyond the course completion. Active learning, including the popular approach of flipped classrooms, enhances instruction and engagement. We have implemented flipped classroom pedagogy in teaching and learning linear algebra for college students from various majors in a private university in South Korea. Research on flipped classrooms in linear algebra is extensive and varied. Various designs, such as flipping single topics, entire courses, and workshops, have been proposed by Talbert~\\cite{talbert2014inverting}. The literature consistently demonstrates the effectiveness of flipped classrooms in linear algebra, such as the study that compares flipped classrooms with traditional lectures, consistently showing improved student understanding in the former~\\cite{love2014student}. Further evidence by Murphy~\\cite{murphy2016student} highlighted the superior comprehension, positive attitudes, enjoyment, and confidence of students in flipped classrooms. Additional support for positive student perspectives, participation, interest, and self-directed learning skills can be found in~\\cite{novak2017flip,nasir2020the,karjanto2019english,karjanto2022sustainable}.\n\nIn this study, we collected students' opinions and perceptions after experiencing flipped classroom pedagogy and analyzed the dataset using a machine learning algorithm called the support vector machine (SVM), a supervised learning algorithm used for classification and regression tasks~\\cite{cortes1995support,noble2006what,suthaharan2016support}. This involves training the SVM model on the dataset to learn patterns and relationships within the data. During training, the SVM identifies a hyperplane that best separates data points belonging to different classes. The goal is to find the hyperplane with the largest margin between the classes, which allows for better generalization to new, unseen data. In cases where the data is not linearly separable, the SVM can use kernel functions to transform the data into a higher dimensional space, making it possible to find a separating hyperplane. Once the SVM is trained, it can be used to predict the class of new, unseen data points based on their features. The model assigns data points to classes depending on which side of the learned hyperplane they fall on. In classification tasks, the SVM can provide a decision boundary that maximizes the separation between classes, while in regression tasks, it predicts a continuous value.\n\nWe are interested in investigating the following research question: Can machine learning algorithms, such as the SVM, identify a hyperplane that separates data points belonging to different genders? If yes, what will be the implication for our practice and implementation in a flipped classroom pedagogy?","""\section{Methodology}\n\n\subsection{Data Collection}\n\nTo understand student perspectives on the flipped classroom method, we crafted a survey comprised of 10 thoughtfully designed questions to collect their opinion and attitudes toward flipped classroom pedagogy in linear algebra. This survey was administered to the students at the semester's end, a time that allowed them to reflect on their entire experience. In the survey, students were prompted to express their level of satisfaction with each statement by selecting a number from~1 to~5, which correspond to responses ranging from strongly disagree to strongly agree.\n\nIn addition to gender identification, we also collect their year of study, their affiliated faculty or school, their expected grade, and their current grade point accumulation (GPA). We obtained a total of 108 valid returned questionnaires during Spring 2016. Recognizing the value of a larger dataset for deeper analysis, we also generated synthetic data. For this, we developed a Python script that mimicked the patterns seen in the original student responses. This approach ensured that our additional data closely mirrored real student feedback.\n\n\subsection{Implementation of Machine Learning}\nTo derive actionable insights from the survey data and to better understand the underlying patterns within the students' responses, we employed machine learning, specifically the support vector classifier (SVC). SVC is a part of the support vector machines (SVM) set of algorithms that are primarily used for classification tasks. At its core, the idea behind SVM is to find a hyperplane that best divides a dataset into classes. The mathematical formulation for this classifier can be expressed as follows:\n\begin{equation*}\n\min_{\mathbf{w}, b, \varepsilon} \frac{1}{2} \| \mathbf{w} \|^2 + C \sum_{i = 1}^{N} \varepsilon_{i},\n\end{equation*}\nsubject to the following conditions:\n\begin{equation*}\ny_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \varepsilon_i, \qquad \text{and} \qquad \varepsilon_i \geq 0, \qquad i = 1, 2, \cdots, N.\n\end{equation*}\nHere, $\mathbf{w}$ is the vector to the hyperplane, $\mathbf{x}_i$ are training vectors, $y_i$ are class labels, $b$ is the bias, $\varepsilon_i$ are nonnegative slack variables that allow for misclassification of the data points, and the positive constant $C$ is a regularization parameter. In our implementation, the two classes defined by the SVC were~0 and~1, where they represent male and female students, respectively.\n\nIntuitively, the SVM will try to maximize the margin between the closest data points (called support vectors) of the two classes (male and female). The hyperplane is the boundary that separates the classes, and its orientation and position are defined by the normal vector. Before feeding our data into the SVC, we implemented principal component analysis (PCA) for dimensionality reduction. PCA is a technique used to emphasize variation and highlight strong patterns in a dataset. It works by calculating the eigenvectors and eigenvalues from the covariance matrix of the data, which is used to determine the principal components (directions of maximum variance). The primary goal is to reduce the number of dimensions without much loss of information. By utilizing PCA, we intended to enhance the computational efficiency and possibly improve the classification performance of our SVC.""","\\section{Conclusion}\nOur analysis of survey data related to the flipped classroom approach revealed crucial insights into educational nuances. The variance in responses highlights the necessity for tailored educational strategies. For instance, if some elements of the flipped classroom are found less effective for females, educators have a clear opportunity to refine their resources to better cater to this group. Furthermore, the data indicates that aspects of this teaching methodology may resonate differently with each gender, urging curriculum designers to ensure inclusivity in their materials. The impressive accuracy of our machine learning model, combined with specific question variance, suggests fertile ground for more in-depth academic investigations. Potential avenues could include qualitative research methods, and exploring the root causes of varied responses. Moreover, institutions can enhance their pedagogical practices by creating a feedback loop with students, emphasizing the value of their opinions and the commitment to continuous educational improvement. This synthesis of educational practices and technological methodologies offers a promising route to tailored, effective learning experiences."
"Introduction\nThe framework of automatic differentiation has revolutionized machine learning.\nAlthough the rules that govern derivatives have long been known, automatically computing derivatives was a nontrivial process that required\n(1) efficient implementations of base-case primitive derivatives,\n(2) software abstractions (autograd and computation graphs) to compose these primitives into complex computations, and\n(3) a mechanism for users to modify or extend compositional rules to new functions.\nOnce libraries such as PyTorch, Chainer, Tensorflow, JAX, and others\n\\citep{tensorflow2015-whitepaper,jax2018github,maclaurin2016modeling,maclaurin2015gradient,paszke2019pytorch,tokui2015chainer}\nfigured out the correct abstractions, the impact was enormous.\nEfforts that previously went into deriving and implementing gradients could be repurposed into developing new models.\n\nIn this paper, we automate another notorious bottleneck for ML methods: performing large-scale linear algebra (e.g. matrix solves, eigenvalue problems, nullspace computations).\nThese ubiquitous operations are at the heart of principal component analysis, Gaussian processes, normalizing flows, equivariant neural networks, and many other applications\n\\citep{anil2020shampoo, cuturi2013sinkhorn, dao2019butterfly, finzi2023nivp, fu2023hippos, kovachi2021neuraloperator, li2018intrinsic, martens2015optimizing, nguyen2022s4nd, perez2018film}.\nModeling assumptions frequently manifest themselves as algebraic structure---such as diagonal dominance, sparsity, or a low-rank factorization.\nGiven a structure (e.g., the sum of low-rank plus diagonal matrices) and a linear algebraic operation (e.g., linear solves),\nthere is often a computational routine (e.g. the linear-time Woodbury inversion formula) with lower computational complexity than a general-purpose routine (e.g., the cubic-time Cholesky decomposition).\nHowever, exploiting structure for faster computation is often an intensive implementation process.\nRather than having an object $\\bm{A}$ in code that represents a low-rank-plus-diagonal matrix and simply calling ${\\tt solve}(\\bm{A}, \\bm{b})$,\na practitioner must instead store the low-rank factor $\\bm{F}$ as a matrix, the diagonal $\\bm{d}$ as a vector, and implement the Woodbury formula from scratch.\nImplementing structure-aware routines in machine learning models is often seen as a major research undertaking.\nFor example, a nontrivial portion of the Gaussian process literature is devoted to deriving specialty inference algorithms\nfor structured kernel matrices\n\\citep[e.g.][]{bonilla2007multitask,cunningham2008fast,gardner2018product,katzfuss2021general,loper2021general,snelson2005sparse,wilson2015kernel,wilson2014fast,kapoor2021skiing}.","""Methodology\nAs with automatic differentiation, structure-aware linear algebra is ripe for automation.\nWe introduce a general numerical framework that dramatically simplifies implementations efforts while achieving a high degree of computational efficiency.\nIn code, we represent structure matrices as \texttt{LinearOperator} objects which adhere to the same API as standard dense matrices.\nFor example, a user can call $\bm A^{-1} \bm b$ or $\texttt{eig}(\bm A)$ on any \texttt{LinearOperator} $\bm A$, and under-the-hood our framework derives a computationally efficient algorithm\nbuilt from our set of compositional \emph{dispatch rules} (see \autoref{tab:matrix_operations}).\nIf little is known about $\bm A$, the derived algorithm reverts to a general-purpose base case (e.g. Gaussian elimination or GMRES for linear solves).\nConversely, if $\bm A$ is known to be the Kronecker product of a lower triangular matrix and a positive definite Toeplitz matrix, for example, the derived algorithm uses specialty algorithms for Kronecker, triangular, and positive definite matrices.\nThrough this compositional pattern matching,\nour framework can match or outperform special-purpose implementations across numerous applications despite relying on only a small number of base \texttt{LinearOperator} types.\n\nFurthermore, our framework offers additional novel functionality that is necessary for ML applications (see \autoref{tab:libraries}).\nIn particular, we automatically compute gradients, diagonals, transposes and adjoints of linear operators,\nand we modify classic iterative algorithms to ensure numerical stability in low precision.\nWe also support specialty algorithms, such as SVRG \citep{johnson2013accelerating} and a novel variation of Hutchinson's diagonal estimator \citep{hutchinson1989stochastic},\nwhich exploit \emph{implicit structure} common to matrices in machine learning applications\n(namely, the ability to express matrices as large-scale sums amenable to stochastic approximations).\nMoreover, our framework is easily extensible in \emph{both directions}:\na user can implement a new linear operator (i.e. one column in \autoref{tab:matrix_operations}),\nor a new linear algebraic operation (i.e. one row in \autoref{tab:matrix_operations}).\nFinally, our routines benefit from GPU and TPU acceleration and apply to symmetric and non-symmetric operators for both real and complex numbers.\n\nWe term our framework \emph{CoLA} (\textbf{Co}mpositional \textbf{L}inear \textbf{A}lgebra), which we package in a library that supports both PyTorch and JAX.\nWe showcase the extraordinary versatility of CoLA with a broad range of applications in \autoref{subsec:dispatch} and \autoref{sec:applications}, including:\nPCA, spectral clustering, multi-task Gaussian processes, equivariant models, neural PDEs, random Fourier features,\nand PDEs like minimal surface or the Schr\\""","Conclusion\nWe have presented the CoLA framework for structure-aware linear algebraic operations in machine learning applications and beyond. Building on top of dense and iterative\nalgorithms, we leverage explicit composition rules via multiple dispatch to achieve algorithmic speedups\nacross a wide variety of practical applications.\nAlgorithms like SVRG and a novel variation of Hutchinson's diagonal estimator exploit implicit structure common to large-scale machine learning problems.\nFinally, CoLA supports many features necessary for machine learning research and development, including memory efficient automatic differentiation, multi-framework support of both JAX and PyTorch, hardware acceleration, and lower precision.\n\nWhile structure exploiting methods are used across different application domains, domain knowledge often does not cross between communities.\nWe hope that our framework brings these disparate communities and ideas together, enabling rapid development and reducing the\nburden of deploying fast methods for linear algebra at scale.\nMuch like how automatic differentiation simplified and accelerated the training of machine learning models---with custom autograd functions as the exception rather than the rule---CoLA has the potential to streamline scalable linear algebra."
"\\section{Introduction}\n\nThe numerical solution of initial value problems for higher-index\ndifferential-algebraic equations (DAEs) is known to be a delicate\nproblem. Classical numerical methods like finite difference or collocation\nmethods loose their convergence properties known from their application\nto explicit ordinary differential equations (ODEs). While appropriate\nmodifications of classical methods are well established for index-1\nand special classes of low-index DAEs featuring usable special structures, their use will fail for\nmore general higher-index DAEs (e.g. \\cite{BCP96,HaiWaII,KuhMeh06,LMT,TMS14}).\n\nKnown solvers for general higher-index DAEs as in \\cite{RaRh94,KuMeRaWe97,EstLam21} are based on the generation and evaluation of so-called derivative array functions, which is extremely costly. The arrays are provided manually or by automatic differentiation. Then the index of the given DAE is reduced \\emph{a-priori} or \\emph{a-posteriori} and the necessary derivatives are determined using the derivative  array functions.\nBy the way,  these solvers need so-called \\emph{consistent initial values} to start, but the computation of consistent initial values is by itself a hard problem \\cite{LePeGe91,EstLam21b}.\n\nHigher-index DAEs are known to lead to ill-posed problems in natural function spaces.\nIn order to overcome the problems related to the ill-posedness of\nhigher-index DAEs, in \\cite{HaMaTiWeWu17,HaMaTi19,HaMa21C} we proposed\na family of least-squares methods for solving them. These methods do not need\nany preprocessing nor higher derivatives of the problem data as those\nproposed earlier \\cite{RaRh94,KuMeRaWe97,EstLam21}. However, this\nmethod needs \\emph{accurate initial conditions} in the sense of \\cite[Definition 2.3]{LMW}\nand \\cite[Theorem 2.52]{LMT}.\n\nAccurate initial conditions for DAEs are something else than consistent initial values.\nRoughly speaking, they consist of exactly $l$ conditions according to the degree of freedom $l$ in order to capture exactly one solution. To formulate reasonable $l$ conditions requires, of course, the knowledge of $l$ itself and, additionally information on canonical subspaces, and hence the determination of accurate initial\nconditions is also a nontrivial problem. In a recent paper \\cite{HaMa231},\nwe were able to substantiate a constructive way  for just this problem.  \n\nThe present paper is devoted to the construction of a numerical algorithm\nbased on the results of \\cite{HaMa231} and its analysis for linear\nDAEs. Then this algorithm will be combined with the time-stepping\nmethod proposed in \\cite{HaMaPad121} in order to establish a fully numerical initial\nvalue solver for linear higher-index DAEs. The latter requires accurately\nstated transfer conditions from one step to the next. We will use\nthe algorithm developed below for this purpose.\n\nThe paper is structured as follows: In Section~\\ref{sec:AIC} we provide basic facts concerning the flow defined by the DAE, the canonical subspaces,  accurate initial and transfer condition.\nIn Section~\\ref{s.Algoritm} we first summarize the method proposed in \\cite{HaMa231} and develop an\nimplementable algorithm for determining a matrix capable of providing\naccurate initial conditions and discuss properties of the associated tools.\nSection~\\ref{s.PropAICnum} is devoted\nto a deeper analysis of the algorithm's properties. The algorithm\nhas been implemented with the goal of robustness and efficiency. Some\numerical examples of its performance are provided.\nFinally, in Section~\\ref{sec:An-initial-value} the convergence of a time stepper based on the least-squares\nmethod of \\cite{HaMaPad121} combined with the algorithm developed\nin the present paper is shown and some numerical results are presented. Some technical details are\ncollected in the appendix.\n\nThroughout the paper we will use the Euclidean norm $\\lvert x\\rvert$ for vectors $x\\in\\R^m$ and the spectral norm\n$\\lvert M\\rvert$ for matrices $M\\in\\R^{m\\times l}$.","""\section{Formulating accurate initial and transfer conditions\label{sec:AIC}}\n\n\subsection{The flow structure of a regular DAE}\label{subs.regular}\n%%%%\nIn this paper, we are interested in solving initial value problems for\nlinear DAEs\n\begin{equation}\nA(t)(Dx(t))'+B(t)x(t)=q(t),\quad t\in[a,b].\label{eq:DAE}\n\end{equation}\nHere, $x:[a,b]\rightarrow\R^{m}$ is the unknown vector-valued function\ndefined on the finite interval $[a,b]$. In order to apply collocation methods later we suppose continuously differentiable solutions satisfying the equation \eqref{eq:DAE} pointwise.\nThe right-hand side $q:[a,b]\rightarrow\R^{m}$\nshall be sufficiently smooth. Moreover, sufficient smoothness is also\nassumed of the coefficient matrix functions $A:[a,b]\rightarrow\R^{m\times k}$\nand $B:[a,b]\rightarrow\R^{m\times m}$. With the intention of creating an appropriate collocation approach we assume that the DAE features\nan explicit splitting of the components of $x$ into differentiated\nand nondifferentiated (algebraic) ones. Without restricting generality\nwe will assume that $x_{1},\ldots,x_{k}$ are the differentiated components\nwhile $x_{k+1},\ldots,x_{m}$ are the algebraic ones. Therefore, we\nset\n\[\nD\in\R^{k\times m},\quad D=[I_{k},0]\n\]\nwith the identity matrix $I_{k}\in\R^{k}$. Moreover, we assume $\rank A(t)\equiv k$\nfor all $t\in[a,b]$.\nIt is convenient\nto rewrite (\ref{eq:DAE}) in the so-called \emph{standard form}\n\begin{equation}\nE(t)x'(t)+F(t)x(t)=q(t),\quad t\in[a,b]\label{eq:EF}\n\end{equation}\nwhere $E=AD$ and $F=B$.\nBoth formulations are equivalent if a solution of (\ref{eq:DAE})\nhas also differentiable algebraic components. This is what we assume\nin the following.\n\medskip\n\nIn the present paper we presume regularity in the sense of \cite[Definition 3.3]{HaMa231} as basic term, which is defined via the level-wise reduction from \cite[Sections 12 and 13]{RaRh02}\footnote{For  details we refer also to Section \ref{subs.Alg} below.}. If the DAE is regular with index $\mu\in \Natu$, then owing to \cite[Theorem 4.3]{HaMa231} also the  regular strangeness index $\mu^S$ \cite[Definition 3.15]{KuhMeh06} and the regular tractability index $\mu^T$ \cite[Definition 2.25]{LMT} are well-defined, and $\mu^{S}=\mu-1$, $\mu^{T}=\mu$ . In particular, this allows to combine tools and results of the related concepts.\n\medskip\n\nAs pointed out in \cite{HaMa231} there are two particular continuously time-varying subspaces which shape the flow structure of a regular DAE, namely the so-called \emph{flow subspace $\Scan$} and its \emph{canonical complement $\Ncan$}. \nMore precisely,\nthe subspace $\Scan(\bar{t})$, $\bar{t}\in[a,b]$, is the set of all\npossible function values of solutions to the homogeneous DAE $A(Dx)'+Bx=0$,\nthat is\n\begin{align*}\n\Scan(\bar{t})= & \{\bar{x}\in\R^{m}:\text{there is a solution }x:(\bar{t}-\delta,\bar{t}+\delta)\cap[a,b]\rightarrow\R^{m}\\\n & \text{ of the homogeneous DAE such that }x(\bar{t})=\bar{x}\}.\n\end{align*}\nThus, $\Scan(\bar t)$ is just the set of consistent initial values for the homogeneous DAE at time  $\bar{t}$. \nWhile $\Scan$ can be seen as a generalization to the state space  or configuration space for regular ODEs, the second subspace does not occur at all in the ODE theory and first appears  in the DAE context.\nThe second subspace $\Ncan(\bar{t})$ is a special pointwise complement to the flow subspace \n$\Scan(\bar{t})$,\n\[\n\R^{m}=\Ncan(\bar{t})\oplus\Scan(\bar{t}),\quad \bar t\in[a,b]\n\]\nsuch that each initial value problem\n\[\nA(t)(Dx(t))'+B(t)x(t)=q(t),\quad t\in(\bar{t}-\delta,\bar{t}+\delta)\cap[a,b],\quad x(\bar{t})-\bar{x}\in\Ncan(\bar{t}),\n\]\nwith arbitrary  sufficiently smooth $q$ and $\bar{x}\in\R^{m}$ is uniquely\nsolvable without requiring any consistency condition for $q$ and its derivatives at $\bar t$ to be satisfied. These conditions define $\Ncan(\bar{t})$ uniquely.\nIt is called the \emph{canonical complement} to the flow subspace at $\bar{t}$. \n\nIt should be underlined that both time-varying canonical subspaces of a regular DAE have constant dimension on $[a,b]$. Furthermore, the inclusion $\ker D\subseteq \Ncan(t)$ is valid.\n\nThe continuous  projector-valued function $\pPi_{\textrm{can}}:[a,b]\rightarrow\R^{m\times m}$ associated with the above  decomposition of $\R^m$ having the property \n\begin{align*}\n \Scan(t)=\im\pPi_{\textrm{can}}(t),\quad \Ncan(t)=\ker\pPi_{\textrm{can}}(t), \quad t\in [a,b],\n\end{align*}\nis called \emph{canonical projector function associated with the DAE}. In the context of the tractability index it plays an important role as completely decoupling tool \cite[Chapter 2]{LMT}. \nThe general solution of the regular DAE \eqref{eq:DAE} is described as\n\begin{align}\n x(t)=X(t,a)x_a+x_q(t),\quad t\in [a,b]   \label{eq:A}\n\end{align}\nin which $X(\cdot,a):[a,b]\rightarrow \R^{m\times m}$ is the maximal-size fundamental solution normalized at $a$, that means, it is the uniquely determined solution of the matrix IVP\n\begin{align*}\n A(DX)'+BX=0, \quad X(a)=\pPi_{\textrm{can}}(a),\n\end{align*}\nthe function  $x_q:[a,b]\rightarrow \R^{m}$ is the unique solution of the IVP\n\begin{align*}\n A(Dx)'+Bx=q, \quad x(a)\in N_{\textrm{can}}(a),\n\end{align*}\nand $x_a\in \R^m$ is the arbitrary free constant that must be fixed in order to capture a specific solution in the flow. Regarding the favorable characteristics\n\begin{align}\n \im X(t,a)=\im \pPi_{\textrm{can}}(t)= \Scan(t), \;  \ker X(t,a)=\ker  \pPi_{\textrm{can}}(a)= \Ncan(a), \; t\in [a,b],\n \label{eq:B}\n\end{align}\nwe know that\n\begin{align*}\n \pPi_{\textrm{can}}(t)x(t)&=X(t,a)x_a+ \pPi_{\textrm{can}}(t)x_q(t),\\\n (I-\pPi_{\textrm{can}}(t))x(t)&=(I-\pPi_{\textrm{can}}(t))x_q(t)=:v_q(t),\quad t\in [a,b],\n\end{align*}\nand hence\n\begin{align}\label{eq:ICa}\n  \pPi_{\textrm{can}}(a)x(a)=\pPi_{\textrm{can}}(a)x_a,\quad \n  (I-\pPi_{\textrm{can}}(a))x(a)=v_q(a).\n\end{align*}\n%\nAs shown in \cite[Section 2.6.2]{LMT}, the term $\pPi_{\textrm{can}}x_q$ depends on $q$ itself, but not at all on its derivatives, while certain components of $q$ and all involved derivatives of $q$ are gathered into  the function $v_q$.\nWe emphasize that the function $v_q$ is invariant of the initial data $a$ and $x_a$.\n\nThe component $(I-\pPi_{\textrm{can}}(a))x_a$ does not have any impact here, only the component $\pPi_{\textrm{can}}(a)x_a\in \Scan(a)$ is significant.\nLet $l$ denote the constant dimension of the flow subspace $\Scan(t)$. It is evident that $l$ is the number of linearly independent solutions of the homogeneous DAE, that is the dynamical degree of freedom of the DAE.\n\nBy construction, one has\n\begin{align*}\n \Ncan(t)&\supseteq\ker D=\ker E(t),\\\n \Scan(t)&\subseteq\{z\in\R^m: B(t)z\in\im A(t)D\}=\{z\in\R^m: F(t)z\in\im E(t)\},\\\n l&\leq k=\rank D=\rank E(t),\n\end{align*}\nbut $l$ itself is unknown, and neither the canonical subspaces nor the canonical projector function are given \n\emph{a-priori}.  \n\n\nWe close this section by recalling the well-known fact \cite{GM86} that in the case of an index-one DAE it holds that \n\begin{align*}\n \Ncan(t)&=\ker D=\ker E(t),\\\n \Scan(t)&=\{z\in\R^m: B(t)z\in\im A(t)D\}=\{z\in\R^m: F(t)z\in\im E(t)\},\\\n l&=k=\rank D=\rank E(t).\n\end{align*}""","\\section{Conclusions and outlook}\n\nIn the present paper, we proposed an implementation of the reduction\nprocedure of \\cite[Section 12]{RaRh02}, \\cite{HaMa231} in order\nto provide practical means to formulate accurate initial and transfer conditions for regular linear higher-index DAEs. This is a problem that does not occur at all with regular ODEs, but only emerges in the context of DAEs. To the best of the authors' knowledge, this is the first attempt ever to replace the rather difficult hand provision as in \\cite{HaMa231} by a numerical method.\nStable\nand accurate algorithms for that are investigated. The efficiency\nand accuracy of the proposal has been demonstrated in numerical\nexamples. \nThe implementation has then be used in a time step method which is\nbased on the least-squares approach advocated in \\cite{HaMaPad121}.\nThe latter requires transfer conditions which must be formulated in\nterms of accurate initial conditions. We showed that the combination\nof these two approaches gives rise to an efficient initial value solver\nfor higher-index DAEs. A numerical example provides some insight\ninto the behavior of the method demonstrating its efficiency and accuracy.\n\nA crucial assumption for the convergence of the time stepping approach\nis Conjecture~AIC which is based on heuristic estimations and our numerical experiments. However, a strict proof of the convergence in terms of the\napproximation polynomials and the discretization stepsize is missing so far. A thorough\nproof would eliminate this gap in our derivations. Additionally, a\ncareful further rounding error analysis of Algorithm~\\ref{alg:Reduction-procedure-1}\nmay contribute  to a better understanding of the numerical experiments.\nIn that respect, Proposition~\\ref{prop:ChebCond} may be of use.\n\nWe believe that this work represents an appreciable step towards a fully numerical method for IVPs in higher-index DAEs which would undoubtedly be a great benefit. So far, there are no fully numerical methods for general higher-index DAEs available. \nThe previously known methods for general higher-index DAEs  use so-called derivative arrays. This is very  effort-intensive and there are no usable estimates of errors.\nThere are approaches \n which evaluate derivative arrays generated by automatic differentiations as in \\cite{EstLam21,EstLam21b} and \nthose that work with a derivative array provided from the beginning and a so-called remodeling into an index-one DAE \\cite{KuhMeh06} as adumbrated in Remark \\ref{r.array} above. Here we quote the assessment from \\cite[p. 234]{TMS14} : \\textit{to perform this reformulation... , there is currently no way to avoid differentiating the  whole model and to use full derivative arrays.} \nThis shows and emphasizes the need to develop appropriate numerical methods together with error analyses."
"\\section{Introduction and preliminaries}\nThis paper is motivated by trying to refine or generalize two pieces of work. \n\n\\vspace{2mm}\n\\noindent\n(A) The first is Theorem 1.1 in \\cite{Pillay-PV} by the second author, which says that an algebraically closed differential field $K$ has no proper Picard-Vessiot extensions \nif and only if Kolchin's constrained cohomology set $H_{\\partial}^{1}(K, G)$ is trivial for any linear differential algebraic group $G$ defined over $K$, \nmeaning that any torsor $X$ for a linear differential algebraic group $G$, all over $K$, has a $K$-point. \n\n\\vspace{2mm}\n\\noindent\n(B) The second is the theorem of Serre \\cite{Serre} that a field $K$ (of characteristic $0$ for simplicity) is {\\em bounded}, namely has finitely many extensions of degree $n$ for each $n$, \nif and only if the Galois cohomology set $H^{1}(K,G)$ is finite for every linear algebraic group $G$ over $K$. \n\n\\vspace{2mm}\n\\noindent\nAt the end of \\cite{Pillay-PV}, the second author asked explicitly for a differential field version of (B). This paper will touch on this last problem, but will mainly\nbe concerned with refinements of (A).  We will be giving an informal account of our results in this introduction, followed by  more precise definitions and references.\n\n\\vspace{2mm}\n\\noindent\nIn (A) an important case is when  $G$ is finite-dimensional.\nLikewise in (B) a  first case is  when  $G$ is finite. See Proposition 8 in \\cite{Serre} (Chapter 3, Section 4.1) written at the level of generality of profinite groups. \n\n\\vspace{2mm}\n\\noindent\nWe will often refer to differential algebraic groups as $DAG$'s, linear differential algebraic groups as $LDAG$'s, and finite-dimensional linear differential algebraic groups\nas $fdLDAG$'s. \n\n\\vspace{2mm}\n\\noindent\nThis paper is about what we may call ``differential field arithmetic\",,
"\\section{Introduction}\nThe notion of generically stable types were introduced  by Hrushovski and Pillay to describe the ``stable-like'' behavior in NIP environment \\cite{HP-NIP-inv-measure}. Briefly, a type over a monster model, called a global type, is generically stable if it is finitely satisfiable in and definable over every small submodel. The theory of algebraically closed valued fields, denoted by ACVF, is considered a typical ``stable-like'' NIP theory, since the non-trivial generically stable types exist. As a contrast, for the purely unstable NIP theories, say  $p$-adically closed fields ($p$CF), there is no non-trivial generically stable type. \n\nIn this paper, we study the structures which are elementarily equivalent to $\\C((t))$, the field of form Laurent series over the complex numbers.  It is natural to consider $\\Th(\\C((t)))$ as a mixture of the ``stable-like'' theory ACVF and the ``purely unstable'' theory $p$CF. To see this, let $K\\models \\Th(\\C((t)))$, $K_1\\models $ACVF and $K_2\\models p$CF, with residue fields $\\kk$, $\\kk_1$ and $\\kk_2$; value groups $\\Gamma$, $\\Gamma_{1}$ and $\\Gamma_{2}$, respectively. Then $\\kk\\equiv \\kk_{1}$ and  $\\Gamma\\equiv \\Gamma_{2}$. On the other side, according to Ax-Kochen-Ershov\\cite{Ax-Kochen, Ershov}, the theory of a henselian valued field is completely determined by the theories of its residue field and its value group if its residue field has characteristic $0$.\n%J. Ax and S. Kochen, Diophantine problems over local fields. I. Amer. J. \n%J. Ershov, On elementary theories of local fields, Algebra I Logika Sem. 41965 no. 2, 5–30. Math. 87 1965 605–630.\nLet $K\\equiv \\C((t))$. We study the definable subsets of $\\cO_K$, our first result is:\n\\begin{theorem}\nLet $\\res: K\\to\\kk$ be the residue map. Suppose that $K\\equiv \\C((t))$, $X\\sq \\cO_K$ a definable set. Then there is finite $Z\\sq \\kk $ such that either $X\\sq \\res^{-1}(Z)$ or $\\res^{-1}(\\kk \\backslash Z)\\sq X$.\n\\end{theorem}\nRecall from \\cite{HP-NIP-inv-measure} that a group $G$ defined in a monster model $\\M$ is generically stable if there is a generically stable type for $G$, where a global type $p\\in S_G(\\M)$ is a generically stable type for $G$ if it is generically stable for each $g\\in G$.   Y. Halevi showed that $\\GL(n,\\cO_K)$, the invertible $n$ by $n$ matrices over $\\cO_K$, is generically stable if $K$ is an algebraically closed valued field.  In this paper, We generalize this result to the case where $K\\equiv \\C((t))$. Let $\\K$ be a monster model of $\\Th(\\C((t)))$, $\\cO_\\K$ the valuation ring of $\\K$, $\\U$ be the units of $\\cO_\\K$, and $\\GL(n,\\cO_\\K)$ the invertible $n$ by $n$ matrices over $\\cO_\\K$. Then we prove the following theorem:\n\\begin{theorem}\nLet $G$ be $\\cO_\\K$, or $\\U$, or $\\GL(n,\\cO_\\K)$, then \n\\begin{description}\n   \\item   [(i)] $G$ has a (unique) generically stable type $p$.\n   \\item   [(ii)] $p$ is $G$-invariant. As a consequence, $G=G^{00}$.\n   \\item  [(iii)] $p$ is dominated by $\\res(p)$ via the residue map, where $\\res(p)$ is the image of $p$ under the residue map.\n\n\\end{description}\n\\end{theorem}\n\nThe paper is organized as follows. For the rest of this section, we recall some basic facts around valued fields and $\\mathbb{C}((t))$ and introduce notations we use. In section \\ref{sec-def-val-ring}, we prove that every definable subset of the valuation ring $\\cO_K$ is either res-finite or res-cofinite. In section \\ref{sec-Gen-O_k-U}, we give a generically stable type $p_{trans,\\K}$ of $\\cO_{\\K}$ and $\\U$. From this onwards, in section \\ref{sec-Gen-GLn}, we show that for every $n$, $\\GL(n,\\cO_\\K)$ has a unique generically stable type that is dominated by its image under the residue map.  ","""\section{Definable subsets of the valuation ring}\label{sec-def-val-ring}\nIn this section, we will study the definable subsets of $\cO_K$. Let $f(x),g(x)\in K[x]$ be polynomials. Suppose that  \n\[\nf(x)=b_0+\cdots+b_nx^n.\n\]\nLet $e_f\in \{b_0,\cdots,b_n\}$  \nsuch that \n\[\nv(e_f)=\min\{v(b_i)|\ i=0,\cdots,n\}  \n\]\nLet $f^*=f/e_f$, then $f^*\in \cO_K[x]$. It is easy to see that $f$ and $f^*$ have the same zeros.  \n\n\begin{lemma}\label{lemm-f(x)=0}\n    Suppose that  $a\in \cO_K$ such that $f(a)\in \cO_K$. If $\res(f)(\res(a))\neq 0$, then  $f(a)\neq 0$.\n\end{lemma}\n\begin{proof}\n    We many replace $f$ by $f^*$ since they have the same zeros. Since $\res(f(a))=\res(f)(\res(a))$,  the Lemma follows.\n\end{proof}\n\nWe conclude directly from the above Lemma that \n\begin{corollary}\label{cor-res(f=0)}\n Let $X=\{a\in \cO_K |\ f(a)=0\}$ and $Z=\{u\in \kk |\ \res(f)(u)=0\}$, then $X\sq \res^{-1}(Z)$ and $\res^{-1}(\kk\backslash Z)\sq \cO_K\backslash X$.\n\end{corollary}\n\n\begin{lemma}\label{lemma-v(f)}\n    Suppose that  $a\in \cO_K$. If $\res(f^*(a))\neq 0$, then  $v(f(a))=v(e_f)$.\n\end{lemma}\n\begin{proof}\n     Clearly, $v(f(a))=v(e_f)+v(f^*(a))$. Since $\res(f^*(a))\neq 0$, we have $v(f^*(a))=0$. So $v(f(a))=v(e_f)$ as required.\n\end{proof}\n\n\begin{corollary}\label{cor-v(f)-leq-v(g)}\n  Let $g\in K[x]$.  If $X=\{a\in \cO_K|\ v(f(a))\leq v(g(a))\}$  then there is a finite set $Z\sq \kk$ such that  either $X\sq \res^{-1}(Z)$ or $\res^{-1}(\kk\backslash Z)\sq X$.  \n\end{corollary}\n\begin{proof}\nSuppose that  $g(x)=c_0+\cdots+c_mx^m$. Take  $e_g\in \{c_0,\cdots,c_m\}$ such that $v(e_g)=\min\{v(c_j)|\ j=0,\cdots,m\}$. Let $g^*=g/e_g$ and \n\[\nZ=\{c\in \kk|\ (\res(f^*)(c)=0)\vee (\res(g^*)(c)=0)\}.\n\]\nThen $Z$ is finite. By Lemma \ref{lemma-v(f)}, if $v(e_f)\leq v(e_g)$, then  $\res^{-1}(\kk\backslash Z)\sq X$. If $v(e_f)> v(e_g)$, then  $ X \sq \res^{-1}(Z)$. \n\end{proof}\n\n\n\begin{lemma}\label{lemma-Pn(f)}\n    If $X=\{a\in \cO_K|\ K\models P_n(f(a))\}$, then   there is a finite set $Z\sq \kk$ such that either $X\sq \res^{-1}(Z)$ or $\res^{-1}(\kk\backslash Z)\sq X$.  \n\end{lemma}\n\begin{proof}\n  Let $Z=\{c\in \kk|\ \res(f^*)(c)=0\}$. By Lemma \ref{lemma-v(f)}, $v(f(a))=v(e_f)$ whenever $a\notin \res^{-1}(Z)$. By Corollary \ref{cor-nth-power}, if $v(e_f)\in n\Gamma$, then $\res^{-1}(\kk\backslash Z)\sq X$, otherwise, $X\sq \res^{-1}(Z)$. \n\end{proof}\n\nLet $Z\sq \kk$ and $X\sq \cO_K$, then it is easy to see that $ X \sq \res^{-1}(Z)$ implies $\res^{-1}(\kk\backslash Z)\sq (\cO_K\backslash X)$, and $\res^{-1}(\kk\backslash Z)\sq X$ implies \n$\cO_K\backslash X \sq \res^{-1}(Z)$. \nSummarising  Corollary \ref{cor-res(f=0)},  Corollary \ref{cor-v(f)-leq-v(g)},   and Lemma \ref{lemma-Pn(f)}, we conclude that \n\begin{corollary}\label{coro-res-1(Z)-finite-cofinite}\n    Suppose that $X$ is either defined by an $L_{VR}(K)$ formula of \textbf{Type (i)-(iii)}  or negation of a formula of \textbf{Type (i)-(iii)}. Then there is a finite set $Z\sq \kk$ such that either $X\cap \cO_K\sq \res^{-1}(Z)$ or $\res^{-1}(\kk\backslash Z)\sq X\cap \cO_K$. In particular $\res(X\cap \cO_K)$ is either finite or cofinite in $\kk$.\n\end{corollary}\n\n\n\n\n\begin{theorem}\label{thm-res(X)-finite-cofinite}\n    Let $Y\sq \cO_K$ be definable, there is a finite set $Z\sq \kk$ such that $\res^{-1}(\kk\backslash Z)\sq Y$ if $\res(Y)$ is infinite. In particular, $\res(Y)$ is either finite or cofinite. \n\end{theorem}\n\begin{proof}\nBy quantifier elimination and   Remark \ref{rmk-atomic formulas},  we may assume that $Y$ is a finite boolean combination of sets defined by formulas of \textbf{Type (i)-(iii)}.  Suppose that  $Y=\bigcap_{i=1}^r\bigcup_{j=1}^s Y_{i,j}$, where each $Y_{i,j}$ is either defined by a formula of \textbf{Type (i)-(iii)} or negation of a formula of \textbf{Type (i)-(iii)}.  By Corollary \ref{coro-res-1(Z)-finite-cofinite}, for each $Y_{i,j}$, there is a finite set $Z_{i,j}\sq \kk$ such that either $Y_{i,j}\sq \res^{-1}(Z_{i,j})$ or $\res^{-1}(\kk\backslash Z_{i,j})\sq \cO_K\backslash Y_{i,j}$. Let $Z=\bigcup_{i\leq r,j\leq s}Z_{i,j}$, then we have that either $Y_{i,j}\sq \res^{-1}(Z )$ or $\res^{-1}(\kk\backslash Z )\sq \cO_K\backslash Y_{i,j}$ for each $i\leq r$ and $j\leq s$. Clearly, $\res(Y)\sq \bigcap_{i=1}^r\bigcup_{j=1}^s \res(Y_{i,j})$.\n\n%If $\res(Y)$ is finite, then  there is $i_0\leq r$ such that $\res(Y_{i_0,j})$ is finite for each $j\leq s$. Thus  $\bigcup_{j=1}^sY_{i_0,j}$ is contained in $\res^{-1}(Z)$, so $Y$ is contained in $\res^{-1}(Z)$. \n\nIf $\res(Y)$ is infinite, then  for each $i\leq r$ there is $j(i)\leq s$ such that $\res(Y_{i,j(i)})$ is infinite, hence is cofinite in $\kk$. Thus each $\res^{-1}(\kk\backslash Z)$ is contained in $Y_{i,j(i)}$ for each $i\leq r$, we conclude that  so $\res^{-1}(\kk\backslash Z)$ is contained in $Y$. \n\n\end{proof}\n\n\begin{definition}\label{def-res-finite-cofinite}\n    We call a definable subset $X$ of $K$ res-finite (resp. res-cofinite) if $\res(X\cap \cO_K)$ is finite (resp. res-cofinite). We call an $L_{VR}(K)$ formula $\varphi(x)$ is res-finite (resp. res-cofinite) if $\varphi(K)$ is res-finite (resp. res-cofinite).\n\end{definition}\n\nBy Theorem \ref{thm-res(X)-finite-cofinite}, every definable subset of $K$ is either res-finite or res-cofinite.\n\n\begin{corollary}\label{coro-res-cofinite-meet-every-model}\n    Let $X$ be a definable subset of $\cO_K$, $K_0$ an elementary submodel of $K$. If $X$ is res-cofinite, then $X\cap K_0\neq \emptyset$.\n\end{corollary}\n\begin{proof}\n    By Theorem \ref{thm-res(X)-finite-cofinite}, there is a cofinite subset $Z^*\sq \kk$ such that $\res^{-1}(Z^*)\sq X$. Let $\kk_0$ be the residue field of $K_0$. Clearly, $Z^*\cap \kk_0$ is nonempty. Take any $u\in Z^*\cap \kk_0$  and $\Tilde{u}\in \cO_{K_0}$ such that $\res(\Tilde{u})=u$, then $\Tilde{u}\in X\cap K_0$.\n\end{proof}\n%7.4Yao""",\\section*{Acknowledgements}\nThe research is supported by The National Social Science Fund of China (Grant No. 20CZX050).
"\\section{Introduction}\n\nWe consider the linear differential-algebraic equation with constant coefficients of the form\n\\begin{equation}\\label{dae}\nE\\dot{x}(t)=Ax(t)+f(t), \\quad t\\in (0,T),    \n\\end{equation}\nsubject to the boundary condition\n\\begin{equation}\\label{BC}\nBx(0)+Cx(T)=d.    \n\\end{equation}\nHere $E, A,B,C\\in \\mathbb{R}^{n\\times n}$, $d\\in\\mathbb{R}^n$, $T>0$. We suppose that the matrix pair $(E,A)$ is regular, i.e. $\\text{det}(\\lambda E - A)\\neq 0$ for some $\\lambda\\in\\mathbb{C}$.\n\nBy a solution of the boundary value problem \\eqref{dae}, \\eqref{BC} we mean a function $x\\in C^1([0,T],\\mathbb{R}^n)$ satisfying equation \\eqref{dae} and the boundary condition \\eqref{BC}.\n\nDifferential-algebraic equations have become widespread over the last decades, being a tool for modeling and simulation of dynamical systems with constraints in numerous applications \\cites{Kunkel:2006,brenan1995numerical,ascher1998computer,lamour2013differential, samoilenko2000linear,boichuk2004generalized,riaza2008differential}. The theory of boundary value problems for differential-algebraic equations started to develop by applying modified versions of the shooting and collocation methods designed for boundary value problems for ordinary differential equations \\cites{marz1984difference,clark1989numerical,lamour1991well,lamour1997shooting,BAI1991269,bai1992modified,ascher1992projected,stover2001collocation,kunkel2002symmetric}. In \\cite{amodio1997numerical},  P.\\ Amodio and F.\\ Mazzia  studied problem \\eqref{dae}, \\eqref{BC} by the method of boundary values. R.\\ März applied the method of projectors and methods of perturbation theory \\cite{marz1996canonical, marz2004solvability, marz2005characterizing} to problem \\eqref{dae}, \\eqref{BC}.  The monograph by R.\\ Lamour, R.\\ März, and C.\\ Tischendorf  \\cite{lamour2013differential}, devoted to the projector study of differential-algebraic equations, provides a detailed review in this field.\n\nA series of papers by C. Trunk et al. \\cite{gernandt2023characteristic, berger2016linear, leben2021finite, berger2021linear} investigate possibilities of generalization and extension of the Kronecker canonical form to differential-algebraic equations with rectangular matrices.  A number of methods and approaches have been developed for constructing their solutions. \n\nHowever, the methods developed for solving problem \\eqref{dae}, \\eqref{BC} may not always be applicable for a wide class of boundary conditions. As a consequence, this requires the development of new methods or modification of known methods of the theory of differential equations, which would be applicable to differential-algebraic equations and would be of constructive nature.\n\n\n\nIn this paper, we use the method of parameterization proposed by Dzhumabaev \\cite{Dzhumabaev:1989}, which has proven to be an efficient constructive method allowing both to derive criteria for the unique solvability and obtain approximate solutions of various classes of boundary value problems \\cite{dzhumabaev2010method, dzhumabaev2016one, dzhumabaev2018computational, asanova2013well, assanova2022solution}. This method was originally proposed for solving the linear boundary value problem \\eqref{dae}, \\eqref{BC} provided $\\text{det}E\\neq 0.$ In this case, a criterion for the unique solvability was obtained in terms of coefficients and an algorithm for approximate solution was developed.  \n\nOur goal is to apply the method of parameterization  to the boundary value problem \\eqref{dae}, \\eqref{BC} in the case when the matrix $E$ is not necessarily non-singular. We derive a criterion for the existence of a unique solution under certain assumptions on the matrices of the boundary condition.",,
"\\section{Introduction}\n\nIn this conceptual paper,\nwe provide fundamental results on positive solutions to \npa\\-ram\\-e\\-trized systems of generalized polynomial {\\em inequalities} (with real exponents and positive parameters),\nincluding generalized polynomial {\\em equations}.\nIn doing so, we also offer a new perspective on fewnomials and (generalized) mass-action systems.\n\nLet \n$A' \\in \\R^{\\rka' \\times m}$ and\n$A'' \\in \\R^{\\rka'' \\times m}$\nbe coefficient matrices,\n$B \\in \\R^{n \\times m}$ % ( b^1 \\ldots \\, b^m ) \nbe an exponent matrix, and\n$c \\in \\R^m_>$ \nbe a positive parameter vector.\nThey define the\nparametrized system of (strict or non-strict) generalized polynomial inequalities\n\\begin{align*}\n\\sum_{j=1}^m a'_{ij} \\, c_j \\, x_1^{b_{1j}} \\cdots x_n^{b_{nj}} > 0 , \\quad i=1,\\ldots,\\rka' , \\\\\n\\sum_{j=1}^m a''_{ij} \\, c_j \\, x_1^{b_{1j}} \\cdots x_n^{b_{nj}} \\ge 0 , \\quad i=1,\\ldots,\\rka'' \n\\end{align*}\nin $n$ positive variables $x_i>0$, $i=1,\\ldots,n$,\nand involving $m$ monomials $x_1^{b_{1j}} \\cdots x_n^{b_{nj}}$, $j=1,\\ldots,m$.\n(We allow $\\rka' = 0$, that is, only non-strict inequalities, \nand analogously $\\rka'' = 0$, that is, only strict inequalities.)\nIn compact form, \n\\begin{equation} \\label{eq:problemAB}\nA' \\left( c \\circ x^B \\right) > 0 , \\quad \nA'' \\left( c \\circ x^B \\right) \\ge 0\n\\end{equation}\nfor $x \\in \\R^n_>$.\n\nWe obtain~\\eqref{eq:problemAB} as follows.\nFrom the exponent matrix $B = (b^1,\\ldots,b^m)$,\nwe define the monomials $x^{b^j} = x_1^{b_{1j}} \\cdots x_n^{b_{nj}} \\in \\R_>$,\nthe vector of monomials $x^B \\in \\R^m_>$ via $(x^B)_j = x^{b^j}$,\nand the vector of monomial terms $c \\circ x^B \\in \\R^m_>$ using the componentwise product~$\\circ$.\n(All notation is formally introduced at the end of this introduction.)\n\nClearly, the vector of monomial terms $(c \\circ x^B) \\in \\R^m_>$ is positive.\nIf~\\eqref{eq:problemAB} holds, \nit lies in the polyhedral cones\n$C' = \\{ y \\in \\R^m_\\ge \\mid A' \\, y \\ge 0 \\}$ and $C'' = \\{ y \\in \\R^m_\\ge \\mid A'' \\, y \\ge 0 \\}$,\nmore specifically, in the positive parts of $\\relint C'$ and $C''$.\nIndeed, the crucial object is the convex cone \n\\begin{equation*}\nC = \\{ y \\in \\R^m_> \\mid A' \\, y > 0, \\, A'' \\, y \\ge 0 \\},\n\\end{equation*}\na polyhedral cone with some faces removed.\nIt allows us to write system~\\eqref{eq:problemAB} as\n\\begin{equation} \\label{eq:problemBC}\n\\left(c \\circ x^B\\right) \\in C .\n\\end{equation}\nIn this work, we start from an arbitrary cone $C \\subseteq \\R^m_>$ in the positive orthant,\ncall it the {\\em coefficient} cone,\nand refer to \\eqref{eq:problemBC} as a {\\em parametrized system of generalized polynomial inequalities}\n(for given $B$ and $C$).\n\nOn the one hand, \nsystem~\\eqref{eq:problemAB}\nencompasses parametrized systems of generalized polynomial {\\em equations},\n\\begin{equation*}\nA \\left( c \\circ x^B \\right) = 0\n\\end{equation*}\nwith $A \\in \\R^{\\rka \\times m}$,\nwhich allows for applications in two areas:\n(i)~fewnomial systems, see e.g.\\ \\cite{Khovanskii1991,Sottile2011}, and\n(ii)~reaction networks with (generalized) mass-action kinetics,\nsee e.g.\\ \\cite{HornJackson1972,Horn1972,Feinberg1972} and \\cite{MuellerRegensburger2012,MuellerRegensburger2014,Mueller2016,MuellerHofbauerRegensburger2019}.\nWe depict a hierarchy of systems in Figure~\\ref{fig:1},\nranging from system~\\eqref{eq:problemBC} to fewnomial and generalized mass-action systems.\n\nOn the other hand, \nsystem~\\eqref{eq:problemBC} \nallows for finitely or infinitely many, strict or non-strict inequalities\nand hence for another area of application:\n(iii) semi-algebraic sets~\\cite{Bochnak1998,Basu2003} with positivity conditions,\nthat is, finite unions of sets\ngiven by equations ${A \\, x^B = 0}$ and strict inequalities ${A' \\, x^B > 0}$ \nwith $x \\in \\R^n_>$, $A \\in \\R^{\\rka \\times m}$, $A' \\in \\R^{\\rka' \\times m}$, \nand $B \\in \\N_0^{n \\times m}$ (over integers, rather than over reals).\nFor a survey on effective quantifier elimination including applications with positivity conditions,\nsee~\\cite{Sturm2017},\nand for the existence of positive solutions to a class of parametrized systems of polynomial inequalities,\nsee~\\cite{HongSturm2018}.\n\n\\begin{figure}[t]\n\\begin{tcolorbox}\n\\vspace{-3ex}\n\\begin{gather*}\n\\left(c \\circ x^B\\right) \\in C \\\\ \n\\text{(in-)finitely many, (non-)strict inequalities,} \\\\ \\text{given by a cone $C$ in the positive orthant} \n\\\\\n\\downarrow \\\\\nA \\left(c \\circ x^B\\right) \\ge 0 \\\\ \n\\text{finitely many, non-strict inequalities,} \\\\ \\text{involving the polyhedral cone $\\{ y \\ge 0 \\mid A \\, y \\ge 0 \\}$} \\\\\n\\downarrow \\\\\nA \\left(c \\circ x^B\\right) = 0 \\\\ \n\\text{finitely many equations,} \\\\ \\text{involving the subspace cone $\\{ y \\ge 0 \\mid A \\, y = 0 \\}$} \\\\\n\\rotatebox[origin=c]{-45}{$\\downarrow$} \\hspace{1cm} \\rotatebox[origin=c]{45}{$\\downarrow$} \\\\\n\\parbox[t]{5cm}{\\centering $A \\, x^B = 0$ \\\\[2ex] fewnomial systems \\\\ (not involving parameters)}\n\\parbox[t]{5cm}{\\centering $\\displaystyle \\dd{x}{t} = N \\left(k \\circ x^V\\right) = 0$ \\\\[1ex] (generalized) \\\\ mass-action systems}\n\\end{gather*}\n\\end{tcolorbox}\n\\label{fig:1}\n\\caption{A hierarchy of (parametrized) systems of generalized polynomial equations and inequalities for positive variables.} % $x>0$\n\\end{figure}\n\nThe primary contributions of this work are:\n\\begin{enumerate}\n\\item\nWe identify the relevant geometric objects of system~\\eqref{eq:problemBC},\nnamely, the {\\em coefficient set}~$P$ (a bounded set), \nthe monomial difference subspace~$\\LL$,\nand the monomial dependency subspace~$D$.\n\nMore specifically,\nwe consider partitions of the monomials into {\\em classes},\ncorresponding to a decomposition of the coefficient cone (as a direct product),\nand we obtain the coefficient set $P = C \\cap \\Delta$ \nby intersecting the coefficient cone $C$ with a direct product $\\Delta$ of simplices (or appropriate affine subspaces) on the classes.\n\nThe monomial difference subspace $L$ is determined by the affine monomial spans of the classes,\nwhereas the monomial dependency subspace $D$ captures affine dependencies within {\\em and} between classes.\nIn particular, the {\\em monomial dependency}~$\\dep = \\dim D$ is crucial.\n\\item\nAs our main result,\nwe rewrite polynomial inequalities in terms of $\\dep$ binomial equations on $P$,\ninvolving $\\dep$ monomials in the parameters.\n\n%In particular, we establish an {\\em explicit} bijection between solutions on $P$ and solutions of the original system~\\eqref{eq:problemBC} via exponentiation.\nIn particular, we establish an {\\em explicit} bijection between\nthe solution set \\[ Z_c = \\{ x \\in \\R^n_> \\mid \\left( c \\circ x^B \\right) \\in C \\} \\] of system~\\eqref{eq:problemBC} \nand the solution set on $P$,\n \\[ Y_c = \\{ y \\in P \\mid y^z = c^z \\text{ for all } z \\in D \\} , \\]\nvia exponentiation.\n\\item\nWe obtain a problem classification.\nThe dependency $d$ and the dimension of~$P$ indicate the complexity of a system.\n\nIf $\\dep=0$ (the ``very few''-nomial case), solutions exist (for all parameters) and can be parametrized explicitly.\n% thereby generalizing monomial parametrizations (of the solution set).\nIf $\\dep>0$, % solutions on the coefficient set can often be determined more easily than solutions of the original system.\nthe treatment of the $\\dep$~equations requires additional objects \nsuch as sign-characteristic functions (introduced in this work).\n\\end{enumerate}\n\nOur results lay the groundwork for a novel approach to ``positive algebraic geometry''.\nThey are based on methods from linear algebra and convex/polyhedral geometry\n(and complemented by techniques from analysis).\n\nIn applications to fewnomial systems, we extend the standard setting in three ways:\nwe allow for generalized polynomial {\\em inequalities},\nwe assign a positive {\\em parameter} to every monomial,\nand we consider partitions of the monomials into {\\em classes}.\n%\nIn applications to (generalized) mass-action systems,\nparameters and classes are standard,\nbut our {\\em classes} differ from the ``linkage classes'' of reaction network theory.\nSee also Example~\\ref{exa:two-component} and the footnote there.\n\nIndeed, we illustrate our results (in particular, the relevant geometric objects) through three examples \nfrom real fewnomial and reaction network theory,\nall of which involve trinomials.\n%\nFirst, we compute an explicit parametrization of the solution set\nfor a ``very few''-nomial system (with $\\dep=0$) arising from a reaction network.\nSecond, we provide a ``solution formula'' for univariate trinomials (with $\\dep=1$).\nTo this end, we introduce sign-characteristic functions and corresponding discriminants and ``roots''.\nThird, \nwe consider a system of one trinomial equation and one tetranomial inequality (with $\\dep=2$ and two classes)\nand provide an explicit parametrization of the region for multistationarity of the underlying reaction network.\n\nWe foresee applications of our approach to many more problems\nsuch as \nexistence and uniqueness of positive solutions, for given or all parameters, % (for first results, see Section~\\ref{sec:unique}),\nupper bounds for the number of solutions/components of fewnomial systems,\nand extensions of classical results from reaction network theory. % (see also Section~\\ref{sec:decomp}).\n%\nFor first applications to real fewnomial theory, see the parallel work~\\cite{MuellerRegensburger2023b}.\nThere, we improve upper bounds (on the number of positive solutions)\nfor (i) $n$ trinomials involving ${n+2}$ monomials in $n$ variables, given in~\\cite{Bihan2021},\nand (ii) one trinomial and one $t$-nomial (with $t\\ge3$) in two variables, given in~\\cite{Li2003,Koiran2015a}.\nFurther, for two trinomials ($t=3$), we refine the known upper bound of five in terms of the exponents.\n%Moreover, we find a counterexample to Kouchnirenko’s Conjecture~\\cite{Khovanskij1980,Sturmfels1998}\n%that is even simpler than the smallest ``Haas system''~\\cite{Haas2002,Dickenstein2007}.\nFor a characterization of the existence of a unique solution\n(and a resulting multivariate Descartes' rule of signs), see the very recent work~\\cite{DeshpandeMueller2024}.\n\n{\\bf Organization of the work.} \nIn Section~\\ref{sec:help}, \nwe formally introduce the geometric objects and auxiliary matrices\nrequired to rewrite system~\\eqref{eq:problemBC}. \nIn Section~\\ref{sec:main}, we state and prove our main results, Theorem~\\ref{thm:main} and Proposition~\\ref{pro:bij}.\nMoreover, \nin~\\ref{sec:d0}, we discuss \nexplicit parametrizations of the solution set beyond monomial parametrizations,\nand \nin~\\ref{sec:decomp}, we consider systems that are decomposable into subsystems given by the classes.\nFinally, in Section~\\ref{sec:app},\nwe apply our results to three examples,\nall of which involve trinomials.\nWe briefly summarize all examples at the beginning of the section.\n\nIn Appendix~\\ref{app:sc}, we introduce sign-characteristic functions,\nwhich serve as a key technique in the analysis of trinomials.",,
,,
"\\section{Introduction}\n\nThis article aims to provide a comprehensive overview of the interplay between \\MW motivic cohomology and linear algebraic groups and to demonstrate the usefulness of \\MW motivic cohomology in understanding the properties and structure of these groups. Linear algebraic groups play a fundamental role in various mathematical disciplines. In order to study them, cohomology has emerged as a critical invariant. In this article, we compute the cohomology of an algebraic group $G$, seen as a variety, using the \\MW motivic cohomology $\\HH$ instead of employing the more commonly used group cohomology of the classifying space $BG$ (the $BG$ here refers to a geometric model described in \\cite[\\S 4.2]{morel19991}), although the cohomology of $G$ and $BG$ are connected (section \\ref{ComputeSp}).\n\nFor context, recall that the \\MW motivic cohomology $\\HH$ is defined as the unit object in the tensor triangulated category $\\DMt(S)$ of Milnor-Witt motives over $S$ \\cite{bachmann2020milnor}, and can be seen in a precise sense as an analogue of singular cohomology in topology. In the sequel, we denote by $\\Mt(X)$ the \\MW motive of a scheme $X$. If $x\\in X(S)$, we denote by $\\Mt(X,x)$ the reduced \\MW motive of $X$. For instance, if $n\\geq 1$, we obtain for $\\afnz{n}:=\\mathbb{A}^n_S\\setminus \\{0\\}$ the following motives\n\\[\n\\Mt(\\afnz{n})=\\Mt(S)\\oplus \\Mt(S)(n)[2n-1],\n\\]\nwhile $\\Mt(\\afnz{n},x)=\\Mt(S)(n)[2n-1]$ for any $S$-rational point $x$.\n\nFor the original motivic cohomology, the computations and motivic decompositions of split reductive groups \\cite{biglari2012motives} and Stiefel varieties \\cite{williams2012motivic} in motivic cohomology are well known. At present, there are only limited results for \\MW motivic cohomology and the main goal of this article is to fill this gap. We present two original results using different methods. Firstly, we compute the \\MW motivic decomposition of $\\Sp_{2n}$ in $\\DMt(S)$, over any reasonable base $S$, by utilizing the $\\Sp$-orientation of \\MW motivic cohomology. The final result is the following. \n\\begin{theorem}[Theorem \\ref{SpMain}]\n\t For any $n\\geq 1$, We have an isomorphism in $\\DMt(S)$\n\t\\[ \\Mt(\\Sp_{2n})\\cong  \\Mt(\\afnz{2n})\\otimes \\Mt(\\Sp_{2n-2}),\\]\n\tyielding a decomposition in $\\DMt(S)$ of the form\n\t\\[\\Mt(\\Sp_{2n})\\cong \\Mt(\\afnz{2n})\\otimes \\Mt(\\afnz{2n-2})\\otimes\\ldots \\otimes \\Mt(\\afnz{2}) \\cong \\bigoplus_{1\\leq i_1<\\ldots<i_j \\leq n} \\tbZ(d(i_1,\\ldots,i_j))  .\\] \n\twhere $\\tbZ(d(i_1,\\ldots,i_j))$ denotes $\\tbZ(\\sum_{l=1}^j 2i_l)[4(\\sum_{l=1}^j i_l) -j] $.\n\\end{theorem}\n\nNext, we address the computation for $\\GL_n$ and $\\SL_n$. The fundamental difference between \\MW motivic cohomology and its more classical version is that the former is not $\\GL$-oriented (i.e. doesn't have a reasonable theory of Chern classes), thus making the computations more delicate. For this article, the $\\eta$-inverted Milnor-Witt cohomology $\\Heta$ provides the most effective mean of computations. This allows us to parallel the classical computations for both $O_n$ and $SO_n$ \\cite{hatcher2002algebraic}. Using the Leray spectral sequence, we can inductively compute for all Stiefel varieties. In the next statement, we write $V_{k}(\\af^{n})$ for the Stiefel variety of full rank $k\\times n$ matrices, where $k\\leq n$ and we set\n\\begin{eqnarray*}\n\\mathbf{HS}_{2k}&:=&\\Meta(\\afnz{2k})\\cong \\Meta(S)\\oplus \\Meta(S)(2k)[4k-1], \\\n \\mathbf{HS}_{2k+1}&:=& \\Meta(S)\\oplus \\Meta(S)(4k+1)[8k].\n\\end{eqnarray*}\nHere, let us note that $\\Meta(S)(1)[1]$ is canonically isomorphic to $\\Meta(S)$, via multiplication by $\\eta$, and we could therefore have set\n\\begin{eqnarray*}\n\\mathbf{HS}_{2k}\\cong\\Meta (S)\\oplus \\Meta(S)[2k-1], \\\n \\mathbf{HS}_{2k+1}\\cong \\Meta(S)\\oplus \\Meta(S)[4k-1].\n\\end{eqnarray*}\nWe however prefer to keep the previous notation, mainly since we plan to compute the integral motive of Stiefel varieties in $\\DMt(S)$ in the near future.\n\n\\begin{theorem}[Theorem \\ref{SVCohMain}]\n\tWe have an isomorphism of graded algebras\n\t\\[\\Heta^{*}(V_{k}(\\af^{n}))\\cong \\Heta^{*}(S)[G]/(g^2-\\delta_{\\gamma_0}(g)[-1]g , g\\in G)\\] \n\twhere the generators $G$ are given by \n\t\\begin{equation*}\n\t\tG=\\begin{cases}\n\t\t\tG_1\\sqcup G_2\\sqcup G_3 &\\text{$n$ even, $n-k$ even}\\\\\n\t\t\tG_2\\sqcup G_3 &\\text{$n$ odd, $n-k$ even}\\\\\n\t\t\tG_1\\sqcup G_2 &\\text{$n$ even, $n-k$ odd}\\\\\n\t\t\tG_2 &\\text{$n$ odd, $n-k$ odd}\\\\\n\t\t\t\\end{cases},\n\t\\end{equation*}\n\there \n\t\\begin{eqnarray*}\n\t\tG_1&=&\\{\\alpha_{n-1}\\in \\Heta^{n-1}(\\mathbf{HS}_n)\\}\\\\\n\t\tG_2&=&\\{\\beta_{4j-1}\\in \\Heta^{4j-1}(\\mathbf{HS}_{2j+1})|n-k<2j<n\\}\\\\\n\t\tG_3&=&\\{\\gamma_{n-k} \\in \\Heta^{n-k}(\\afnz{n-k+1})\\}\n\t\\end{eqnarray*}\n\tAnd $\\delta_{\\gamma_0}(\\gamma_0)=1$, otherwise $ \\delta_{\\gamma_0}(g)=0$.\n\\end{theorem}\n\nAs a consequence, we obtain the following theorem.\n\\begin{theorem}[Theorem \\ref{SVMain}]\nWe have a motivic decomposition of the following form for any $i,j\\in\\mathbb{N}$:\n\t\\[\\Meta(V_{2j}(\\af^{2i})) \\cong \\mathbf{HS}_{2i} \\otimes  \\mathbf{HS}_{2i-1}\\otimes \\mathbf{HS}_{2i-3}\\otimes \\ldots \\otimes \\mathbf{HS}_{2i-1-2(j-2)} \\otimes \\Meta(\\afnz{2i+1-2j}) \\]\n\t\\[\\Meta(V_{2j+1}(\\af^{2i})) \\cong \\mathbf{HS}_{2i} \\otimes \\mathbf{HS}_{2i-1}\\otimes \\mathbf{HS}_{2i-3}\\otimes \\ldots \\otimes \\mathbf{HS}_{2i-1-2(j-1)}  \\]\n\t\\[\\Meta(V_{2j}(\\af^{2i+1})) \\cong \\mathbf{HS}_{2i+1}\\otimes \\mathbf{HS}_{2i-1}\\otimes \\ldots \\otimes \\mathbf{HS}_{2i+1-2(j-1)}  \\]\n\t\\[\\Meta(V_{2j+1}(\\af^{2i+1})) \\cong \\mathbf{HS}_{2i+1}\\otimes \\mathbf{HS}_{2i-1}\\otimes \\ldots \\otimes \\mathbf{HS}_{2i+1-2(j-1)} \\otimes \\Meta(\\afnz{2i+1-2j}).  \\]\n\\end{theorem}",,
"\\section*{Introduction}\n\n\\textbf{1.} Let $F$ be a number field. Grothendieck has defined the category of pure motives over $F$. (See \\cite{Dem}, \\cite{Ma} for the definitions. In this section, which is aimed at motivating our results, we assume the optimistic properties of motives, defined using the relation of numerical equivalence of cycles.) \n%For more information see the two volumes \\cite{?}.\n\nAssume $E$ is another number field. There is a natural notion of motives with coefficients in $E$. If $\\sigma:E\\rg \\C$ is an embedding, one conjectures the existence of an $L$-function $L(\\sigma,M,s)$ having the usual properties, analytic continuation and functional equation. See \\cite[\\S~1]{Del}.\n\nFrom a motive $M$ over $E$ we can deduce, for $E'\\supset E$, a motive $M_{E'}=M \\otimes E'$. For a suitable choice of $E'$, $M_{E'}$ will be absolutely irreducible. For $\\sigma:E'\\rg \\C$, it is conjectured that $L(\\sigma,M_{E'},s)$ is equal to the $L$-function $L(s,\\pi)$ of a cuspidal (complex) representation of $\\GL(N,\\A_F)$  where $N$ is the dimension of $M_{E'}$. This conjecture was formulated  in the  70's, see Langlands \\cite{Lan} and Serre \\cite{Se}.\n\nThe class of cuspidal representations so obtained was specified by Borel \\cite{Bo}, and more precisely in \\cite{Cl} where the correspondence was precisely spelled out \\cite[Ch.~4]{Cl}.\n\nNow write simply $M$ for an absolutely irreducible (pure) motive with $E$-coefficients. In \\cite{Del} Deligne has specified a set of critical integers and given a conjecture for the value $L(M,\\sigma,n)$ for $n$ a critical integer. It should be, up to an element of $E^\\times$, a \\textit{period} given by the relation between the Betti and the de Rham cohomology of $M$. In particular, under this conjecture\n\\begin{equation}\nL(M,\\sigma,n) = 0 \\Leftrightarrow L(M,\\sigma',n)=0\n\\end{equation}\nfor two embeddings $\\sigma,\\sigma'$. (Gross further conjectured that the order of vanishing should be the same.)\n\n\\vspace{2mm}\n\n\\noindent\\textbf{2.} Given the conjectural correspondence, this can be formulated unconditionally for the $L$-functions of algebraic cuspidal representations, as defined in \\cite{Cl}. Consider  such a representation $\\pi$ of $\\GL(N,\\A_F)$. From now on we assume $\\pi$ self-dual and $N$ even. We also assume $\\pi$ algebraic and regular in the sense of \\cite{Cl}. Then the finite component $\\pi_f$ of $\\pi$ is defined over a number field $E$ \\cite[Theorem 3.3]{Cl}\\footnote{ In the statement of the Theorem, $\\pi$ should be in $\\Alg^0(n)$, the set of \\textbf{cuspidal} algebraic representations.}. In particular, for $a \\in \\Aut(\\C)$, there exists a representation $a(\\pi)$ with finite part $a(\\pi)_f = a(\\pi_f)$.\n\nFor $N$ even, the correspondence between $L(\\sigma,M,s)$ and $L(s,\\pi)$ --- where $\\pi$ should be associated to $(M,\\sigma)$ --- is not trivial. Indeed, the usual multiplication by $q_v^{1/2}$ in the formation of the $L$--function intervenes (Hecke, cf. Tate \\cite{Ta}). This is detailed in \\S~1.4. It follows that the critical values for $L(s,\\pi)$ are contained in $\\Z+\\frac{1}{2}$. Since $\\pi$ (cuspidal, regular, self-dual) is tempered at all primes, the Archimedean factors $L_\\infty(s,\\pi)$ and $L_\\infty(1-s,\\pi)$ are holomorphic at $s=1/2$. Deligne's  conjecture then implies\n\n\\vspace{2mm}\n\n\\noindent\\textbf{Conjecture 1.2.} \\textit{For} $a\\in \\Aut(\\C)$\n$$\nL(1/2,\\pi) = 0 \\Leftrightarrow L(1/2,a(\\pi))=0.\n$$\n\n\\noindent\\textbf{3.} When the field $F$ is totally imaginary, Conjecture 1.2 has been proved by Moeglin \\cite{M}. Here we assume $F$ totally real. The Rankin $L$--function $L(s,\\pi\\times\\pi)$ can be written\n$$\nL(s,\\pi\\times\\pi) = L(s,\\pi,\\Lambda^2)L(s,\\pi,S^2)\n$$\nassociated to the representations $\\Lambda^2$ and $S^2$ of the $L$-group. We say that $\\pi$ is symplectic (resp. orthogonal) if $L(s,\\pi,\\Lambda^2)$ $(\\resp.~L(s,\\pi,S^2)$) has a pole at $s=1$. (This applies for $N$ arbitrary.). For $N$ even and $\\pi$ algebraic regular, $\\pi$ is always symplectic if $F$ is totally real (Proposition~1.1).\n\nWe now impose a stronger condition on $\\pi$, superregularity (see Definition~1.1)\n\nOur first result is a new proof of a theorem of Gr\\\",,
"\\section{Introduction}\nLinear Algebra, a fundamental component of the undergraduate STEM curriculum, is often acknowledged as a challenging course. Its abstract concepts gradually unfold, unveiling the essence of the subject: the art of solving linear equations. In this paper, we present a top-down approach that promptly introduces the determinant, serving as a natural gateway to a comprehensive understanding of all fundamental concepts in Linear Algebra.\n\nTo foster an enriching educational experience, we have deliberately chosen not to provide exhaustive proofs and solutions. We firmly believe that an overemphasis on memorization and regurgitation detracts from genuine learning which can only be achieved by students doing their own thinking as much as possible. Consequently, we refrain from permanently posting detailed solutions to homework problems on the internet.\n\nFor a thorough comprehension of the concepts expounded in our paper, we recommend consulting the referenced works \\cite{GS1} or \\cite{GS2}.\n\nThe author wishes to express sincere gratitude to Dr.Nikolay Brodskiy for his invaluable assistance in organizing the GTA Mentoring program in Linear Algebra. This program played a pivotal role in significantly deepening the author's understanding of this mathematical field and refining his teaching methodologies. Special thanks are also extended to the author's former PhD students who actively participated in the program: Dr.Kyle Austin, Dr.Michael Holloway, Dr.Ryan Jensen, Dr.Kevin Sinclair, Dr.Logan Higginbotham, Dr.Pawe\\l\\ Grzegrz\\' o\\l ka, Dr.Thomas Weighill, and Dr.Jeremy Siegert.",,
,,"\\section{Concluding Remarks}\nThe state of the art for learning linear algebra is, to our minds, unsatisfactory, though getting better.  Technological platforms are split: some are proprietary, while some others are unsupported at the level needed for reliable use.  Methods and syntax are not standardized (or, rather, there are too many standards).  The textbooks largely do not integrate mechanized mathematical tools into the learning process. [A very notable exception is~\\cite{VanLoan2010}, which uses Matlab extensively.]  Yet failing to use a mechanized approach does a true disservice to students who will go on to practice linear algebra in some kind of mechanized environment.\n\nThe role of technology, including formal methods, is therefore multiplex.  We believe that people must be trained in its use.  In particular, people must be trained to want proof, and to want formal methods.  We feel that having students write their own programs plays a motivating role in that training as well as a developmental role.  The first linear algebra course is important not only because its tools and concepts are critical for science, but also as a venue for teaching the responsible use of mathematical technology."
,"""\section{DK-STP}\n\n\begin{dfn}\label{d2.1} Let $A\in {\cal M}_{m\times n}$ and $B\in {\cal M}_{p\times q}$, $t=\lcm(n,p)$. The DK-STP of $A$ and $B$, denoted by $A\ttimes B\in {\cal M}_{m\times q}$, is defined as follows.\n\begin{align}\label{2.1}\nA\ttimes B:=\left(A\otimes \J^T_{t/n}\right)\left(B\otimes \J_{t/p}\right).\n\end{align}\n\end{dfn}\n\n\begin{rem}\label{r2.2}\n\begin{itemize}\n\item[(i)] It is easy to verify that when the dimension matching condition is satisfied, i.e., $n=p$, the DK-STP coincides with classical matrix product. Hence, similarly to two kinds of MM-STPs, the DK-STP is also a generalization of classical matrix product.\n\n\item[(ii)] The two kinds of MM-STPs are not suitable for matrix-vector product, because in general the results are not vectors. Hence they can not realize linear mappings over vector spaces, and  the two corresponding MV-STPs have been established to perform linear mappings. Unlike them, DK-STP can realize MM-product and MV-product simultaneously.\n\n\item[(iii)]  Comparing with the MM-STP defined in Definition \ref{d1.1} \cite{che11,che12}, this DK-STP has minimum size $m\times q$, no matter whether the dimension matching condition is satisfied. That is why the product is named as dimension keeping STP.\n\n\item[(iv)] If two matrices $A$ and $B$ have the same dimension, the dimension of their DK-STP remains the same. This is a nice property.\n\n\end{itemize}\n\end{rem}""","\\section{Concluding Remarks}\n\nIn this paper a new STP, called (left) DK-STP and denoted by $\\ttimes$, has been proposed. Using it, the corresponding ring, Lie algebra, and Lie group are presented. The algebraic objects concerned in this paper can be described as:\n$$\nR(m \\times n, \\F) \\xrightarrow{\\ttimes} \\gl(m\\times n,\\F)\\xrightarrow{\\Exp} \\GL(m\\times n,\\F).\n$$\n\nThe action of $G(m\\times n,\\F)$ on dimension-free vector space $\\R^{\\infty}$ is also considered, which proposed discrete-time/continuous-time dynamic systems as\n$$\nG(m \\times n, \\F) \\xrightarrow{\\ttimes} \\R^{\\infty} \\ra ~\\mbox{S-system} \\ra \\mbox{dynamic system}. %\\xrightarrow{\\Pi_A}\n$$\nMeanwhile, by introducing the square restriction  $\\Pi_A\\in {\\cal N}_{m\\times n}$ of $A\\in {\\cal M}_{m\\times n}$, some interesting things have been obtained, including eigenvalue, eigenvector, determinant, invertibility, etc., for non square matrices. Particularly, the  Cayley-Hamilton theory can also be extended to non-square matrices.\n\nIn addition, the right DK-STP, weighted (left) DK-STP, and weighted right DK-STP are also briefly introduced. They have similar properties as (left) DK-STP.\n\nThis paper may pave a road for further development of STP of matrices.\n\nThough these new STPs shown many interesting properties, it can not be used to replace existing STPs, because they have quite different properties, which makes their functions different. Unlike existing STPs, application of these new STPs is still waiting for exploring.\n\nThere are many related topics remain for further study. The following are some of them.\n\n\n\\begin{itemize}\n\\item[(i)] Understanding $\\gl(m\\times n,\\F)$ and $\\GL(m\\times n,\\F)$.\n\nThe investigation of non-square general linear group and general linear algebra is only a beginning. To reveal their more properties is theoretically important and interesting. Particularly, general ``non-square\"
,,
"\\section{Introduction}\nTwo square matrices $A,B$ with coefficients over a field $k$ are called congruent if there is an invertible matrix $P$ of appropriate dimension such that $A=P^tBP$. The question of the existence of such an invertible matrix, when two matrices $A,B$ are provided, is called the congruence problem of matrices. It is an important classical problem in mathematics, with a vast and interesting history \\cite{Teran_history}. This question is solved completely through the classification of the matrix pencils, $P(A,B)=\\lambda A + \\mu B$, where two such pencils $P(A,B)$ and $P(A',B')$ are called equivalent if there are invertible matrices, $\\alpha, \\beta$ of appropriate size such that $P(A,B)=P(\\alpha A'\\beta, \\alpha B'\\beta)$. Classification of pencils up to equivalence is an important problem in linear algebra. It is a particular case of the class of problems called ``simultaneous equivalence\",,
"\\section{Introduction}\n\nWhen one wishes to investigate a nonclassical logic one has a\nchoice between two approaches: the syntactical  and the\nalgebraic.\nThe first usually gives rise to a {\\em relational (Kripke-style) semantics},\nwhile the other deals with {\\em algebraic semantics}.\nThe great success of Kripke in the sixties with his  relational semantics for\nmodal and intuitionistic logic was a source of inspiration for many\nresearches based on his methods, while the algebraic approach\nreceded into the background. The algebraic approach became fashionable again\nstarting in the late seventies, mainly because of the work of W. Blok and\nD. Pigozzi. W. Blok, in\nhis Ph.D. thesis \\cite{Blok1976},  conducted an\nin-depth study of Lewis' modal logic S4, and in \\cite{Blok1980} he\ninvestigated the entire {\\em lattice of modal logics} by purely algebraic\nmeans. Later he and D. Pigozzi investigated thoroughly the matter of\nalgebraizability of logics \\cite{BlokPigozzi1989,BlokPigozzi1992}. This\ninvestigation\nset the foundation for a new field, now commonly called {\\em abstract\nalgebraic logic}.\n\nOne of the first results of their line of investigation was the\nidentification  of the ``right'' concept of {\\em algebraizable logic}.\nRoughly speaking, a logic $\\sf L$ is algebraizable if there is a class $\\vv\nK$\n of algebras (no\ninfinitary operations, no relations, no second\norder axioms) which is to $\\sf L$ what the variety of Boolean algebras is to\nclassical propositional calculus.  The class $\\vv K$ is called the\n{\\em equivalent algebraic semantics}  of {\\sf L}. The\nknowledge that a given class $\\vv K$ of algebras\nis the equivalent algebraic semantics of a known logical system\nyields a good deal of information on its algebraic structure.\nConversely, one can discover algebraic properties of members of $\\vv K$\nthat can be transformed into logical data.\n\n\nIn this note we will apply this machinery to two logics:\n{\\em multiplicative-additive linear logic} ({\\sf MALL}) and\n{\\em classical linear logic} ({\\sf LL}).\n Both {\\sf MALL} and {\\sf LL} will turn out to be\nalgebraizable. As is  usually the case it is no surprise what\ntheir equivalent classes are: they consist of residuated lattices (possibly with a modal operator) obeying equations reflecting the logical\naxioms.",,
,,
"\\section{Context, motivation and basic examples} \\label{sec:intro}\nIn this text we consider linear differential equations (LDEs) of order $r$\n\\begin{equation}\n\\label{eq:lineardiff}\na_r(x) y^{(r)}(x) + a_{r-1}(x) y^{(r-1)}(x) + \\cdots + a_1(x) y'(x) + a_0(x) y(x) = 0,\n\\end{equation}\nwhere the $a_i$'s are known rational functions in $\\Q(x)$, with $a_r$ not\nidentically zero and $y(x)$ is an unknown ``function''. \nIn many applications, the desired solution $y(x)$ is a formal power series with coefficients in $\\Q$. Therefore, in what follows, when we write ``function'' we actually mean an element of $\\Q[[x]]$ unless otherwise specified.\nWe will say that a function $y\\in\\Q[[x]]$ is \\emph{differentially finite} (in short, \\emph{D-finite}) if it satisfies a linear differential equation like~\\eqref{eq:lineardiff}.\n\nA function $y\\in\\Q[[x]]$ is called \\emph{algebraic} if it is algebraic over $\\Q(x)$, that is, if $y(x)$ satisfies a polynomial equation of the form $P(x,y(x))=0$, for some $P\\in\\Q[x,y]\\setminus \\{ 0 \\}.$ \nOtherwise, $y(x)$ is called \\emph{transcendental}.\nThe simplest algebraic functions are polynomials in~$\\Q[x]$, closely followed by rational power series: these are rational functions in $\\Q(x)$ that have no pole at $x=0$ and therefore admit a Taylor expansion around the origin. \nA little more general are $N$-th roots of rational power series, such as $y(x)=1/\\sqrt[N]{1-x}$.\nIn all these three cases, $y(x)$ is clearly D-finite and satisfies a linear differential equation of order $r=1$.\n\nMany other examples of interesting functions that might or might not \nbe solutions of linear differential equations arise from combinatorics.\nA basic example is given by the Catalan numbers.\n\n\\begin{example}[Catalan numbers]\\label{ex:Catalan}\nBy definition, a \\emph{Dyck path} is a path drawn in the quarter\nplane $\\N^2$ that starts at $(0,0)$, consists of steps $\\nearrow$ \n(directed by the vector $(1,1)$) or $\\searrow$ (directed by the\nvector $(1, -1)$) and finally ends on the $x$-axis\n(see Figure~\\ref{fig:Dyck}).\n\nLet $C_n$ be the number of Dyck paths ending at $(2n,0)$; we say\nthat such paths have length~$n$.\nFor instance $C_1=1$ since there is a single Dyck path ending at $(2,0)$, namely\n$\\nearrow$--$\\searrow$, while $C_2=2$ since there are two Dyck paths ending at $(4,0)$, namely\n$\\nearrow$--$\\nearrow$--$\\searrow$--$\\searrow$ and $\\nearrow$--$\\searrow$--$\\nearrow$--$\\searrow$.\nWe use the convention that $C_0=1$.\nWe notice that any Dyck path of length $n+1$ can be written uniquely \nas the concatenation of (1)~a step $\\nearrow$, (2)~a Dyck path of \nlength $k{-}1$ (translated by $(1, 1)$), \n(3)~a step $\\searrow$ and (4)~a Dyck path of length\n$n{-}k$. It follows that the sequence $(C_n)_{n \\geq 0}$ satisfies \nthe following nonlinear recurrence relation:\n\\[C_n = \\sum_{k=1}^{n} C_{k-1} C_{n-k},\n\\qquad \\text{for all } n \\geq 1.\\]\nIf $y(x)$ denotes the generating function of the $C_n$'s, \\emph{i.e.}\n$y(x) = \\sum_{n=0}^\\infty C_n x^n$, the previous relation translates\nto the algebraic identity \n\\begin{equation}\n\\label{eq:algCatalan}\ny(x) = 1 + x{\\cdot}y(x)^2\n\\end{equation}\n(the summand $1$ comes from the fact that $C_0 = 1$).\nTherefore $y(x)$ is algebraic and one can even solve equation~\\eqref{eq:algCatalan} and\nget the closed formula $y(x) = \\frac{1-\\sqrt{1 - 4x}}{2x}$.\nIt is worth noting that, starting from the algebraic relation \n\\eqref{eq:algCatalan}, one can also derive a linear differential\nequation satisfied by $y(x)$.\nIndeed differentiating~\\eqref{eq:algCatalan}, one gets\n$y'(x) = y(x)^2 + 2 x\\: y(x)\\:y'(x)$. Therefore:\n\\[y'(x) = \\frac{y(x)^2}{1 - 2x\\:y(x)}.\\]\nThe right hand side in the latter expression can be further simplified\nusing again equation~\\eqref{eq:algCatalan}. Indeed notice that\n$$\\big(1 - 2x\\:y(x)\\big)^2 = 1 - 4x \\:y(x) + 4x^2 \\: y(x)^2 = 1 - 4x$$\nand consequently\n\\[\\frac{y(x)^2}{1 - 2x\\:y(x)} =\n \\frac{y(x)^2 \\cdot \\big(1 - 2x\\: y(x)\\big)}{1 - 4x} =\n \\frac{\\big(y(x) - 1\\big) \\big(1 - 2x\\: y(x)\\big)}{x(1 - 4x)} =\n \\frac{2x\\:y(x) - y(x) + 1}{x(1 - 4x)}\\]\nafter replacing two times $y(x)^2$ by $\\frac{y(x) - 1}x$.\n\nFinally, one obtains the inhomogeneous differential equation\n\\[(4x^2-x)y'(x) + (2x-1)y(x) + 1 = 0.\\]\nFrom this, we can derive new interesting information about the \nsequence $(C_n)_{n \\geq 0}$. For instance, it easily implies the\nsimpler recurrence relation $C_n = \\frac{4n-2}{n+1} \\cdot C_{n-1}$ \nfor all $n \\geq 1$, from which we further derive the closed formula \n$C_n = \\frac 1{n+1} \\binom{2n}n$. Using Stirling's formula, we also \ndeduce the asymptotic estimate $C_n \\sim {4^n}/{\\sqrt{\\pi n^3}}$.\n\\end{example}",,
,,
"\\section{Introduction}\n\\smallskip\\hspace{.6 cm}\nWe know that any real-valued continuous function in \\,$[\\,a,\\,b\\,]$\\, can be approximated by a set of polynomials.\\,There are also another facts on approximations, for example any open interval in the real line can be approximated by a set of closed intervals.\\,A natural setting for the problem of approximation is as follows.\n\nLet \\,$X$\\, be a normed linear space and \\,$G$\\, be a subset of \\,$X$.\\,Let \\,$x_{0} \\,\\in\\, X$\\, and \\,$\\delta \\,=\\, \\inf\\limits_{g \\,\\in\\, G}\\,\\left\\|\\,x_{0} \\,-\\, g\\,\\right\\|$.\\,Then $\\delta$\\, is the distance of \\,$x_{0}$\\, from \\,$G$.\\,The problem for best approximation of  \\,$x_{0}$\\, out of the element of \\,$G$\\, is to find an element \\,$g_{0} \\,\\in\\, G$\\, such that \\,$\\delta \\,=\\, \\left\\|\\,x_{0} \\,-\\, g_{0}\\,\\right\\|$ .\\,We see that a best approximation which occurs for \\,$g_{0}$, an element of minimum\ndistance from the given \\,$x_{0}$.\\,Such a \\,$g_{0} \\,\\in\\, G$\\, may or may not exist; which raises the problem of existence.\\,The problem of uniqueness is of practical interest too, since for given \\,$x_{0}$ and $G$\\, there may be more than one best approximation.\n\nNagumo \\cite{MN} introduced the notion of Banach algebra in 1936 and thereafter further development of the theory of Banach algebra was given by Gelfand et al. \\cite{IMG, IM}.\\,In recent times, various Banach algebra's techniques are used to simplify the theories related to matrices, operators, integral equations and dynamical systems etc.\n\nThe idea of linear 2-normed space was first introduced by S. Gahler \\cite{Gahler} and thereafter the geometric structure of linear 2-normed spaces was developed by Y. J. Cho and R. W. Freese \\cite{Freese}.\\,The concept of \\,$2$-Banach space is briefly discussed in \\cite{White}.\\,Mohammed and Siddiqui \\cite{NMA} introduced the concept of \\,$2$-Banach algebra and derived some known results of the usual Banach algebra in \\,$2$-Banach algebra.\\,For more on \\,$2$-normed algebras one can go through the papers \\cite{NSS, RU}.\\,H.\\,Gunawan and Mashadi \\cite{Mashadi} developed a generalization of a linear $2$-normed space for \\,$n \\,\\geq\\, 2$.\\,Some fundamental results of classical normed space with respect to \\,$b$-linear functional in linear\\;$n$-normed space  have been studied by P. Ghosh and T. K. Samanta \\cite{Prasenjit, K, KK}.\\,Also they have studied few fixed point theorems in linear \\,$n$-normed space \\cite{KP}.\\,The concept of \\,$2$-inner product space was first introduced by Diminnie et al.\\,\\cite{Diminnie} in 1970's.\\;In 1989, A.\\,Misiak \\cite{Misiak} developed a generalization of a \\,$2$-inner product space for \\,$n \\,\\geq\\, 2$.\n\nIn this paper, first we have studied best approximations theory in linear \\,$n$-normed space and observed that finite dimensionality plays an important role for existence of best approximation.\\,Strictly convex and uniformly convex in linear \\,$n$-normed space are also discussed.\\,The notion of a Banach algebra in \\,$n$-Banach spaces is being discussed with a few examples.\\,We shall establish a set-theoretic property of invertible and non-invertible elements in a \\,$n$-Banach algebra and then present topological divisor of zero in a \\,$n$-Banach algebra.\\,Finally, the idea of a complex homeomorphism in a \\,$n$-Banach algebra is being given and few results with respect to complex \\,$b$-homeomorphism in \\,$n$-Banach algebra are established.",,
,"""\section{Auto-tuning Pipeline High Level Overview}\n\nThe focus of this work is to develop an auto-tuner for selecting the optimal format for the \emph{DynamicMatrix} provided by \emph{Morpheus} to switch to given a matrix, an operation and a target hardware. The most straightforward approach for achieving this task is to utilize a run-first tuner that runs the operation of interest for every format supported, measures the desired metric and selects the best performing format as the optimum. Such an approach will be at expense of the overall runtime performance (even though it will provide the most accurate prediction) as it requires multiple expensive conversions between the different formats, with the expense increasing as more formats are added. A better approach, which reduces the prediction cost, is to use \gls{ml} models to find the optimal format. A high-level overview of our proposed auto-tuning pipeline is shown in Figure~\ref{fig:autotuner_overview}.\n\n\begin{figure}[h]\n    \centering\n    \includegraphics[width=\columnwidth]{figures/autotuner.pdf}\n    \caption{High-level overview of the auto-tuning pipeline. Red and green boxes represent offline and online operations respectively.}\n    \label{fig:autotuner_overview}\n\end{figure}\n\nThe auto-tuning pipeline is divided in the offline (red) and online (green) stage. The offline stage has to be executed once for every new architecture and operation we want to do predictions for, and the results from that stage can then be reused during the online stage.\n\n\subsection{Offline Stage}\nThe first stage of the pipeline is the actual model generation where we train, tune and extract the \gls{ml} model in a file, for a given architecture and operation, to be used later on by the auto-tuner. We use approximately 2200 real-valued, square matrices of varying sizes, sparsity patterns and different application domains, available from the \emph{SuiteSparse Collection}~\cite{suitesparse}.\n\nFor every matrix in the dataset, we first perform profiling runs on the operation and architecture of interest, and measure the runtime in order to determine the optimal format, i.e the format with the shortest runtime for the operation, and export its format ID to be used in the later stages. At the same time, we perform a feature extraction routine, described in detail in Section~\ref{sec:features}, on the matrices in order to generate inputs to be used during the training process. Both the input features (input data) and format ID (input targets) are used to train and tune the \gls{ml} model, a description of which is given in Section~\ref{sec:ml_model}, and once the tuned model is obtained it is exported to a file and stored for later use.\n\nTo streamline the training process for users, we wrap this process in a \emph{Python} framework called \emph{Sparse.Tree}\footnote{Available at: https://github.com/morpheus-org/sparse.tree} which uses \emph{scikit-learn}\cite{scikit-learn} under the hood. Users can use \emph{Sparse.Tree} to generate models for new systems or use the pre-trained models from the \emph{Model Database} for the x86 and ARM CPUs or NVIDIA and AMD GPUs used in this work.\n\n\subsection{Online Stage}\nIn order to be able to automatically select the optimum format to be used by \emph{Morpheus} in an application, we need to be able to make the decision efficiently and online, i.e. while the application is running. In the second stage of the pipeline, we implement \emph{Oracle}, a C++ architecture-independent auto-tuner that uses the \emph{DynamicMatrix} provided by \emph{Morpheus} and loads an \gls{ml} model from a file specified at runtime in order to make the decision. Note that by ``architecture-independent'' we refer to the fact that the tuner is agnostic to the target hardware it is tuning for, as this information is captured by the model loaded at runtime.\n\nFor the auto-tuner to be able to use the model, the features of the input matrix need to be extracted in the same way as during the first, training, stage. Then by traversing the model, \emph{Oracle} returns the optimal format ID that \emph{Morpheus} uses to switch to and perform the operation of interest. \emph{Oracle} is described in more detail in Section~\ref{sec:oracle}.""",
,,
,,
"\\section{Introduction} \n\nThe study of transcendence properties and unlikely intersections in Diophantine geometry is a fascinating topic possessing a vast literature developing in many directions including those related to the interplay between Hodge theory and the geometry of homogenous spaces such as Abelian varieties and their moduli spaces (that is, Shimura varieties). In this context, an impressive number of heuristic principles and rigorous results\\footnote{Such as Ax--Schanuel conjectures, Ax--Lindemann type theorems, Andr\\'e--Oort and Zilber--Pink conjectures.} was discovered by many authors, and, as a way to unify these statements and also suggest new ones, the point of view of \\emph{bi-algebraic structures} became increasingly popular: see, for instance, the survey \\cite{KUY} and the articles \\cite{BKT} and \\cite{BKU}. \n\nIn their recent work \\cite{KL}, Klingler and Lerer proposed\\footnote{This is a natural goal because the moduli spaces of Abelian differentials seem to ``behave'' like homogenous spaces: for example, the celebrated breakthroughs by Eskin, Mirzakhani and Mohammadi \\cite{EM}, \\cite{EMM} show that this is the case from the point of view of Dynamical Systems.} to extend the bi-algebraic point of view to the non-homogenous setting of moduli spaces of Abelian differentials. More concretely, the moduli space of Abelian differentials of genus $g$ is stratified into complex quasi-projective algebraic orbifolds $H(\\kappa)$ parametrising non-trivial Abelian differentials whose zeroes have multiplicities prescribed by a list $\\kappa=(k_1,\\dots,k_{\\sigma})$ such that $k_1+\\dots+k_{\\sigma}=2g-2$. As it was proved by Veech and Masur, the relative periods of the elements of $H(\\kappa)$ can be used to define the so-called \\emph{period charts} inducing a linear integral structure on the analytification of $H(\\kappa)$. In particular, after projectivising the stratum $H(\\kappa)$, $\\kappa=(k_1,\\dots,k_{\\sigma})$, we obtain a quasi-projective orbifold $\\mathcal{H}(\\kappa)$ of dimension $2g-2+\\sigma$ whose analytification possesses a linear projective structure. In this setting, a closed, irreducible, algebraic subvariety $W$ of $\\mathcal{H}(\\kappa)$ is \\emph{bi-algebraic} if its analytification $W^{\\textrm{an}}$ is algebraic in period charts\\footnote{That is, the relative periods of Abelian differentials projectively lying in $W^{\\textrm{an}}$ satisfy exactly $\\textrm{codim}_{\\mathcal{H}(\\kappa)}(W)$ independent algebraic relations (over $\\mathbb{C}$).}, cf. \\cite[Def. 1.1]{KL}, and Klingler and Lerer proved that all bi-algebraic curves in the strata $\\mathcal{H}(2)$ and $\\mathcal{H}(1,1)$ of Abelian differentials of genus two are (projectively) linear (in period charts), and, in general, any bi-algebraic curve in $\\mathcal{H}(\\kappa)$ is linear \\emph{provided} their condition $(\\star)$ is satisfied (cf. \\cite[Thm. 2.8 \\& 2.10]{KL}). Furthermore, they asked whether the bi-algebraicity of a subvariety of $\\mathcal{H}(\\kappa)$ is already enough to automatically ensure its linearity without extra conditions (cf. \\cite[Conjecture 2.7]{KL}). The main results of this paper say that some bi-algebraic subvarieties can be non-linear: \n\n\\begin{theorem}\\label{t: non linear bi algebraic curve} The projectivisation of the family \\(\\{ (C_u, \\omega_{u} ) \\} _{u\\in \\C \\setminus\\{0,\\pm 1\\}} \\) of Abelian differentials defined by \n\\begin{equation}\\label{eq: curve family} C_u := \\overline{\\{ y^6 = x (x-1) (x+1) (x-u)\\}} \\text{ and } \\omega_{u} :=  x^2dx/y^5, \\end{equation}  \nis a bi-algebraic curve in \\(\\mathcal H(12) \\) which is not linear. \n\\end{theorem}\n\n\\begin{remark} Note that \\(C_u\\) is a branched cover of \\(\\overline{\\{z^2=x(x-1)(x+1)(x-u)\\}}\\). In particular, the family of Abelian differentials $\\{(C_u, [dx/y^3])\\}_{u\\in\\mathbb{C}\\setminus\\{0,\\pm1\\}}$ corresponds to an arithmetic Teichm\\\",,
