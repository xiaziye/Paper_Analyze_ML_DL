Introduction,Methodology,Conclusion,Others
"[""\\section{Introduction and statement of the result}This paper is concerned with the algebraic reflexivity of the set of all diameter-preserving linear bijections between $C(X)$-spaces. We shall denote by $C(X)$ the Banach algebra of all continuous complex-valued functions on a compact Hausdorff space $X$, with the usual supremum norm. Our interest focuses on the local behaviour of linear maps on $C(X)$ which preserve the diameter of the ranges of functions in $C(X)$. Let us recall that for compact Hausdorff spaces $X$ and $Y$, a map $T$ from $C(X)$ into $C(Y)$ is said to be diameter-preserving if $\\diam(T(f))=\\diam(f)$ for all $f\\in C(X)$, where $\\diam(f)$ denotes the diameter of $f(X)$. Gy\\H{o}ry and Moln\\'{a}r \\cite{GyoMol-98} introduced this kind of maps and stated the general form of diameter-preserving linear bijections of $C(X)$, when $X$ is a first countable compact Hausdorff space. Cabello S\\'anchez \\cite{Cab-99} and, independently, Gonz\\'{a}lez and Uspenskij \\cite{GonUsp-99} extended this description by removing the hypothesis of first countability. A problem addressed by different authors is the Banach--Stone type representation of diameter-preserving maps between function spaces. See, for example, the papers by Aizpuru and Rambla \\cite{AizRam-07}, Barnes and Roy \\cite{BarRoy-02}, Font and Hosseini \\cite{FonHos-19}, Gy\\H{o}ry \\cite{Gyo-99}, Jamshidi and Sady \\cite{JamSad-16}, and Rao and Roy \\cite{RaoRoy-01}.On the other hand, a linear map $T$ of $C(X)$ into itself is called a local isometry (respectively, local automorphism) if for every $f\\in C(X)$, there exists a surjective linear isometry (respectively, automorphism) $T_f$ of $C(X)$, depending on $f$, such that $T(f)=T_f(f)$. The algebraic reflexivity of both sets of surjective linear isometries and automorphisms of $C(X)$ was stated by Moln\\'ar and Zalar in \\cite[Theorem 2.2]{MolZal-99} whenever $X$ is a first countable compact Hausdorff space. Furthermore, Cabello and Moln\\'ar \\cite{CabMol-02} gave an example where that reflexivity fails even if $X$ lacks first countability at only one point. Motivated by the precedent considerations, we introduce the following concept. We say that the set of all diameter-preserving linear bijections from $C(X)$ to $C(Y)$ is algebraically reflexive if every local diameter-preserving linear map from $C(X)$ to $C(Y)$ is a diameter-preserving bijection.Our main result is the following: Let $X$ and $Y$ be first countable compact Hausdorff spaces. Then the set of all diameter-preserving linear bijections from $C(X)$ to $C(Y)$ is algebraically reflexive. <!--TRUNCATED-->""]",,,"[""\\section{Proof of Theorem \\ref{main}}Before proving our result, we fix some notation and recall the existence of certain peaking functions. Given a set $X$ with cardinal number $|X|\\geq 2$, we denote \\begin{align*}\\widetilde{X}&=\\left\\{(x_1,x_2)\\in X\\times X\\colon x_1\\neq x_2\\right\\},\\\\X_2&=\\left\\{\\{x_1,x_2\\}\\colon (x_1,x_2)\\in\\widetilde{X}\\right\\}.\\end{align*}As usual, $\\mathbb{T}$ stands for the set of all unimodular complex numbers. We also denote $$\\mathbb{T}^+=\\left\\{e^{it}\\colon t\\in [0,\\pi[\\right\\}.$$An application of Urysohn's lemma shows that if $X$ is a first countable compact Hausdorff space and $(x_1,x_2)\\in\\widetilde{X}$, then there exists a continuous function $h_{(x_1,x_2)}\\colon X\\to [0,1]$ with $h_{(x_1,x_2)}^{-1}(\\{1\\})=\\{x_1\\}$ and $h_{(x_1,x_2)}^{-1}(\\{0\\})=\\{x_2\\}$, and hence $$h_{(x_1,x_2)}(x_1)-h_{(x_1,x_2)}(x_2)=1=\\diam(h_{(x_1,x_2)})$$and $$\\left\\{(x,y)\\in\\widetilde{X}\\colon h_{(x_1,x_2)}(x)-h_{(x_1,x_2)}(y)=1\\right\\}=\\left\\{(x_1,x_2)\\right\\}.$$<!--TRUNCATED-->""]"
,,,
"['\\section{Introduction}In this paper, we describe a class of Poisson algebras which appear in the context of infinitesimal geometry of Poisson submanifolds, known also as first class constraints. One of our motivations is to provide a suitable framework for a non-intrinsic Hamiltonian formulation of linearized Hamiltonian dynamics along Poisson submanifolds of nonzero dimension. This question can be viewed as a part of a general Hamiltonization problem for projectable dynamics on fibered manifolds studied in various situations.Let $S$ be an embedded Poisson submanifold of a Poisson manifold \\ec{(M,\\{,\\}_{M})}. Then, for every \\,\\ec{H \\in \\Cinf{M}},\\, the Hamiltonian vector field \\ec{X_{H}} on $M$ is tangent to $S$ and hence can be linearized along $S$. The linearized procedure for \\ec{X_{H}} at $S$ leads to a linear vector field \\,\\ec{\\mathrm{var}_{S}X_{H} \\in \\X{\\mathrm{lin}}(E)}\\, on the normal bundle of $S$ defined as a quotient vector bundle \\,\\ec{E=\\T_{S}M/\\T{S}}.\\,This fact gives rise to the so-called Hamiltonization problem for \\ec{\\mathrm{var}_{S}X_{H}} which is formulated in a class of Poisson algebras on the space of fiberwise affine functions \\ec{\\Cinf{\\mathrm{aff}}(E)} on $E$. In general, this setting can not be extended to the level of Poisson structures on $E$, because of the following observation due to I. M\\u{a}rcut: a first-order local model for the Poisson structure around the Poisson submanifold $S$ does not always exists.By using the infinitesimal data of the Poisson submanifold $S$, we introduce a family of Poisson algebras on \\ec{\\Cinf{\\mathrm{aff}}(E)} whose Lie brackets \\ec{\\{,\\}^{\\LL}} are parameterized by transversals $\\LL$ of $S$. These algebras are called \\emph{infinitesimal Poisson algebras} and, in fact, are independent of $\\LL$ modulo isomorphisms. For every $\\LL$, the first variation system defines a derivation of the corresponding Poisson algebra.If the flow of the Hamiltonian vector field \\ec{X_{H}} admits an invariant transversal \\,\\ec{\\LL \\subset \\T_{S}M}\\, of the Poisson submanifold $S$, then the first variation system \\ec{\\mathrm{var}_{S}X_{H}} is a Hamiltonian derivation of the corresponding infinitesimal Poisson algebra. The converse is also true.In the case, when $S$ is a symplectic leaf, this criterion is valid in a class of Poisson structures around $S$, called coupling Poisson structures. Here, we also give an application of this result to the linearization of Hamiltonian group actions at $S$. An interesting question is to extend such a criterion to general Poisson submanifolds using, for example, an approach developed in recent unpublished results on the existence of local models. <!--TRUNCATED-->']",,,"['\\section{Preliminaries}\\label{preliminaries}Here, we recall some facts about Poisson submanifolds; for more details see \\cite{We83, M.C.Marle2000, Zambon}.Let \\ec{(M,\\Pi)} be a Poisson manifold equipped with a Poisson bivector field \\,\\ec{\\Pi \\in \\Gamma\\wedge^{2}\\T{M}}\\, and the Poisson bracket    \\begin{equation*}        \\{f,g\\}_{M} \\,=\\, \\Pi(\\dd{f},\\dd{g}), \\quad f,g \\in \\Cinf{M}.    \\end{equation*}An (immersed) submanifold \\,\\ec{\\iota: S \\hookrightarrow M}\\, is said to be a \\emph{Poisson submanifold} of $M$ if the \\emph{Poisson bivector field $\\Pi$ is tangent} to $S$:    \\begin{equation}\\label{Po1}        \\Pi_{q} \\in \\wedge^{2}\\T_{q}S, \\quad \\forall\\, q \\in S.    \\end{equation}This means that $S$ inherits a (unique) Poisson structure \\,\\ec{\\Pi_{S} \\in \\Gamma\\wedge^{2}\\T{S}}\\, such that the inclusion $\\iota$ is a Poisson map. The corresponding Poisson bracket is denoted by    \\begin{equation*}        \\big\\{\\bar{f},\\bar{g}\\big\\}_{S} \\,:=\\, \\Pi_{S}\\big(\\dd{\\bar{f}},\\dd{\\bar{g}}\\big), \\quad \\bar{f},\\bar{g} \\in \\Cinf{S}.    \\end{equation*}There are several equivalent characterizations of when a submanifold is Poisson. Consider the induced bundle morphism \\,\\ec{\\Pi^{\\natural}:\\T^{\\ast}M \\rightarrow \\T{M}}\\, defined by \\,\\ec{\\alpha \\mapsto \\Pi^{\\natural}(\\alpha) := \\ii_{\\alpha}\\Pi},\\, and denote by \\ec{{\\T{S}}^{\\circ}} the annihilator of $\\T{S}$. Then, condition (\\ref{Po1}) can bereformulated in one of the following ways:    \\begin{equation}\\label{Po2}        \\Pi^{\\natural}\\big({\\T{S}}^{\\circ}\\big) = \\{0\\} \\qquad \\text{or} \\qquad \\Pi^{\\natural}\\big(\\T_{S}^{\\ast}M\\big) \\,\\subseteq\\, \\T{S}.    \\end{equation}This implies that every Hamiltonian vector field \\,\\ec{X_{H} = \\Pi^{\\natural}\\dd{H}}\\, is \\emph{tangent} to $S$. Moreover, if $S$ is an embedded submanifold, then the first condition in (\\ref{Po2}) is equivalent to the following: the vanishing ideal \\ec{I(S) = \\left\\{f \\in \\Cinf{M} \\,|\\, f|_{S = 0\\right\\}} is also an \\emph{ideal in the Lie algebra} \\ec{(\\Cinf{M},\\{,\\}_{M})}. <!--TRUNCATED-->']"
,,,"['The first variation system admits the following $\\LL$-dependent decomposition into horizontal and vertical components    \\begin{equation}\\label{HF1}        \\mathrm{var}_{S}X_{H} \\,=\\, \\mathrm{hor}_{\\dd{h}}^{\\D^{\\LL}} +\\, \\ii_{\\dd\\ell_{\\eta^{\\LL}}}\\Lambda,    \\end{equation}where \\,\\ec{h = H|_{S}}\\, and \\,\\ec{\\eta^{\\LL} \\in \\Gamma{E^{\\ast}}}\\, is defined by (\\ref{LH2}).<!--TRUNCATED-->']"
"['\\section{Introduction}Different versions of Burnside Problem ask what one can say about finitely generated periodic groups under additional assumptions. Kurosh type problems ask similar questions about properties of finitely generated nil (more generally, algebraic) associative algebras. In case of finitely generated Lie algebras, the periodicity is replaced by the condition that the adjoint mapping is nil. In particular, for Lie $p$-algebras one assumes that the $p$-mapping is nil. One of recent important directions in these areas is to study the growth of finitely generated (periodic) groups and (nil) algebras~\\cite{ErshlerZheng20,BellZel19}. The goal of this paper is to construct finitely generated nil restricted Lie algebras with extremely slow quasi-linear growth, these algebras are needed in further research~\\cite{Pe20flies}.The General Burnside Problem asks whether a finitely generated periodic group is finite. The first negative answer was given by Golod and Shafarevich: they proved that there exist finitely generated infinite $p$-groups for each prime $p$~\\cite{Golod64}. As an important instrument, they first construct finitely generated infinite dimensional associative nil-algebras~\\cite{Golod64}. Using this construction, there are also examples of infinite dimensional 3-generated Lie algebras $L$ such that $(\\ad x)^{n(x,y)}(y)=0$, for all $x,y\\in L$, the field being arbitrary~\\cite{Golod69}. Similarly, one easily obtains infinite dimensional finitely generated restricted Lie algebras $L$ with a nil $p$-mapping.The Grigorchuk and Gupta-Sidki groups play fundamental role in modern group theory. They are natural examples of self-similar finitely generated periodic groups. The author constructed their analogue in case of restricted Lie algebras of characteristic 2~\\cite{Pe06}, Shestakov and Zelmanov extended this construction to an arbitrary positive characteristic~\\cite{ShZe08}. Now, we construct a family of so called {\\it clover } 3-generated restricted Lie algebras $\\mathbf{T}(\\Xi)$, where a field of positive characteristic is arbitrary and $\\Xi$ an infinite tuple of positive integers. All these algebras have a nil $p$-mapping. We prove that $1\\le \\mathrm{GKdim}\\mathbf{T}(\\Xi)\\le 3$. We compute Gelfand-Kirillov dimensions of clover restricted Lie algebras with periodic tuples and show that these dimensions for constant tuples are dense on $[1,3]$. We construct a subfamily of nil restricted Lie algebras $\\TT(\\Xi_{q,\\kappa})$, with parameters $q\\in \\N$, $\\kappa\\in\\R^+$, having extremely slow {\\it quasi-linear} growth of type: $\\gamma_{\\mathbf{T}(\\Xi_{q,\\kappa})}(m)=m \\big(\\underbrace{\\ln\\cdots\\ln}_q  m\\big )^{\\kappa+o(1)}$, as $m\\to\\infty$.The present research is motivated by a construction by Kassabov and Pak of groups of oscillating growth~\\cite{KasPak13}. As an analogue, we construct nil restricted Lie algebras of intermediate oscillating growth in~\\cite{Pe20flies}. We call them {\\it Phoenix algebras} because, for infinitely many periods of time, the algebra is ""almost dying"" by having a ""quasi-linear"" growth as above, for infinitely many $n$ it has a rather fast intermediate growth of type $\\exp( n/ (\\ln n)^{\\lambda})$, for such periods the algebra is ""resuscitating"". The present construction of 3-generated nil restricted Lie algebras of quasi-linear growth is an important part of that result, responsible for the lower quasi-linear bound in that construction. <!--TRUNCATED-->']",,,"['\\section{Basic notions: restricted Lie algebras, Growth}\\label{Sdef}Let $K$ be an arbitrary field of positive characteristic $p$, $\\langle S\\rangle_K$ denotes a linear span of a subset $S$ in a $K$-vector space. Let $L$ be a Lie algebra, then $U(L)$ denotes the universal enveloping algebra. Long commutators are {\\it right-normed}: $[x,y,z]:=[x,[y,z]]$. We use a standard notation $\\ad x(y)=[x,y]$, where $x,y\\in L$. Also, we use the notation $[x^k,y]:=(\\ad x)^k (y)$, where $k\\ge 1$, $x,y\\in L$; in case $k=p^l$, we have also $[x^{p^l},y]=[x^{[p^l]},y]$, in terms of the $p$-mapping (see below).Let $L$ be a Lie algebra over a field $K$ of characteristic $p>0$. Then $L$ is called a \\textit{restricted Lie algebra} (or \\textit{Lie $p$-algebra}), if it is additionally supplied with a unary operation $x\\mapsto x^{[p]}$, $x\\in L$, that satisfies the following axioms~\\cite{JacLie,Ba,Strade1,StrFar,BMPZ}:\\begin{itemize}\\item $(\\lambda x)^{[p]}=\\lambda^px^{[p]}$, for $\\lambda\\in K$, $x\\in L$;\\item $\\ad(x^{[p]})=(\\ad x)^p$, $x\\in L$;\\item $(x+y)^{[p]}=x^{[p]}+y^{[p]}+\\sum_{i=1}^{p-1}s_i(x,y)$, for all $x,y\\in L$, where $i s_i(x,y)$~is the coefficient of $t^{i-1}$ in the polynomial $\\operatorname{ad}(tx+y)^{p-1}(x)\\in L[t]$.\\end{itemize}Let $A$ be an associative algebra over a field ~$K$. If the vector space $A$ is supplied with a new product $[x,y]=xy-yx$, $x,y\\in A$, one obtains a Lie algebra denoted by $A^{(-)}$. In case $\\operatorname{char}K=p>0$, the mapping $x\\mapsto x^p$, $x\\in A^{(-)}$, satisfies the three axioms above. <!--TRUNCATED-->']"
,,,
,,,"['\\bibitem{PeSh18FracPJ}   Petrogradsky\\,V., and Shestakov I.P.,   Fractal nil graded Lie, associative, Poisson, and Jordan superalgebras, preprint, arXiv:1804.08441.\\bibitem{Rad86}   Radford~D. E.,   Divided power structures on Hopf algebras and embedding   Lie algebras into special-derivation algebras,   {\\it J. Algebra}, {\\bf 98} (1986), 143--170.\\bibitem{Razmyslov}   Razmyslov Yu.P., {\\it Identities of algebras and their representations}.   AMS, Providence RI 1994.\\bibitem{Rozh96}   Rozhkov, A.V.   Lower central series of a group of tree automorphisms,   {\\it Math. Notes} {\\bf 60}, No.2, 165--174 (1996);   translation from   {\\it Mat. Zametki} {\\bf 60}, No.2, 225--237 (1996).<!--TRUNCATED-->']"
"['\\section{Introduction}\\label{sec:intro}Linear programming (LP) is one of the most useful tools available to theoreticians and practitioners throughout science and engineering. It has been extensively used to solve various problems in a wide range of areas, including operations research, engineering, economics, or even in more abstract mathematical areas such as combinatorics. Also in machine learning and numerical optimization, LP appears in numerous settings, including $\\ell_1$-regularized SVMs~\\cite{zhu20041}, basis pursuit (BP)~\\cite{yang2011alternating}, sparse inverse covariance matrix estimation (SICE)~\\cite{yuan2010high}, the nonnegative matrix factorization (NMF)~\\cite{recht2012factoring}, MAP inference~\\cite{meshi2011alternating}, etc.One of the most successful paradigms for solving LPs is the family of Interior Point Methods (IPMs), pioneered by Karmarkar in the mid 1980s~\\cite{karmarkar84}. Path-following IPMs and, in particular, long-step path following IPMs, are among the most practical approaches for solving linear programs. The core computational bottleneck in IPMs is the need to solve the linear system of eqn.~(\\ref{eq:normal}) at each iteration. This leads to two key challenges: first, for high-dimensional matrices $\\Ab$, solving the linear system is computationally prohibitive. Second, an alternative to direct solvers is the use of iterative solvers, but the situation is further complicated since $\\Ab\\Db^2\\Ab^\\ts$ is typically ill-conditioned.In this paper, we address the aforementioned challenges, for the special case where $m \\ll n$, i.e., the number of constraints is much smaller than the number of variables. This is a common setting in many applications of LP solvers. For example, in machine learning, $\\ell_1$-SVMs and basis pursuit problems often exhibit such structure when the number of available features ($n$) is larger than the number of objects ($m$). First, we propose and analyze a preconditioned Conjugate Gradient (CG) iterative solver for the normal equations of eqn.~(\\ref{eq:normal}), using matrix sketching constructions from the Randomized Linear Algebra (RLA) literature. Second, building upon the work of~\\cite{Mon03}, we propose and analyze a provably accurate long-step \\textit{infeasible} IPM algorithm. Third, we empirically show that our algorithm performs well in practice. We consider solving LPs that arise from $\\ell_1$-regularized SVMs and test them on a variety of synthetic and real-world data sets. <!--TRUNCATED-->']",,,"[""\\subsection{Comparison with Related Work}\\label{sxn:comparison}There is a large body of literature on solving LPs using IPMs. We only review literature that is immediately relevant to our work. Recall that we solve the normal equations inexactly at each iteration, and develop a way to \\emph{correct} for the error incurred. We also focus on IPMs that can use an sufficiently positive, infeasible initial point (see Section~\\ref{sxn:contrib}). The use of an approximate iterative solver for eqn.~(\\ref{eq:normal}), followed by a correction step to ``fix'' the approximate solution was proposed in~\\cite{Mon03}. We propose efficient, RLA-based approaches to precondition and solve eqn.~(\\ref{eq:normal}), as well as a novel approach to correct for the approximation error in order to guarantee the convergence of the IPM algorithm. Specifically,~\\cite{Mon03} propose to solve eqn.~\\eqref{eq:normal} using the so-called \\emph{maximum weight basis} preconditioner \\cite{RV93}. However, computing such a preconditioner needs access to a maximal linearly independent set of columns of $\\Ab\\Db$ in each iteration, which is costly, taking $\\Ocal(\\dimone^2\\dimtwo)$ time in the worst-case.The line of research in the Theoretical Computer Science literature that is closest to our work is~\\cite{daitch2008faster}, who presented an IPM that uses an approximate solver in each iteration. However, their accuracy guarantee is in terms of the final objective value which is different from ours. More importantly,~\\cite{daitch2008faster} focuses on \\textit{short-step}, feasible IPMs, whereas ours is \\emph{long-step} and does not require a feasible starting point. We also note that in the Theoretical Computer Science literature,~\\cite{LS13,lee2013path2, LS14,LS15,lee2019solving,CLS19} proposed and analyzed theoretically ground-breaking algorithms for LPs based on novel tools such as the so-called \\emph{inverse maintenance} for accelerating the linear system solvers in IPMs. However, all these endeavors are primarily focused on the theoretically fast but practically inefficient short-step feasible IPMs. In contrast, our work is focused on infeasible \\textit{long-step} IPMs, known to work efficiently in practice. <!--TRUNCATED-->""]"
,"['\\section{The Infeasible IPM algorithm}\\label{sxn:IIPM}In order to avoid spurious solutions, primal-dual path-following IPMs bias the search direction towards the \\emph{central path} and restrict the iterates to a neighborhood of the central path. This search is controlled by the \\emph{centering parameter} $\\sigma\\in[0,1]$.At each iteration, given the current solution $(\\xb^{k},\\yb^{k},\\sbb^{k})$, a standard infeasible IPM obtains the search direction $(\\Delta\\xb^k,\\Delta\\yb^k,\\Delta\\sbb^k)$ by solving the following system of linear equations:\\begin{subequations}\\label{eq:system}\t\\begin{flalign}\t\t\\Ab\\Db^2\\Ab^\\ts\\Delta\\yb^k=~&\\pb^k\\,,\\label{eq:normal1}\\\\\t\t\\Delta\\sbb^k=~&-\\rb^k_d-\\Ab^\\ts\\Delta\\yb^k\\,,\\label{eq:dels}\\\\\t\t\\Delta\\xb^k=~&-\\xb^k+\\sigma\\mu_k\\Sb^{-1}\\one_\\dimtwo-\\Db^2\\Delta\\sbb^k.\\label{eq:delx}\t\\end{flalign}\\end{subequations}After solving the above system, the infeasible IPM Algorithm~\\ref{algo:iipm} proceeds by computing a step-size $\\alphabar$ to return:\\begin{flalign}\\label{eqn:update}\t(\\xb^{k+1},\\yb^{k+1},\\sbb^{k+1}) = (\\xb^{k},\\yb^{k},\\sbb^{k}) + \\alphabar (\\Delta \\xb^k,\\Delta \\yb^k,\\Delta \\sbb^k).\\end{flalign}Given $\\Delta\\yb^k$ from eqn.~(\\ref{eq:normal1}), $\\Delta\\sbb^k$ and $\\Delta\\xb^k$ are easy to compute from eqns.~\\eqref{eq:dels} and \\eqref{eq:delx}, as they only involve matrix-vector products. However, since we use Algorithm~\\ref{algo:PCG} to solve eqn.~\\eqref{eq:normal1} approximately using the sketching-based preconditioned CG solver, the primal and dual residuals \\textit{do not} lie on the line segment between $\\zero$ and $\\rb^0$.For notational simplicity, we now drop the dependency of vectors and scalars on the iteration counter $k$. Let $\\hat{\\Delta \\yb}=\\Qb^{\\nicefrac{-1}{2}}\\tilde{\\zb}^t$ be the approximate solution to eqn.~(\\ref{eq:normal1}). In order to account for the loss of accuracy due to the approximate solver, we compute $\\hat{\\Delta\\xb}$ as follows:\\begin{flalign}\t\\hat{\\Delta\\xb}=~-\\xb+\\sigma\\mu\\Sb^{-1}\\one_\\dimtwo-\\Db^2\\hat{\\Delta\\sbb}-\\Sb^{-1}\\vb\\label{eq:delxhat}.\\end{flalign}Here $\\vb\\in\\R{n}$ is a perturbation vector that needs to exactly satisfy the following invariant at each iteration of the infeasible IPM:\\begin{flalign}\t\\Ab\\Sb^{-1}\\vb=\\Ab\\Db^2\\Ab^\\ts\\hat{\\Delta\\yb}-\\pb\\,\\label{eq:addl}.\\end{flalign}We note that the computation of $\\hat{ \\Delta \\sbb}$ is still done using, essentially, eqn.~\\eqref{eq:dels}, namely\\begin{flalign}\\Delta\\hat{\\sbb}^k=~&-\\rb^k_d-\\Ab^\\ts\\hat{\\Delta\\yb}^k.\\label{eq:delshat}\\end{flalign}In \\cite{Mon03} it is argued that if $\\vb$ satisfies eqn.~\\eqref{eq:addl}, the primal and dual residuals lie in the correct line segment.\\vspace{0.02in}\\noindent\\textbf{Construction of $\\vb$.} There are many choices for $\\vb$ satisfying eqn.~\\eqref{eq:addl}. To prove convergence, it is desirable for $\\vb$ to have a']",,"['\\begin{lemma}\\label{lem:fullrankR}\tLet $\\Wb\\in\\RR{\\dimtwo}{w}$ be the sketching matrix of Section~\\ref{sxn:background} and $\\vb$ be the perturbation vector of eqn.~(\\ref{eq:compv}). Then, with probability at least $1-\\delta$, $\\rank(\\Ab\\Db\\Wb)=\\dimone$ and\t$\\vb$ satisfies eqn.~\\eqref{eq:addl}.\\end{lemma}\\begin{proof}\tLet $\\Ab\\Db=\\Ub\\Sigmab\\Vb^\\ts$ be the thin SVD representation of $\\Ab\\Db$.\tWe use the exact same $\\Wb$ as discussed in Section~\\ref{sxn:PCG}.  Therefore, eqn.~\\eqref{eq:cnd1} holds with probability $1-\\delta$ and it directly follows from the proof of Lemma~\\ref{lem:cond3} that $\\rank(\\Ab\\Db\\Wb)=\\dimone$.Recall that $\\Ab\\Db\\Wb$ has full \\emph{row-rank} and thus $\\Ab\\Db\\Wb\\,(\\Ab\\Db\\Wb)^\\dagger=\\Ib_\\dimone$. Therefore, taking $\\vb=(\\Xb\\Sb)^{\\nicefrac{1}{2}}\\Wb(\\Ab\\Db\\Wb)^{\\dagger}(\\Ab\\Db^2\\Ab^\\ts\\hat{\\Delta\\yb}-\\pb)$, we get%\t\\begin{flalign*}\t\\Ab\\Sb^{-1}\\,\\vb=&~\\Ab\\Sb^{-1}(\\Xb\\Sb)^{\\nicefrac{1}{2}}\\Wb(\\Ab\\Db\\Wb)^{\\dagger}(\\Ab\\Db^2\\Ab^\\ts\\hat{\\Delta\\yb}-\\pb)\\nonumber\\\\\t=&~\\Ab\\Db\\Wb(\\Ab\\Db\\Wb)^{\\dagger}(\\Ab\\Db^2\\Ab^\\ts\\hat{\\Delta\\yb}-\\pb)\\nonumber\\\\\t=&~\\Ab\\Db^2\\Ab^\\ts\\hat{\\Delta\\yb}-\\pb\\,,\t\\end{flalign*}\twhere the second equality follows from $\\Db = \\Xb^{1/2}\\Sb^{-1/2}$.\\end{proof}']"
,,,"['\\begin{figure}[t]\t\\centering\t\\subfigure[Max. Inner CG Iterations.]{\t\t%\\label{fig:ARCENE_iter} %% label for first subfigure\t\t\\includegraphics[width=2.57in]{svm_ARCENE_m100_n20001_6_cg_iter.eps}}\t\\subfigure[Max. Condition Number.]{\t\t\\label{fig:ARCENE_iter} %% label for first subfigure\t\t\\includegraphics[width=2.57in]{svm_ARCENE_m100_n20001_7_kap.eps}}\t\\caption{\\emph{ARCENE data set}: for various ($w$, \\tolCG) settings,\t\t(a)~the maximum number of inner iterations used by our algorithm and (b)~the maximum condition number of \\precNormal, across outer iterations. The standard IPM, across all settings, needed on the order of 1,000 iterations and  $\\kappa(\\Ab \\Db^2\\Ab^T)$ was on the order of $10^{8}$.\t\tThe relative error was fixed to $0.04\\%$. }\t\\label{fig:ARCENE} %% label for entire figure\\end{figure}']"
,,,"[""\\section{Richardson Iteration}\\label{sxn:rchardson}Here, we show that all our analyses still hold if we replace Step 4 of Algorithm~\\ref{algo:PCG} (CG solver) with Richardson's iteration. Basically, all we need to show is that the condition of eqn.~\\eqref{eq:pdcond2} holds. Note that the condition of eqn.~\\eqref{eq:pdcond1} already holds from Lemma~\\ref{lem:cond3}, as we use the sketching matrix $\\Wb\\in\\RR{n}{w}$ discussed in Section~\\ref{sxn:PCG}.\\begin{algorithm}[H]\t\\caption{Richardson Iteration Solver}\\label{algo:iterative_solver}\t\\begin{algorithmic}\t\t\\State \\textbf{Input:}\t\t$\\Ab\\Db\\in\\RR{\\dimone}{\\dimtwo}$, $\\pb\\in\\R{\\dimone}$;\t\tnumber of iterations $t>0$;\t\tsketching matrix $\\Wb \\in \\mathbb{R}^{\\dimtwo\\times w}$;\t\t\\State \\textbf{Initialize:}\t\t$\\tilde{\\zb}^{0} \\gets \\zero_\\dimone $;\t\t\t\t\\For{$j=1$ \\textbf{to} $t$}\t\t\\State $\\tilde{\\zb}^{j} \\gets \\tilde{\\zb}^{j-1}+\\Qb^{-\\nicefrac{1}{2}}(\\pb-\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}\\tilde{\\zb}^{j-1})$;\t\t\\EndFor\t\t\t\t\\State \\textbf{Output:} return $\\tilde{\\zb}^{t}$;\t\\end{algorithmic}\\end{algorithm}\\noindent Our first result expresses the residual vector $\\tilde{\\fb}^{(j)}$ in terms of $\\tilde{\\fb}^{(j-1)}$ for $j=1\\ldots t$.%\\begin{lemma}\\label{lem:recursive}\tLet $\\tilde{\\fb}^{(j)}$, $j=1\\ldots t$ be the residual vectors at each iteration.Then,\t\\begin{flalign}\t\\tilde{\\fb}^{(j)}=~\\left(\\Ib_n-\\Qb^{-\\nicefrac{1}{2}}\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}\\right)\\tilde{\\fb}^{(j-1)}\\label{eq:error_recursive2}\\,.\t\\end{flalign}\tRecall that $\\Qb=\\Ab\\Db\\Wb\\Wb^\\ts\\Db\\Ab^\\ts$ and $\\tilde{\\fb}^{(j)}=\\Qb^{-\\nicefrac{1}{2}}(\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}\\tilde{\\zb}^j-\\pb)$.\\end{lemma}\\begin{proof}\tUsing Algorithm~\\ref{algo:iterative_solver}, we express $\\tilde{\\fb}^{(j)}$ as\t%\t\\begin{flalign}\t\\tilde{\\fb}^{(j)}=&~\\Qb^{-\\nicefrac{1}{2}}\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}\\tilde{\\zb}^{j}-\\Qb^{-\\nicefrac{1}{2}}\\pb\\nonumber\\\\\t=&~\\Qb^{-\\nicefrac{1}{2}}\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}\\left(\\tilde{\\zb}^{j-1}+\\Qb^{-\\nicefrac{1}{2}}(\\pb-\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}\\tilde{\\zb}^{j-1})\\right)-\\Qb^{-\\nicefrac{1}{2}}\\pb\\nonumber\\\\\t=&~\\left(\\Qb^{-\\nicefrac{1}{2}}\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}\\tilde{\\zb}^{j-1}-\\Qb^{-\\nicefrac{1}{2}}\\pb\\right)\\nonumber\\\\\t&~~~~~~~~~~~~~~~~~~~~~-\\Qb^{-\\nicefrac{1}{2}}\\Ab\\Db^2\\Ab^\\ts\\Q""]"
"[""\\section{Introduction}Interior point methods (IPMs) have emerged as one of the most powerful algorithms for solving linear programming (LP) problems. The primal-dual path-following variants are particularly attractive due to their polynomial complexity and excellent practical performance. At each iteration, these methods compute a Newton step toward a point on the central path, where the duality gap is reduced by a factor that determines the iteration complexity.The main computational bottleneck in IPMs lies in solving large sparse symmetric linear systems to compute the search direction. For problems with specific structure, specialized direct solvers can be highly effective. However, for general large-scale LPs, iterative methods - particularly preconditioned conjugate gradient (PCG) - have become increasingly popular due to their lower memory requirements and suitability for parallelization.This paper focuses on improving the performance of IPMs through better preconditioning strategies for the PCG solver. The key challenge is that the matrix to be inverted changes at each IPM iteration, requiring either recomputation or effective updating of the preconditioner. We propose a novel adaptive preconditioner that automatically adjusts to the changing matrix properties while maintaining computational efficiency.Our main contributions are: (1) A new class of preconditioners based on approximate matrix factorizations that adapt to the IPM's progression; (2) Theoretical analysis showing how the preconditioner quality affects both the PCG iterations and overall IPM convergence; (3) Implementation techniques that make the approach practical for large-scale problems; and (4) Extensive numerical experiments demonstrating significant improvements over state-of-the-art methods on standard test problems. <!--TRUNCATED-->""]",,,['\\newcommand{\\tolCG}[0]{$\\emph{tolCG}$} % \\newcommand{\\tolOuterRes}[0]{$\\tau$}\\newcommand{\\precNormal}[0]{$\\Qb^{-\\nicefrac{1}{2}}\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}$}\\newcommand{\\kprecNormal}[0]{$\\kappa(\\Qb^{-\\nicefrac{1}{2}}\\Ab\\Db^2\\Ab^\\ts\\Qb^{-\\nicefrac{1}{2}}$)}']
"[""\\section{Introduction}Spinor norm was first defined by Dieudonn\\'{e} and Kneser using Clifford algebras. Wall~\\cite{wa} defined the spinor norm using bilinear forms. These days, to compute the spinor norm, one uses the definition of Wall. In this paper, we develop a new definition of the spinor norm for split and twisted orthogonal groups. Our definition of the spinor norm is rich in the sense, that it is algorithmic in nature. Now one can compute spinor norm using a Gaussian elimination algorithm that we develop in this paper. In computational group theory, one always looks for algorithms to solve the word problem. For a group $G$ defined by a set of generators $\\langle X\\rangle=G$, the problem is to write $g\\in G$ as a word in $X$: we say that this is the \\textbf{word problem} for $G$ (for details, see~\\cite[Section 1.4]{ob3}). Brooksbank~\\cite{brooksbank} and Costi~\\cite{costi} developed algorithms similar to ours for classical groups over finite fields. It is worth noting that, Chernousov et.~al.~\\cite{ceg2000} also used Steinberg presentation for Gauss decomposition for Chevalley groups over arbitrary fields. We refer the reader to a book by Carter~\\cite[Theorem 12.1.1]{ca} for Steinberg presentation. We prove the following: \\begin{theorema}\\namedlabel{maintheorem}{Theorem A}Let $G$ be a reductive linear algebraic group defined over an algebraically closed field $k$ of $\\mathrm{char}\\; \\neq 2$ which has the Steinberg presentation. Then every element of $G$ can be written as a word in Steinberg generators and a diagonal matrix. The diagonal matrix is:  $\\diag(\\alpha,1,\\ldots,1,\\lambda,\\mu(g),\\ldots, \\mu(g),\\mu(g)\\lambda^{-1})$, where $\\alpha, \\lambda, \\mu(g)\\in k^{\\times}$, in its natural presentation. Furthermore, we prove that the length of the word is bounded by $\\mathcal{O}(l^3)$, where $l$ is the rank of the group $G$.\\end{theorema}Now we move on to discuss two applications of our algorithm. One is spinor norm and the other is double coset decomposition. Murray and Roney-Dougal~\\cite{mr} studied computing spinor norm earlier. From our algorithm, one can compute the spinor norm easily (for details see Section~\\ref{spinornorm}).~\\ref{maintheorem} has the following surprising corollary: \\begin{corollarya1}\\namedlabel{corollary}{Corollary A1}Let $k$ be a field of ${\\rm char} \\neq 2$. In the split orthogonal group $\\mathrm{O}(n, k)$, the image of $\\lambda$ in $k^\\times/k^{\\times 2}$ is the spinor norm. \\end{corollarya1}Furthermore, the spinor norm can also be computed using our algorithm for the twisted orthogonal group. For terminologies of the next result, we refer to Definition~\\ref{to-} and Section~\\ref{elementarymattwist}.\\begin{corollarya2}\\namedlabel{corollary11}{Corollary A2}Let $g\\in \\mathrm{O}^{-}(2l,q)$, then the spinor norm $\\Theta(g)$ of $g$ is the following:\\begin{equation*}\\label{twistspinornorm}\\Theta(g)=\\left\\{\\begin{array}{ll}\\lambda(1-t)\\mathbb{F}_q^{\\times 2} & \\text{when}\\; \\mathrm{det}(g)=1,\\\\2\\epsilon\\lambda(1-t)\\mathbb{F}_q^{\\times 2} & \\text{when}\\; \\mathrm{det}(g)=-1.\\end{array}\\right.\\end{equation*}Here $t\\in \\mathbb{F}_q$.\\end{corollarya2}Suppose we want to construct infinite-dimensional representations of reductive linear algebraic groups. One way to construct such representations is parabolic induction. Let $P$ be a parabolic subgroup of $G$ with Levi decomposition $P=MN$, where""]","['\\section{Preliminaries}\\label{basics}In this section, we fix some notations and terminologies for this paper. We denote the transpose of a matrix $X$ by $\\tr{X}$.\\subsection{Algebraic groups}An \\emph{algebraic group} $G$ defined over $\\bar{k}$ is a group as well as an affine variety over $\\bar{k}$ such that the maps $\\mu \\colon G\\times G \\rightarrow G$, and $i \\colon G \\rightarrow G$ given by $\\mu(g_1,g_2)=g_1g_2$, and $i(g)=g^{-1}$ are morphisms of varieties. An \\emph{algebraic group} $G$ is defined over $k$, if the polynomials defining the underlying affine variety $G$ are defined over $k$, with the maps $\\mu$ and $i$ defined over $k$, and the identity element $e$ is a $k$-rational point of $G$. We denote the $k$-rational points of $G$ by $G(k)$. Any algebraic group$G$ is a closed subgroup of $\\GL(n,k)$ for some $n$. \\subsection{Similitude groups}Let $V$ be an $n$-dimensional vector space over $k$, where  $n=2l$ or $n=2l+1$ and $l\\geq 1$.  Let $\\beta\\colon V\\times V \\rightarrow k$ bea bilinear form. By fixing a basis of $V$ we can associate a matrix to $\\beta$. With abuse of notation, we denote the matrix of the bilinear form by $\\beta$ itself. Thus $\\beta(x,y)=\\tr x\\beta y$, where $x,y$ are column vectors. We work with the non-degenerate bilinear forms, i.e., $\\det\\beta\\neq 0$. A symmetric (resp. skew-symmetric) bilinear form $\\beta$ satisfies $\\beta=\\tr\\beta$ (resp. $\\beta=-\\tr\\beta$). \\subsubsection{Symplectic similitude groups}\\begin{definition}\\label{defsymplecticgroup}The {\\normalfont symplectic group} is defined for $n=2l$ as $$\\mathrm{Sp}(n,k):=\\{ g \\in \\GL(n,k) \\mid \\tr g\\beta g=\\beta \\}, \\text{ where } \\beta=\\begin{pmatrix}0 & I_l \\\\ -I_l & 0\\end{pmatrix}.$$\t\\end{definition}\\begin{definition}\tThe {\\normalfont symplectic similitude group} with respect to $\\beta$ \t(as in Equation \\eqref{beta1}), is defined by\t\\[\\GSp(n,k)=\\{g\\in \\GL(n,k) \\mid \\tr g \\beta g=\\mu(g)\\beta,\\, \\text{ for some } \\mu(g) \\in k^{\\times}\\},\\] \twhere $\\mu : \\GSp(n,k) \\rightarrow k^{\\times}$  \tis a group homomorphism with $\\mathrm{ker}\\;\\mu=\\mathrm{Sp}(n,k)$ and the factor $\\mu(g)$ is called the \\textit{multiplier} of $g$.\\end{definition}\\subsubsection{Orthogonal similitude groups}  \\begin{definition}The {\\normalfont orthogonal group} is defined as \\[ \\mathrm{O}(n,k):=\\{ g \\in \\GL(n,k) \\mid \\tr g\\beta g=\\beta \\}, \\text{ where } \\beta \\text{ as in Equation }\\eqref{beta2}.\\]\\end{definition}\\begin{definition}\tThe {\\normalfont orthogonal similitude group} with respect to $\\beta$ (as in Equation~\\eqref{beta2}) is defined by\t\\[\\GO(n,k)=\\{g\\in \\GL(n,k) \\mid \\tr g \\beta g=\\mu(g)\\beta, \\text{ for some }\\; \\mu(g) \\in k^{\\times} \\},\\] \twhere $\\mu : \\GO(n,k)\\rightarrow k^{\\times}$ \tis a group homomorphism with $\\mathrm{ker}\\;\\mu=\\mathrm{O}(n,k)$ and the factor $\\mu(g)$ is']",,"['\\subsection{Clifford algebra}Clifford algebras are far-reaching generalizations of the classical Hamiltonian quaternions. One motivation to study Clifford algebras comes from the Euclidean rotational groups.For details, we refer to the reader \\cite[Chapters 8 and 9]{gr}. Let $(V, \\beta)$ be a quadratic space.Let \\[\\mathrm{C}(V, \\beta)=\\frac{T(V)}{\\langle x\\tensor x-\\beta(x, x).1 \\mid x\\in V\\rangle}\\]be the {\\it Clifford algebra}, where $T(V)$ is the tensor algebra.Then $\\mathrm{C}(V,\\beta)$ is $\\mathbb Z/2\\mathbb Z$-graded algebra, say, $\\mathrm{C}(V,\\beta)=\\mathrm{C}_0(V,\\beta)\\oplus \\mathrm{C}_1(V,\\beta)$.The subalgebra $C_0(V,\\beta)$ is called {\\it special Clifford algebra} and it is a Clifford algebra in its own right. Then there is a unique anti-automorphism, say $\\alpha : \\mathrm{C}(V, \\beta) \\rightarrow \\mathrm{C(V, \\beta)}$ such that $\\alpha|_{V}=Id_V$ (see, for example, \\cite[Proposition 8.15.]{gr}). Now suppose that $u_1, u_2, \\ldots, u_m$ are non-zero anisotropic vectors in $V$ such that $\\rho_{u_1}\\rho_{u_2}\\cdots\\rho_{u_m}=Id_V$, then $\\prod_{i=1}^m \\frac{1}{2}\\beta(u_i, u_i)\\in k^{\\times 2}$ (for details, see \\cite[Proposition 9.1.]{gr}). So from the above, we get a well-defined map from the orthogonal group to $k^{\\times}/k^{\\times 2}$ using Cartan-Dieudonne theorem. This map is called the \\textit{spinor norm} on orthogonal group. See the next section for precise definition.']"
,"['\\subsubsection{Gaussian elimination algorithm for $\\GO^{-}(2l,q)$}\\label{gausstwist}The algorithm is as follows:\\vspace{4mm}\\noindentStep $1$: \\vspace{-11mm}\\begin{enumerate}\t\\item[] \\textit{Input}: A matrix $g=\\begin{pmatrix}A_0 &X&Y\\\\E&A&B\\\\F&C&D\\end{pmatrix} \\in \\GO^{-}(2l,k)$.\t\\item[] \\textit{Output}: The matrix $g_1=\\begin{pmatrix}(A_0)_1 &X_1&Y_1\\\\E_1&A_1&B_1\\\\F_1&C_1&D_1\\end{pmatrix}$ \tis one of the following kind:\t\\begin{enumerate}\t\t\\item The matrix $A_1$ is a diagonal matrix $\\mathrm{diag}(1,\\ldots,1,\\lambda)$ with $\\lambda \\neq 0$.\t\t\\item The matrix $A_1$ is a diagonal matrix $\\mathrm{diag}(1,\\ldots,1,0,\\ldots,0)$ with number of $1$s\t\tequal to $m (<l-1)$.\t\\end{enumerate}\t\\item[] \\textit{Justification}: Using ER$1$ and EC$1$ we do the classical Gaussian elimination on a \t$(l-1)\\times (l-1)$ matrix $A$.\\end{enumerate}\\vspace{3mm}Step $2$: \\vspace{-12.2mm}\\begin{enumerate}\t\\item[] \\textit{Input}: matrix $g_1=\\begin{pmatrix}(A_0)_1 &X_1&Y_1\\\\E_1&A_1&B_1\\\\F_1&C_1&D_1\\end{pmatrix}$.\t\\item[] \\textit{Output}: matrix $g_2=\\begin{pmatrix}(A_0)_2 &0&Y_2\\\\E_2&A_2&B_2\\\\0&0&D_2\\end{pmatrix}$, where $A_2=\\mathrm{diag}\\,(1,\\ldots, 1, \\lambda)$.\t\t\\item[] \\textit{Justification}: Observe the effect of ER$3$. In the first case, $C_1$ becomes zero matrix. In the second case, first interchange all zero rows of $A_1$ with the corresponding rows of $C_1$ using $w_i$.  This will make $C_1=0$. Then if needed use ER$1$ and EC$1$ on $A_1$ to make it diagonal. From the above equations we get $X_1=0$, $\\tr AD=\\mu(g)I_{l-1}$, and $F_1=0$ which ensures that $A_2$ has full rank. \\end{enumerate}\\vspace{3.5mm}Step $3$: \\vspace{-11.5mm}\\begin{enumerate}\t\\item[] \\textit{Input}: matrix $g_2=\\begin{pmatrix}(A_0)_2 &0&Y_2\\\\E_2&A_2&B_2\\\\0&0&\\mu(g)A_2^{-1}\\end{pmatrix}$.\t\\item[] \\textit{Output}: \tmatrix $g_3=\\begin{pmatrix}(A_0)_3 &0&0\\\\0&A_3&B_3\\\\0&0&\\mu(g)A_3^{-1}\\end{pmatrix}$, where $A_3=\t\\mathrm{diag}(1,\\ldots,1,\\lambda)$.\t\t\\item[] \\textit{Justification}: Use EC$4$ and EC$5$ to make $E_2=0$. Then from the above equation we get $Y_2=0$.\\end{enumerate}\\vspace{4mm}Step $4$:\\vspace{-11.0mm}\\begin{enumerate}\t\\item[] \\textit{Input}: matrix $g_3=\\begin{pmatrix}(A_0)_3 &0&0\\\\0&A_3&B_3\\\\0&0&\\mu(g)A_3^{-1}\\end{pmatrix}$\t\\item[] \\']",,"['\\begin{remark}The above algorithm works for groups defined over an arbitrary field not necessarily algebraically closed field. For example, \t\\begin{enumerate}\t\t\\item Since all non-degenerate skew-symmetric bilinear forms are equivalent~\\cite[Corollary 2.12]{gr}, we have a Gaussian elimination algorithm for all symplectic similitude groups over an arbitrary field. \t\t\\item Since non-degenerate symmetric bilinear forms over a finite field of odd characteristics are classified~\\cite[Page 79]{gr} according to the $\\beta$ (see Equations~\\eqref{beta2} and~\\eqref{twisted_beta}), we have a Gaussian elimination algorithm for all orthogonal similitude groups over a finite field of odd characteristics.\t\t\\item Furthermore, we have a Gaussian elimination algorithm for orthogonal similitude groups that are given by the above bilinear form (see Equation~(\\ref{beta2})) over an arbitrary field.\t\t\\item For simplicity, we assume that  $\\mathrm{char}\\;(k)\\neq 2$, though our algorithm works well on fields of all characteristics for symplectic and orthogonal similitude groups.\t\tAlgorithms that we develop in this paper work only for a given bilinear form $\\beta$ (see Equations (2.1)-(2.3)). Though in our algorithm, we work with only one bilinear form $\\beta$, given by a fixed basis, with a suitable change of basis matrix our algorithm works for all  equivalent bilinear forms. \t\\end{enumerate}\t\\end{remark}']"
"['\\section{Introduction}\\label{Sec:Intro}The need for increasingly accurate simulations in sciences and technology results in large-scale mathematical models. Model order reduction (MOR) is a well-known tool to deal with such problems. Founded about half a century ago, this field is still getting attraction due to the fact that many complicated or large problems have not been considered and many advanced methods have not been invoked yet.Often, the full order model (FOM) depends on parameters. The reduced-order model (ROM), preferably parameter-dependent as well, is therefore required to approximate the FOM on a given parameter domain. This problem, so-called parametric MOR (PMOR), has been addressed by various approaches such as Krylov subspace-based, optimization, interpolation, and reduced basis technique. We focus here on the methods that use interpolation to build a ROM for the linear parametric control system.When balanced truncation is used for model reduction, one has to solve a pair of Lyapunov equations for two Gramians. Although advances in solving such equations have been made, it is still the most expensive step of this reduction method. Parametric model order reduction aims to determine reduced-order models for parameter-dependent systems. Popular techniques for parametric model order reduction rely on interpolation. Nevertheless, interpolation of Gramians is rarely mentioned, most probably due to the fact that Gramians are symmetric positive semidefinite matrices, a property that should be preserved by the interpolation method.In this contribution, we propose and compare two approaches for Gramian interpolation. In the first approach, the interpolated Gramian is computed as a linear combination of the data Gramians with positive coefficients. The second approach aims to tackle this issue by performing the interpolation on the manifold of fixed-rank positive semidefinite matrices. The results of the interpolation step are then used to construct parametric reduced-order models, which are compared numerically on two benchmark problems. <!--TRUNCATED-->']","[""\\section{Balanced truncation for parametric linear systems and standard interpolation}\\label{Sec:BT_standard interpolation}Balanced truncation is a well-known method for model reduction. In this section, we briefly review the square root procedure proposed in \\cite{TombP87} which is more numerically efficient than its original version. As other projection-based methods, a balancing projection for system \\eqref{eq:psys} must be constructed. This projection helps to balance the controllability and observability energies on each state so that one can easily decide which state component should be truncated. To this end, one has to solve a pair of the generalized Lyapunov equations for the controllability Gramian $P(\\mu)$ and the observability Gramian $Q(\\mu)$. The manifold $\\psd{k}{n}$ is here seen as a quotient manifold $\\R_*^{n \\times k}/\\calO_k$, where $\\R_*^{n \\times k}$ is the set of full-rank $n \\times k$ matrices and $\\calO_k$ is the orthogonal group in dimension $k$. This geometry has been developed in~\\cite{JouBAS10, MassA18,MassAH19} and has already been used in, e.g.,~\\cite{GouMMAJHM17, SabAVGE2019, SzcDBDPM2019} for solving different fitting problems. It relies on the fact that any matrix $A \\in \\psd{k}{n}$ can be factorized as $A  = YY^{T}$ with $Y \\in \\R_*^{n \\times k}$. Curve interpolation on $\\M$ is often done by encapsulating the interpolation into an optimization problem, e.g., one seeks the curve $\\bspline:\\mathbb{R}\\to\\M$ minimizing the mean squared acceleration of the piecewise surface. This is done with a technique close to the one used for curves, i.e., transferring the optimization problem on carefully chosen tangent spaces. The only difference here is that the curve itself is not computed on the tangent space; instead, the optimality conditions obtained on a Euclidean space are generalized to manifolds.Interpolation via surfaces is a little bit more intricate. In this work, we rely on B\\'ezier surfaces presented in~\\cite{Absil2016a} as a generalization of Euclidean B\\'ezier surfaces inspired from the manifold generalization of curves to manifolds already presented by Popiel et al.~\\cite{Popiel2007}. Consider a Euclidean space $\\R^r$. A Euclidean B\\'ezier curve and B\\'ezier surface of degree $K \\in \\mathbb{N}$ are functions $\\beta_{K,\\ell}$, $\\ell=1,2$, defined via Bernstein polynomials and control points. This equivalence permits to easily generalize B\\'ezier surfaces to a manifold $\\M$ by using the generalization of B\\'ezier curves based on the De Casteljau algorithm. <!--TRUNCATED-->""]","['\\section{Conclusion}\\label{Sec:Concl}In this paper, we have proposed and compared two approaches for interpolating Gramians in the context of parametric model order reduction. The first approach is based on a linear algebraic interpolation of the Gramians, while the second one performs the interpolation on the manifold of fixed-rank positive semidefinite matrices. Both approaches preserve the positive semidefiniteness of the Gramians, which is crucial for balanced truncation.The numerical experiments demonstrate that both methods can successfully construct reduced-order models for parametric systems. The algebraic approach is simpler to implement but may lead to higher-rank interpolated Gramians. The geometric approach, while more involved mathematically, maintains the rank structure of the original Gramians throughout the interpolation process.The choice between these methods depends on the specific requirements of the application, particularly regarding computational efficiency versus accuracy in maintaining the geometric properties of the Gramians. Future work could explore hybrid approaches that combine the strengths of both methods or extend these techniques to other types of matrix equations in model reduction. <!--TRUNCATED-->']","['\\section{Numerical examples}\\label{Sec:NumerExam}In this section, we consider two numerical examples. Before going into detail, we would like to discuss the general setting. First, for the choice of positive weight coefficients used in the algebraic approach, options are the weights based on distance from the test point to training points and linear splines. Our tests revealed that the latter delivers a smaller error. Moreover, since we have to gather all data to make a big matrix in this method, too much data may result in inefficiency. Therefore, in the numerical tests, we only use linear splines.To verify the accuracy of ROMs, we compute an approximate $\\mathcal{H}_\\infty$-norm of the absolute errors in the frequency response defined as the supremum of the 2-norm of the difference between the full and reduced transfer functions over a frequency range. For the reference of efficiency, all computations are performed with MATLAB R2018a on a standard desktop computer.\\subsection{A model for heat conduction in solid material}This model is adapted from the one used in \\cite{KrePT14}. Consider the heat equation with a heat conductivity coefficient that varies in two subdomains, leading to a parameter-dependent system. The equation is discretized using the finite element method with piecewise linear basis functions resulting in a system of dimension 1580. The input matrix originates from the source function, and the output matrix is a simple averaging operator.\\begin{figure}[th]\t\\begin{minipage}{.5\\textwidth}\t\t\\includegraphics[width=1\\textwidth]{morerr_interp.pdf}\\\\\t\t\\includegraphics[width=1\\textwidth]{morerr_interp3.pdf}\t\\end{minipage}%\\hspace*{2mm}\t\\begin{minipage}{.5\\textwidth}\t\t\\includegraphics[width=1\\textwidth]{morerr_maninterp.pdf} \\\\\t\t\\includegraphics[width=1\\textwidth]{morerr_maninterp3.pdf}\t\\end{minipage}\t\\caption{The heat conduction model: absolute errors at test points. Top figures show results for one training grid, bottom figures for another. The left figures present the errors for the ROMs obtained by the algebraic method and the right figures present that computed by the geometric method.}\t\\label{fig:err_example1}\\end{figure}']"
,,"['\\section{Conclusion}\\label{Sec:Concl}We presented two methods for interpolating the Gramians of parameter-dependent linear dynamical systems for using in parametric balanced truncation model reduction. The first method is merely based on linear algebra which takes no geometric structure of data into account. Thanks to  simplicity, it can be combined with the reduction process which enables an offline-online decomposition. This decomposition in turn accelerates the MOR process  in the online stage which suits very well in parametric settings. Moreover, it is more flexible with the change of parameter values and easier to implement. Meanwhile, the second method exploits the positive semidefiniteness of the data set and recent developments in matrix manifold theory. It reformulates the problem as interpolation on the underlying manifold and relies on the advanced techniques involving interpolating on different tangent spaces and blending the resulting objects to preserve the geometric structure as well as the regularity of data. This method is a good choice for fast interpolating the low-rank solutions of parametric Lyapunov equations and expected to work well if the numerical rank of such solutions does not change much. While the implementation of the geometric approach is challenging, it can result in lower reduced order  as it often offers  better approximation to the solution of the Lyapunov equations. <!--TRUNCATED-->']","['First, we fix a uniform grid $\\mu_1,\\dots, \\mu_q \\in \\mathcal{D}$ , which will be specified in the caption of error figures. At those points, we solve \\eqref{eq:LyapContr} and \\eqref{eq:LyapObser} using the low-rank ADI method \\cite{LiW02} with a prescribed tolerance $10^{-10}$. We end up with local approximate solutions whose rank varies from 25 to 27. In order to apply the geometric interpolation method, we truncate them to make all the Gramians of rank 25. In this case, the working manifold is $\\mathcal{S}_+(25,1580)$. Note that for the algebraic method presented in Section~\\ref{Sec:BT_standard interpolation}, local solutions at training points do not necessarily have the same rank.The computed solutions are then employed as the local Gramians to compute the interpolated Gramians which in turn are used to construct the ROM at test points. To determine the reduced order $r$, we use the criterion that $\\sigma_r(\\mu)/\\sigma_1(\\mu) < 10^{-8}$ which gives $r$ between 12 to 15 at different test points. <!--TRUNCATED-->']"
"['\\section{Introduction}Parametric model order reduction (PMOR) has become an important tool for reducing the computational complexity of large-scale dynamical systems that depend on parameters. The main goal is to approximate the original high-dimensional system by a reduced-order model that preserves the input-output behavior while allowing fast evaluation for different parameter values. A key challenge in PMOR is to construct reduced models that remain accurate over a wide range of parameter variations. Traditional approaches like balanced truncation and moment matching have been extended to handle parameterized systems, but they often require expensive computations for each new parameter value. Recent advances have focused on interpolation-based methods that construct reduced-order models by interpolating between precomputed models at sampled parameter points. These methods exploit the geometric structure of the reduced model spaces, particularly on matrix manifolds like the Grassmann manifold or the manifold of fixed-rank positive-semidefinite matrices. Several interpolation techniques have been proposed, including subspace interpolation on Grassmann manifolds, matrix interpolation, and interpolation of reduced basis vectors. However, many existing methods face limitations when dealing with more complex parameter dependencies or when preserving system properties like stability and passivity. This paper investigates new manifold interpolation approaches for PMOR, focusing on the geometry of the manifold of fixed-rank positive-semidefinite matrices. We explore how Riemannian geometric properties can be exploited to develop more accurate and efficient interpolation schemes for parameterized<!--TRUNCATED-->']",,,"['\\bibitem{FengRK05}Feng, L.H., Rudnyi, E.B., Korvink, J.G.: Preserving the film coefficient as a parameter in the compact\t\tthermal model for fast electrothermal simulation. \\newblock IEEE Trans. Computer-Aided Design Integr. Circuits Syst., \\textbf{24}(12), 1838--1847 (2005)\\newblock \\doi{10.1109/TCAD.2005.852660}']"
"['\\section{Introduction}\\subsection{Overview}Let $\\ku$ be a field. Let $\\GK$ be an abbreviation of Gelfand-Kirillov dimension. In this paper we contribute to the ongoing program of classifying Hopf algebras with finite $\\GK$. Let $H$ be a Hopf algebra and let $\\ydh$ be the category of Yetter-Drinfeld modules over $H$. Assume that $H$ is pointed. Basic invariants of $H$ are:\\begin{enumerate}[leftmargin=*,label=\\rm{(\\roman*)}]\\item the group of grouplikes $\\Gamma = G(H)$,\\item the diagram $R = \\oplus_{n \\in \\N_0} R^n$, a graded connected Hopf algebra in $\\ydg$,\\item the infinitesimal braiding $V := R^1$, an object in $\\ydg$.\\end{enumerate}In order to classify those $H$ with finite $\\GK$, one first needs to understand all such $R$ with finite $\\GK$. Since $R$ is coradically graded and connected it is strictly graded. Strictly graded Hopf algebras $R$ in $\\ydg$ with $R^1 \\simeq V$ are called post-Nichols algebras of $V$; also, graded Hopf algebras $R$ in $\\ydg$ generated by $R^1 \\simeq V$ are called pre-Nichols algebras of $V$.The Nichols algebra $\\toba(V)$ is isomorphic to the subalgebra of $R$ generated by $V$. When $\\car\\ku =0$ and $\\dim H< \\infty$, it was conjectured that $R = \\toba(V)$, which reduces our problem to classifying finite-dimensional Nichols algebras in $\\ydg$. The conjecture was proved valid assuming $\\Gamma$ is abelian. But beyond those hypotheses this fails to be true. Thus, it does not seem avoidable to consider:\\begin{enumerate}[leftmargin=*,label=\\rm{(\\Alph*)}]\\item classify all $V \\in \\ydg$ such that $\\toba(V)$ has finite $\\GK$,\\item for such $V$ classify all post-Nichols algebras with finite $\\GK$.\\end{enumerate}Lemma \\ref{lemma:prenichols-finite-gkd-postnichols} reduces Question (B) for $V$ as in (A) to classifying all pre-Nichols algebras of $V^*$ with finite $\\GK$.As usual it is more flexible to deal with these questions considering classes of braided vector spaces rather than classes of groups $\\Gamma$ and correspondingly pre-Nichols algebras as braided Hopf algebras. For Question (C) we point out that all pre-Nichols algebras of $V$ form a poset $\\pre(V)$ with $T(V)$ minimal and $\\toba(V)$ maximal and those with finite $\\GK$ form a saturated subposet $\\pref(V)$. We say that a pre-Nichols algebra is \\emph{eminent} if it is a minimum in $\\pref(V)$. We shall show that many other Nichols algebras with finite $\\GK$ have eminent pre-Nichols algebras. From now on we assume that $\\ku$ is algebraically closed and $\\car \\ku =0$. In this paper we are concerned with Question (C) for braided vector spaces $V$ of diagonal type, i.e. with braiding determined by a matrix $\\bq = (q_{ij})_{i, j \\in \\I}$ with entries in $\\ku^{\\times}$ where $\\theta \\in \\N$ and $\\I = \\{1, \\dots, \\theta\\}$. First we need to discuss Question (A) for this class. Finite-dimensional Nichols algebras of diagonal type, i.e. those with $\\GK = 0$, were classified through the notion of (generalized) root system. More generally the list of all Nichols algebras of diagonal type with finite root system is given. It was conjectured that Nichols algebras of diagonal type with finite $\\GK$ are those with finite root system. This conjecture was verified in various cases. We shall assume in a few proofs that the conjecture is true.Let $\\toba(V)$ be a finite-dimensional Nichols algebra of diagonal type with connected Dynkin diagram.']",,,"['\\subsection{The main results}In the present paper we focus on braided vector spaces of diagonal type of two kinds.Fix $V$ of diagonal type, with braiding given by the matrix $\\bq = (q_{ij})_{i, j \\in \\I}$.\\subsubsection{Quantum linear spaces}\\label{subsubsec:intro-qls}Here  we assume that $\\bq$ satisfies $q_{ij} q_{ji} = 1$ for all $i \\neq j \\in \\I$.The \\emph{distinguished pre-Nichols algebra} $\\wtoba_{\\bq}$  is presented  by generators$(x_i)_{i \\in \\I}$ and relations $x_i x_j - q_{ij} x_j x_i$, for all $i \\neq j \\in \\I$. We need some notation to state our first Theorem. Set\\begin{align}\\label{eq:intro-qls-subsets}\\begin{aligned}\\I^{\\infty} &= \\{i \\in \\I \\colon q_{ii} \\notin \\G_{\\infty} \\},&  \\I^N &= \\{ i\\in \\I \\colon \\ord q_{ii}  = N\\},\\ N \\geq 1,\\\\\\I^t &= \\bigcup_{N > 3} \\I^N, &\\I^{\\pm} &= \\{i \\in \\I \\colon q_{ii} = \\pm 1 \\} = \\I^1 \\sqcup \\I^2.  \\end{aligned}\\end{align}Thus $\\I = \\I^{\\pm} \\sqcup \\I^3 \\sqcup \\I^t \\sqcup \\I^{\\infty}$. For  $\\star\\in  \\N \\cup \\{\\pm, t,\\infty \\}$, let $V^{\\star}$ be the subspace of $V$ spanned by $(x_i)_{i \\in \\I^{\\star}}$ and $\\bq^{\\star}$ the restriction of $\\bq$ to $V^{\\star}$. Then \\begin{align*}V &=   V^{\\pm} \\oplus V^{3} \\oplus V^{t} \\oplus V^{\\infty}.  \\end{align*}The pre-Nichols algebras of $V^{\\pm}$ with finite $\\GK$ are described in \\S \\ref{subsubsec:pre-supersym}.<!--TRUNCATED-->']"
,,,"['Assume $y_3 \\neq 0$, so $W$ is 3-dimensional by a degree argument and its Dynkin diagram $\\bD$ is \\eqref{diagram:QLS-i-ij-j}. We show that $\\GK\\toba(W) = \\infty$. Consider the subspaces $ V_1 = \\ku y_1 \\oplus \\ku y_3$, $ V_2 = \\ku y_3 \\oplus \\ku y_2 \\subset W$; denote their corresponding Dynkin diagrams by $\\bD _1 $ and $\\bD _2$, respectively. From $q_{ij} q_ {ji} = 1$ it follows $x_{ij} = - q_{ij} x_{ji}$, so the image of $x_{ji}$ in $\\toba$ is not zero.We split the proof in several cases according to the possibilities for $\\ord q_{ii}$ and $\\ord q_{jj}$.\\medbreak\\textbf{Case 1: $q_{ii} \\notin \\G_{\\infty}$ or $q_{jj} \\notin \\G_{\\infty}$}. This essentially goes back to \\cite{R quantum groups}.Assume first $q_{ii} \\notin \\G_{\\infty}$.If $\\GK \\toba < \\infty$, it follows from \\cite [Lemmas 2.6 and 2.7] {AAH-triang} that there exists a natural number $k$ such that $(k)^! _ {q_{ii}} \\prod_{h=0}^{k-1} (1- q^h_{ii}) = 0$, which contradicts $q_{ii} \\notin \\G_{\\infty}$. The case $q_{jj} \\notin \\G_{\\infty}$ is  similar: since the image of $x_{ji}$ is not zero, we may apply the same argument as with $q_{ii}$. \\textbf{Case 2: $q_{jj} = 1$}. By the previous case, we might assume $q_{ii}$ is a root of unity, and by hypothesis its order must be $N_i > 3$.The  diagram $\\bD_1 $ is \\begin{align*}\\xymatrix{ \\underset{ i }{\\overset{q_{ii}}{\\circ}} \\ar  @{-}[rr]^{q_{ii}^2}  & & \\underset{ ij }{\\overset{q_{ii} }{\\circ} }}, &&\\text{Cartan type }\\quad \\begin{pmatrix}2 & 2 - N_i  \\\\2 - N_i & 2\\end{pmatrix}.\\end{align*}If $\\GK \\toba(V_1) < \\infty$ then \\cite{AAH-diag} implies that the Cartan matrix is of finite type. Thus we conclude $N_i=3$, a contradiction.\\textbf{Case 3: $q_{jj} = -1$}. By Case 1, assume that $q_{ii}$ is a root of unity. Its order is $\\geq3$. By  \\cite {AAH-diag}, $\\GK \\toba (V_1) = \\infty$ since the Dynkin diagram of $V _1$ is $$ \\xymatrix{ \\underset{ i }{\\overset{q_{ii}}{\\circ}} \\ar  @{-}[rr]^{q_{ii}^2}  & & \\underset{ ij }{\\overset{- q_{ii} }{\\circ} }} $$and this does not appear in \\cite [Table 1] {H-class}.\\textbf{Case 4: $q_{ii},q_{jj} \\notin \\Gp_2$}. Now $W$ has a connected  Dynkin diagram:$$ \\bD = \\xymatrix{ \\underset{ i }{\\overset{q_{ii}}{\\circ}} \\ar  @{-}[rr]^{q_{ii}^2}  & & \\underset{ ij }{\\overset{ q_{ii} q_{jj} }{\\circ} } \\ar  @{-}[rr]^{q_{jj}^2}  & & \\underset{ j }{\\overset{ q_{jj}']"
,,,"['\\subsection{Types $E_{6}, E_{7}$ and $E_{8}$ with $N=2$} By \\cite[p. 407] {AA-diag} the distinguished pre-Nichols algebra is presented by the quantum Serre relations and\\begin{align*}[x_{ijk}, x_j]_c=0 \\, \\text { if } i, j, k \\text { are all different and }\\widetilde{q_{ij}}, \\widetilde{q_{jk}} \\neq 1.\\end{align*}\\begin{lemma}\\label{lem:eminent-Etheta}If $\\ba$ is of type $E_{6}, E_{7}$ or $E_{8}$ with $N=2$, then $\\wtoba_{\\bq}$ is eminent.\\end{lemma}\\pfLet $\\toba \\in \\pref (\\bq)$. First we deal with the quantum Serre relations, which are always primitive. Fix $i\\neq j \\in \\I_{\\theta}$. Consider two possibilities.\\smallbreak $\\heartsuit$ $\\widetilde{q_{ij}} = 1$. In this case choose $k \\in \\I_{\\theta}$ different from $i$ and $j$ such that $\\widetilde{q_{i}} = 1$ but $\\widetilde{q_{ik}} \\neq 1$. We get $\\bq(\\alpha_{i}+\\alpha_{j}, \\alpha_{i}+\\alpha_{j})= 1$ and $\\wbq(\\alpha_{i}+\\alpha_{j}, \\alpha_{k})\\neq 1$. By \\cite[Lemma 2.8]{AAH-triang}, this warranties $x_{ij}=0$ in $\\toba$.\\smallbreak $\\heartsuit$ $\\widetilde{q_{ij}} \\neq 1$. In this case $i$ and $j$ are consecutive vertices in a subdiagram of type $A_4$ with $N=2$. By Lemma \\ref{lem:eminent-Atheta-N=2} \\ref{item:Atheta-N=2-x{iij}=0} below, it follows that $x_{iij}=0$ except in the following cases: $(i,j) \\in \\{ (2,1), (\\theta-3, \\theta), (\\theta-2, \\theta -1)\\}$. Fix such $(i,j)$, assume $x_{iij}\\neq 0$ and consider $\\ku x_1 \\oplus \\dots \\oplus \\ku x_{\\theta} \\oplus \\ku x_{iij} \\subset \\mP(\\toba)$. Then the Dynkin diagram of this braided vector space is of indefinite Cartan type. We illustrate the case $(i,j)=(\\theta-3, \\theta)$, the other cases being similar.\\begin{align*}\\xymatrix @C50pt %@R-15pt{  %\\overset{-1}{\\underset{221 }{\\circ}}&  &   \\overset{-1}{\\underset{\\theta }{\\circ}} &\\overset{-1}{\\underset{\\theta-3 \\, \\theta-3 \\, \\theta }{\\circ}} &%\\overset{-1}{\\underset{\\theta-2 \\, \\theta-2 \\, \\theta-1 }{\\circ}} \\\\\\overset{-1}{\\underset{1 }{\\circ}}\\ar  @{-}[r]^{-1}  &  \\overset{-1}{\\underset{2 }{\\circ}}  \\ar@{.}[r]  & %\\ar @{-}[ul]_{-1}\\overset{-1}{\\underset{\\theta-3 }{\\circ}} \\ar @<0.3ex> @{-}[u]_{-1}^{} \\ar@{-}[r]^{-1} \\ar @{-}[ur]_{-1}&  \\overset{-1}{\\underset{\\theta-2 }{\\circ}}  \\ar  @{-}[r]^{-1}&%  \\ar @{-}[ur]_{-1} &\\overset{-1}{\\underset{\\theta-1 }{\\circ}}.}\\end{align*}Thus Conjecture \\ref{conj:AAH} and Lemma \\ref{lem:subspace-of-primitives} imply $\\GK \\toba = \\infty$.\\medbreakFinally, fix $i,j']"
"['\\section{Introduction}Recent work in Randomized Numerical Linear Algebra (RandNLA) focuses on using random sketches of the input data in order to construct approximate solutions more quickly than with traditional deterministic algorithms. In this article, we consider \\emph{statistical} aspects of recently-developed fast RandNLA algorithms for the least-squares (LS) linear regression problem.The statistical analysis of Randomized Numerical Linear Algebra (RandNLA) algorithms within the past few years has mostly focused on their performance as point estimators. However, this is insufficient for conducting statistical inference, e.g., constructing confidence intervals and hypothesis testing, since the distribution of the estimator is lacking. In statistics, uncertainty assessment can be conducted through confidence interval construction and significance testing. There are two main challenges in studying the statistical and distributional properties of RandNLA algorithms. The first challenge is that there are two sources of randomness contributing to the statistical performance of RandNLA sampling estimators: one source is the random errors in the model, i.e., the $\\varepsilon_i$s, which are typically attributed to measurement error or random noise inherited by $\\mbf{Y}$; and the other source is the randomness in the random sampling procedure within the approximation algorithm.The second challenge is that these two sources of randomness couple together within the estimator in a nontrivial way. We address these challenges to studying the asymptotic distribution of a general RandNLA sampling estimator for LS problems. Our results are fundamentally different from previous results on the statistical properties of RandNLA algorithms (e.g.,~\\cite{Ma:13,Ma:15,raskutti:2015,chen2016statistical,2016arXiv160102068W,DCMW19_TR}), in that we provide asymptotic distribution analysis, rather than finite-sample concentration inequalities. The resulting asymptotic distributions open the possibility of performing statistical inference tasks such as hypothesis testing and constructing confidence intervals, whereas finite sample concentration inequality results may not. \\subsection{Main Results and Contributions}We study the asymptotic distribution of a general RandNLA sampling estimator for the LS linear regression problem, from both a theoretical and empirical perspective. {\\bf Data are a random sample.} We first consider the data as a random sample from a population, in which case the goal is to estimate the parameters of the population model.In this case, for this \\emph{unconditional inference}, we establish the asymptotic normality, i.e., deriving the asymptotic distribution, of sampling estimators for the linear model under general regularity conditions.We show that sampling estimators are asymptotically unbiased estimators with respect to the true model coefficients,and we obtain an explicit form for the asymptotic variance, for both fixed number of predictors (Theorem~\\ref{thm:beta_tilde-beta}) and diverging number of predictors (Theorem~\\ref{coro:diverg:p}). {\\bf Sampling Estimators.} Using these distributional results, we propose several efficient and asymptotically optimal estimators. Depending on the quantity of interest (e.g., $\\sbf\\beta_0$ versus some linear function of $\\sbf\\beta_0$ such as $\\mbf{Y}=\\mbf X\\sbf\\beta_0$ or $\\mbf{X}^T\\mbf X\\sbf\\beta_0$), we obtain different optimal sampling probabilities (Propositions~\\ref{thm:beta:pi1}, \\ref{thm:beta:pi3}, and~\\ref{thm:beta:pi2}) that lead to sampling estimators that minimize the Asymptotic Mean Squared Error (AMSE) in the respective context. {\\bf Data are given and fixed.} We then consider the data as given/fixed, in which case the goal is to approximate the full sample OLS estimate. In this case, for this \\emph{conditional inference}, we establish the asymptotic normality, i.e., deriving the asymptotic distribution, of sampling estimators for the linear model under general regularity conditions.We show that sampling estimators are asymptotically unbiased with respect to the OLS estimate, and we obtain an explicit form of the asymptotic variance and the Expected Asymptotic Mean Squared Error (EAMSE) of sampling estimators (Theorem~\\ref{thm:beta_tilde-beta_hat}).{\\bf Sampling Estimators}. Using these results, we construct sampling probability distributions that lead to sampling estimators that minimize the E']","['\\section{Sampling Estimation Methods}\\label{sec:weighted-estimate}In this section, we derive asymptotic properties of the RandNLA sampling estimator $\\tilde{\\sbf{\\beta}}$ under two scenarios: unconditional inference, which involves estimating the true model parameter $\\sbf{\\beta}_0$; and conditional inference, which involves approximating the full sample OLS estimator $\\hat{\\sbf{\\beta}}_{OLS}$. We use the AMSE and EAMSE to develop two criteria for sampling estimators, and we obtain several optimal estimators. \\subsection{Unconditional Inference: Estimating Model Parameters}\\label{subsec:est:beta}For Model (\\ref{linreg-matrix}), from the traditional statistical perspective of using the data to perform inference, one major goal is to estimate the underlying true model parameters, i.e., $\\sbf\\beta_0$. We refer to this as \\emph{unconditional inference}. For unconditional inference, both randomness in the data and randomness in the algorithm contribute to randomness in the RandNLA sampling estimators. The following theorem states that, in unconditional inference, the asymptotic distribution of the sampling estimator $\\tilde{\\sbf\\beta}$  is a normal distribution (with mean $\\sbf\\beta_0$ and variance $\\sigma^2\\mbf{\\Sigma}_0$). \\begin{thm}[\\textbf{Unconditional inference, fixed $p$}]\\label{thm:beta_tilde-beta}Assume the number of predictors $p$ is fixed and the following regularity conditions hold.\\begin{itemize}   \\item   \\textit{(A1)[Data condition].}    There exist positive constants $b$ and $B$ such that    $ b\\le \\lambda_{min}\\le\\lambda_{max} \\le B, $     where $\\lambda_{min}$ and  $\\lambda_{max}$  are the minimum and maximum eigenvalues of matrix $\\mbf X^T\\mbf X/n$, respectively.    \\item   \\textit{(A2)[Sampling condition].}  The sample size $r=O(n^{1-\\alpha})$, where $0\\le\\alpha<1$ and where the minimum sampling probability $\\pi_{min} = O(n^{-\\gamma_0})$, where $\\gamma_0\\ge1$.   The parameters $\\gamma_0$ and $\\alpha$ satisfy $\\gamma_0+\\alpha<2$. \\end{itemize} Under these assumptions, as the sample size $n\\to\\infty$, we have\\begin{eqnarray}   (\\sigma^2\\mbf{\\Sigma}_0)^{-\\frac 12}(\\sbf{\\tilde\\beta}-\\sbf\\beta_0)   &\\stackrel{d}{\\rightarrow}&   \\label{eqn:thm:beta_tilde-beta}   \\textbf{N}(\\sbf 0,\\mbf{I}_p)\\end{eqnarray}where $$\\mbf{\\Sigma}_0 =    (\\mbf X^T\\mbf X)^{-1}   \\left( \\mbf{X}^T(\\mbf I_n+\\mbf \\Omega)\\mbf{X} \\right)   (\\mbf X^T\\mbf X)^{-1}, \\quad \\mbf{\\Omega} = \\text{diag}\\{1/r\\pi_i\\}_{i=1}^n,$$ and $\\mbf{I}_p$ is the $p\\times p$ identity. Thus, for unconditional inference, the asymptotic mean of $\\sbf{\\tilde\\beta}$~is\\begin{equation}\\label{eqn:thm:uncond:ae}\\text{AE}(\\sbf{\\tilde\\beta})=\\sbf\\beta_0, \\end{equation}i.e., $\\sbf{\\tilde\\beta}$ is an asymptotically unbiased estimator of $\\sbf\\beta_0$, and the asymptotic variance of $\\sbf{\\tilde\\beta}$ is \\begin{equation}\\label{eqn:thm:uncond:avar}AVar(\\sbf{\\tilde\\beta})= \\sigma^2\\mbf{\\Sigma}_0. \\end{equation}\\end{thm}\\begin{prop}\\label{thm:beta:pi1}For the $AMSE(\\tilde{\\sbf\\beta}, {\\sbf\\beta}_{0}) $, we have that \\begin']","['\\section{Conclusion}\\label{sec:conclusion}In this article, we have studied the asymptotic properties of RandNLA sampling estimators for least squares problems. We derived the asymptotic distributions of sampling estimators in two complementary settings: unconditional inference, where the goal is to estimate the true model parameters; and conditional inference, where the goal is to approximate the full sample OLS estimate. In both settings, we showed that the sampling estimators are asymptotically normally distributed and asymptotically unbiased. Based on our asymptotic analysis, we proposed several optimal sampling estimators that minimize different criteria (AMSE for unconditional inference and EAMSE for conditional inference). These include the IC, RL, and PL estimators for unconditional inference, and the ICNLEV, RLNLEV, and PLNLEV estimators for conditional inference. Several of these estimators are new to the literature, and they provide improved performance compared to existing methods.Our theoretical results clarify the role of leverage scores in the sampling process, showing that while leverage scores are important, they are not always optimal. The empirical results demonstrate that our proposed estimators achieve better performance than existing methods in various settings. The asymptotic distributions we derived also enable downstream statistical inference tasks such as confidence interval construction and hypothesis testing.There are several directions for future work. First, it would be interesting to extend our analysis to other RandNLA algorithms beyond sampling methods. Second, our current analysis focuses on linear models - extending it to generalized linear models or other nonlinear models would be valuable. Finally, developing distributed implementations of our proposed methods could further improve their computational efficiency for large-scale problems.']","['\\section{MSE, AMSE and EAMSE: Technical Definition}\\label{sec:AMSE:defn}In this section, we review the well-known Mean Squared Error (MSE) criterion, and we also define and discuss the standard but less well-known Asymptotic Mean Squared Error (AMSE) and Expected Asymptotic Mean Squared Error (EAMSE) criteria.Let $\\sbf T_n$ be a $p\\times 1$ estimator of a $p\\times 1$ parameter $\\sbf\\nu$, for every $n$. One popular quality metric for the estimator $\\sbf T_n$ is the MSE, which is defined to be\\begin{eqnarray}MSE(\\sbf T_n; \\sbf \\nu)   &=& \\nonumber \\text{E}[ (\\sbf T_n-\\sbf\\nu)^T(\\sbf T_n-\\sbf\\nu) ]\\\\   &=& \\nonumber \\text{tr}(\\text{Var}(\\sbf T_n))+(\\text{E}(\\sbf T_n)-\\sbf\\nu)^T(\\text{E}(\\sbf T_n)-\\sbf\\nu).  \\end{eqnarray}The MSE can be decomposed into two terms: one term, $\\text{tr}(\\text{Var}(\\sbf T_n))$, quantifying the \\emph{variance} of the estimator; and one term, $(\\text{E}(\\sbf T_n)-\\sbf\\nu)^T(\\text{E}(\\sbf T_n)-\\sbf\\nu)$, quantifying the \\emph{squared bias} of the estimator. To evaluate the RandNLA sampling estimator $\\tilde{\\sbf\\beta}$ in estimating the true model parameter ${\\sbf\\beta}_{0}$ and the full sample OLS estimate $\\hat{\\sbf\\beta}_{OLS}$, we will be interested in the AMSE and EAMSE, respectively. These are the asymptotic counterparts of MSE in large sample theory.To define the AMSE, let $\\sbf T_n$ be a $p\\times 1$ estimator of a $p\\times 1$ parameter $\\sbf\\nu$, for every $n$, and let ${\\sbf\\Sigma_n}$ be a sequence of $p\\times p$ positive definite matrices. Assume $\\sbf\\Sigma_n^{-1/2}(\\sbf T_n-\\sbf \\nu)\\stackrel{d}{\\rightarrow}\\sbf Z$, where $\\stackrel{d}{\\rightarrow}$ denotes convergence in distribution, and assume $\\sbf Z$ is a $p\\times 1$ random vector such that its $i^{th}$ element $Z_i$ satisfies $0<\\text{E} ( Z_i^2)<\\infty$, for $i=1,\\ldots, p$. Then, the AMSE of $\\sbf T_n$, denoted $AMSE(\\sbf T_n; \\sbf \\nu)$, is defined to be\\begin{eqnarray}AMSE(\\sbf T_n; \\sbf \\nu)   &=& \\nonumber \\text{E}(\\sbf Z^T\\sbf\\Sigma_n\\sbf Z)  \\\\   &=& \\nonumber \\text{tr}(\\sbf\\Sigma_n^{1/2}\\text{Var}(\\sbf Z)\\sbf\\Sigma_n^{1/2})+(\\text{E}(\\sbf Z)^T\\sbf\\Sigma_n\\text{E}(\\sbf Z))  \\\\   &=&  \\label{eqn:mse20} \\text{tr}(\\text{AVar}(\\sbf T_n))+(\\text{AE}(\\sbf T_n)-\\sbf\\nu)^T(\\text{AE}(\\sbf T_n)-\\sbf\\nu), \\end{eqnarray}where $\\text{AVar}(\\sbf T_n)=\\sbf\\Sigma_n^{1/2}\\text{Var}(\\sbf Z)\\sbf\\Sigma_n^{1/2}$ and $\\text{AE}(\\sbf T_n)=\\sbf\\nu+\\sbf\\Sigma_n^{1/2}\\text{E}(\\sbf Z)$ denote the \\emph{asymptotic variance-covariance matrix} and the \\emph{asymptotic expectation} of $\\sbf T_n$ in estimating $\\sbf \\nu$, respectively.  To define the EAMSE']"
,,"['\\section{Conclusion}\\label{sec:conclusion}We have studied the asymptotic properties of RandNLA sampling estimators in LS linear regression models. We showed that under certain regularity conditions on the data distributions and sampling probability distributions, the sampling estimators are asymptotically normally distributed.  Moreover, the sampling estimators are asymptotically unbiased for approximating the full sample OLS estimate and for estimating true coefficients.Based on these asymptotic results, we proposed optimality criteria to assess the performance of the sampling estimators, based on AMSE and EAMSE.In particular, we developed six sampling estimators, i.e.,  IC, RLEV, PL, ICNLEV, RLNLEV, and PLNLEV, for minimizing AMSE and EAMSE, under a variety of settings. These empirical results demonstrate that these new sampling estimators outperform the conventional ones in the literature.For generalization, depending on the application, one may consider criteria other than AMSE and EAMSE. For example, when hypothesis testing problems are of primary interest, the power of the test is a more reasonable choice to serve as a criterion.Developing scalable sampling methods to optimize criteria such as this are of interest. <!--TRUNCATED-->']","[""\\subsection{Relationship of the Sampling Estimators}\\label{subsec:relation}Here, we study the relationships between the probability distributions given by IC, RL, PL, ICNLEV, RLNLEV, PLNLEV, and those given by SLEV and BLEV.\\subsubsection{ ``Shrinkage'' Properties of Proposed Estimators }\\begin{figure}[t] \\centering  \\includegraphics[width=0.75\\textwidth]{Figures_new/illu_orthogonal.pdf}\\caption{Relationship between sampling methods.Left panel: RLNLEV score ($\\sqrt{(1-h_{ii})h_{ii}}$) versus BLEV score ($h_{ii}$). Middle panel: RL score ($\\sqrt{h_{ii}} $) versus BLEV score ($h_{ii}$). Right panel: {SLEV score ($0.9h_{ii}+0.1p/n$, where $p/n=0.2$) versus BLEV score ($h_{ii}$).}}\\label{fig:OptimalShrinkage}\\end{figure}We illustrate the ``shrinkage'' property of the proposed optimal sampling probabilities, compared to the BLEV sampling probabilities.  For convenience,  we refer to the numerators of the sampling probabilities in a sampling estimators as the scores, e.g., the RL score is $\\sqrt{h_{ii}}$ and the RLNLEV score is $\\sqrt{(1-h_{ii})h_{ii}}$. In Figure~\\ref{fig:OptimalShrinkage}, we plot the RL score, RLNLEV score, and SLEV score ($0.9h_{ii}+0.1p/n$ with $p/n=0.2$) as functions of the leverage score $h_{ii}$ (i.e., the BLEV score in Figure~\\ref{fig:OptimalShrinkage}). Observe that the RLNLEV score  amplifies small $h_{ii}$s but shrinks large $h_{ii}$s.  Both RLNLEV and RL scores provide nonlinear shrinkage of the BLEV. The SLEV scores also shrink large $h_{ii}$s and amplify small $h_{ii}$s, but in a linear fashion.The advantage of such ``shrinkage'' is two-fold. On the one hand, the data with high leverage scores could be ``outliers.'' Shrinking the sampling probabilities of high leverage data points reduces the risk of selecting outliers into the sample. On the other hand, amplifying the sampling probabilities of low leverage data points reduces the variance of the resulting sampling estimators. <!--TRUNCATED-->""]"
,,,"['\\textbf{Remark.}The assumption that $\\delta>0$ implies that $\\rho((\\mbf{X}^T\\mbf{X})^{-1}\\mbf{X}^T(\\mbf{W}-\\mbf{I})\\mbf{X})\\rightarrow 0$ as $r\\rightarrow\\infty$. By the convergence of geometric series of matrices, the inverse of $[\\mbf{I}+(\\mbf{X}^T\\mbf{X})^{-1}\\mbf{X}^T(\\mbf{W}-\\mbf{I})\\mbf{X}]=\\mbf{X}^T\\mbf{W}\\mbf{X}$ exists and the expansion in ($\\ref{eqn:talor:inverse}$) is valid asymptotically. In the proof of Theorem~\\ref{thm:beta_tilde-beta} and Theorem~\\ref{thm:beta_tilde-beta_hat}, we will verify the condition in Lemma~\\ref{lem:1}, i.e., that $\\delta>0$.The exact magnitude of $\\delta$ depends on $(\\mbf W-\\mbf I)$, and it is different in Theorem~\\ref{thm:beta_tilde-beta} and Theorem~\\ref{thm:beta_tilde-beta_hat}. In  Appendix~\\ref{sec:proof:thm:beta_tilde-beta} and Appendix~\\ref{sec:proof:thm:beta_tilde-betahat}, we present the proofs of Theorem~\\ref{thm:beta_tilde-beta} and Theorem~\\ref{thm:beta_tilde-beta_hat}, respectively. The proof of Theorem~\\ref{thm:beta_tilde-beta} is much more complicated than that of Theorem~\\ref{thm:beta_tilde-beta_hat}. In conditional inference of Theorem~\\ref{thm:beta_tilde-beta_hat}, the data are given and the only randomness comes from sampling. However, in unconditional inference of Theorem~\\ref{thm:beta_tilde-beta}, we consider both unobserved hypothetical data sampled from the underlying population as well as the sample sampled from observations. Thus, one more layer of randomness needs to take into account. <!--TRUNCATED-->']"
"['\\section{Introduction}Apolarity is an important tool in commutative algebra and algebraic geometry which studies a form, $f$, by the action of polynomial differential operators on $f$. The quotient of all polynomial differential operators by those which annihilate $f$ is called the \\textit{apolar algebra} of $f$. In general, the apolar algebra of a form is useful for determining its Waring rank, which can be seen as the problem of decomposing the supersymmetric tensor, associated to the form, minimally as a sum of rank one supersymmetric tensors. In this article we study the apolar algebra of a product of linear forms, which generalizes the case of monomials and connects to the geometry of hyperplane arrangements.In the first part of the article we provide a bound on the Waring rank of a product of linear forms under certain genericity assumptions; for this we use the defining equations of so-called star configurations due to Geramita, Harbourne, and Migliore. In the second part of the article we use the computer algebra system \\textsc{Bertini}, which operates by homotopy continuation methods, to solve certain rank equations for catalecticant matrices. Our computations suggest that, up to a change of variables, there are exactly six homogeneous polynomials of degree six in three variables which factor completely as a product of linear forms defining an irreducible multi-arrangement and whose apolar algebras have dimension six in degree three.As a consequence of these calculations, we find six cases of such forms with cactus rank six, five of which also have Waring rank six. Among these are products defining subarrangements of the braid and Hessian arrangements. <!--TRUNCATED-->']",,"['\\section{Conclusions and further questions}There are two main results of this paper. The first is a bound on the initial degree of the apolar ideal of a generic arrangement, attained using defining equations of star configurations from~\\cite{GHM13}. From this we obtained a necessary condition on the size of a generic arrangement with a complete intersection apolar algebra, as well as a lower bound on the Waring rank of a generic arrangement. A subsequent question raised by Wakefield~\\cite{W06} remains wide open -- is the apolar algebra of a generic arrangement ever a complete intersection? To this we add two additional questions concerning the optimality of Proposition~\\ref{prop:genericlowdeg} and Corollary~\\ref{cor:genericWaring}. First, are there arbitrarily large generic arrangements in $\\mathbb{P}^n$ whose apolar ideals have initial degree $n+1$? Second, are there arbitrarily large generic arrangements in $\\mathbb{P}^n$ whose Waring rank is $\\binom{2n}{n}$? <!--TRUNCATED-->']","['\\section{Preliminaries}Let $\\mbk$ be an algebraically closed field of characteristic zero and put $R = \\mbk[X_0,\\ldots, X_n]$. Let $S=\\mbk[x_0,\\ldots, x_n]$ be the $R$-module defined by $R$ acting on $S$ via partial differentiation. Given a form $f\\in S$, the \\textit{apolar ideal} of $f$ is $\\text{Ann}_R(f) = \\stl \\vp\\in R :\\vp\\circ f = 0 \\str$. We write $\\ap_f=R/\\text{Ann}_R(f)$; this is the \\textit{apolar algebra} of $f$. The apolar algebra $\\ap_f$ is a graded Artinian Gorenstein algebra, and every graded Artinian Gorenstein algebra arises in this way.Now suppose $f\\in S_d$. A \\textit{Waring decomposition} of $f$ is a decomposition $f=c_1\\ell_1^d+\\cdots+c_k\\ell_k^d$, where $\\ell_1,\\ldots,\\ell_k$ are linear forms and $c_1,\\ldots,c_k\\in\\mbk$. The smallest number of linear forms needed in a Waring decomposition of $f$ is the \\textit{Waring rank} of $f$. We will focus on forms $f\\in S=\\mbk[x_0,\\ldots,x_n]$ which decompose as a product of (not necessarily distinct) linear forms as $f=\\ell_1^{m_1}\\cdots\\ell_k^{m_k}$. A natural geometric object to attach to the product $f=\\ell_1^{m_1}\\cdots\\ell_k^{m_k}$ is the \\textit{multi-arrangement} $(\\A,\\m)$ where $\\A=\\cup_{i=1}^k V(\\ell_i)$ is the union of the hyperplanes $V(\\ell_i)\\subset\\mbk^{n+1}$ and $\\m$ is a function which assigns to each hyperplane $H\\in\\A$ the integer $\\m(H)$, where $\\m(H)$ is the power to which the corresponding linear form appears in $f$. We put $|\\m|=\\sum_H \\m(H)$, which is the degree of the polynomial $f$. If $\\m(H)=1$ for all $H\\in\\A$ we will say $(\\A,\\m)$ is a \\textit{simple} arrangement and write $\\A$ instead of $(\\A,\\m)$. Given a multi-arrangement $(\\A,\\m)$ we define $\\Q(A,\\m):=\\prod_{H\\in\\A} \\alpha_H^{\\m(H)}$, where $\\alpha_H$ is a choice of linear form vanishing on $H$. If $\\A$ is simple then we write $\\Q(\\A)$ for the product $\\prod_{H\\in\\A} \\alpha_H$. We call $\\Q(\\A,\\m)$ and $\\Q(\\A)$ the \\textit{defining polynomial} of the multi-arrangement and arrangement, respectively. Moreover we write $|\\A|$ for the number of hyperplanes in $\\A$, so that if $f=\\Q(\\A,\\m)$, then $|\\A|$ is the number of distinct linear factors of $f$. For simplicity, throughout this note we will conflate a multi-arrangement or arrangement with its defining polynomial. For instance, by ``the apolar algebra of an arrangement"" we will mean the apolar algebra of its defining equation. <!--TRUNCATED-->']"
,,,"['The general problem of determining the degree $d$ irreducible multi-arrangements in $\\mathbb P^n$ that have minimal Waring rank (and minimal cactus rank) is currently out of  reach but  we leave it  as a suggestion for  a further path of research. It is worth  noting that each of the extremal  examples we found has interesting combinatorial properties.  In particular, after a change of coordinates, one is the  defining ideal of the $A_3$ braid arrangement. Another is half  of the Hessian arrangement. Perhaps there is a clue in the structure of these examples  that can help one search for higher degree extremal examples.  One promising avenue is to look for extremal behavior among the simplicial line arrangements catalogued by Grunbaum~\\cite{Grun09}; such arrangements have recently led to interesting examples for the \\textit{containment problem} between regular and symbolic powers~\\cite{SM17}.For now, we leave this as an open problem for the interested reader. <!--TRUNCATED-->']"
"['\\section{Introduction}Numerical linear algebra (NLA) is one of the great achievements of scientific computing. On most computational platforms, we can now routinely and automatically solve small- and medium-scale linear algebra problems to high precision. The purpose of this survey is to describe a set of probabilistic techniques that have joined the mainstream of NLA over the last decade. These new techniques have accelerated everyday computations for small- and medium-size problems, and they have enabled large-scale computations that were beyond the reach of classical methods.NLA definitively treats several major classes of problems, including solution of dense and sparse linear systems; orthogonalization, least-squares, and Tikhonov regularization; determination of eigenvalues, eigenvectors, and invariant subspaces; singular value decomposition (SVD) and total least-squares. In spite of this catalog of successes, important challenges remain. The sheer scale of certain datasets (terabytes and beyond) makes them impervious to classical NLA algorithms. Modern computing architectures (GPUs, multi-core CPUs, massively distributed systems) are powerful, but this power can only be unleashed by algorithms that minimize data movement and that are designed ab initio with parallel computation in mind.Randomization offers novel tools for addressing all of these challenges. This paper surveys these new ideas, provides detailed descriptions of algorithms with a proven track record, and outlines the mathematical techniques used to analyze these methods. Randomized methods can handle certain NLA problems faster than any classical algorithm. Randomization allows us to tackle problems that otherwise seem impossible. From an engineering point of view, randomization has another crucial advantage: it allows us to restructure NLA computations in a fundamentally different way.Before we decide what algorithm to use for a linear algebra computation, we must ask how we are permitted to interact with the data. A recurring theme of this survey is that randomization allows us to reorganize algorithms so that they control whichever computational resource is the most scarce (flops, communication, matrix entry evaluation, etc.). This paper covers fundamental mathematical ideas, as well as algorithms that have proved to be effective in practice. The balance shifts from theory at the beginning toward computational practice at the end. With the practitioner in mind, we have attempted to make the algorithmic sections self-contained, so that they can be read with a minimum of references to other parts of the paper. <!--TRUNCATED-->']",,,"['\\section{Trace estimation by sampling}\\label{sec:trace-est}Randomized methods for trace estimation depend on a natural technical idea: One may construct an unbiased estimator for the trace and then average independent copies to reduce the variance of the estimate. Algorithms of this type are often called \\emph{Monte Carlo methods}. We describe how to use standard methods from probability and statistics to develop \\emph{a priori} and \\emph{a posteriori} guarantees for Monte Carlo trace estimators. We show how to use structured random distributions to improve the computational profile of the estimators. Last, we demonstrate that trace estimators also yield approximations for the Frobenius norm and the Schatten 4-norm of a general matrix.Randomized trace estimation is based on the insight that it is easy to construct a random variable whose expectation equals the trace of the input matrix. Consider a random test vector $\\vct{\\omega} \\in \\F^n$ that is isotropic: $\\Expect[ \\vct{\\omega} \\vct{\\omega}^* ] = \\Id$. By the cyclicity of the trace and by linearity, $X = \\vct{\\omega}^* (\\mtx{A} \\vct{\\omega})$ satisfies $\\Expect X = \\trace(\\mtx{A})$. A single sample of $X$ is rarely adequate because its variance, $\\Var[X]$, will be large. The most common mechanism for reducing the variance is to average $k$ independent copies of $X$.The original application of randomized trace estimation was to perform \\emph{a posteriori} error estimation for large least-squares computations. More specifically, it was used to accelerate cross-validation procedures for estimating the optimal regularization parameter in a smoothing spline. See~\\citeasnoun{FORF18:Improved-Stochastic} for a list of contemporary applications in machine learning, uncertainty quantification, and other fields. <!--TRUNCATED-->']"
,,,"[""A curious observation is that techniques developed for kernel matrices appear to alsobe applicable for certain symmetric positive definite (pd) matrices that are not explicitlypresented as kernel matrices. This is a consequence of the well known fact that any pd matrix$\\mtx{K}$ admits a factorization\\begin{equation}\\label{eq:pd_K_fact}\\mtx{K} = \\mtx{G}^{*}\\mtx{G}\\end{equation}for a so called ``Gramian matrix'' $\\mtx{G}$. (If the eigenvalue decomposition of $\\mtx{K}$takes the form $\\mtx{K} = \\mtx{U}\\mtx{\\Lambda}\\mtx{U}^{*}$, then a matrix of the form$\\mtx{G} = \\mtx{V}\\mtx{\\Lambda}^{1/2}\\mtx{U}^{*}$ is a Gramian if and only if $\\mtx{V}$ is unitary.)The factorization (\\ref{eq:pd_K_fact}) says that the entries of $\\mtx{K}$ are formed bythe inner products between the columns of $\\mtx{G}$,\\begin{equation}\\label{eq:gofmm_kernel}\\mtx{K}(i,j) = \\ip{\\vct{g}_{i}}{\\vct{g}_{j}} = k(\\vct{g}_{i},\\vct{g}_{i}),\\end{equation}where $\\vct{g}_{i}$ is the $i$'th column of $\\mtx{G}$ (the ``Gram vector'') andwhere the $k$ is the inner product kernel of Example \\ref{ex:innerprodkernel}.At this point, it becomes plausible that the techniques of \\cite{2015_biros_ASKIT}for kernel matrices associated with points in high dimensional spaces may applyto certain pd matrices. The key to make this work is the observation that it isnot necessary to explicitly form the Gram factors $\\mtx{G}$. All that is needed inorder to organize the points $\\{\\vct{g}_{i}\\}_{i=1}^{n}$ are relative distancesand angles between the points, and we can evaluate thesefrom the matrix entries of $\\mtx{K}$, via the formula$$\\|\\vct{g}_{i} - \\vct{g}_{j}\\|^{2} =\\|\\vct{g}_{i}\\|^{2} - 2\\mbox{Re}\\,\\ip{\\vct{g}_{i}}{\\vct{g}_{j}} + \\|\\vct{g}_{j}\\|^{2} =\\mtx{K}(i,i) - 2\\mbox{Re}\\,\\mtx{K}(i,j) + \\mtx{K}(j,j).$$The resulting technique was presented in  \\cite{2017_biros_GOFMM} as a ``geometry oblivious FMM (GOFMM)'',along with numerical evidence of its usefulness for important classes of matrices.""]"
,,,"['Consider a standard normal random vector $\\vct{\\omega} \\sim \\normal(\\vct{0}, \\Id)$.  The variance of the resulting trace estimator~\\eqref{eqn:trace-est}--\\eqref{eqn:trace-est-avg} satisfies  \\begin{equation} \\label{eqn:trace-est-girard-var}  \\Var[ \\bar{X}_k ] = \\frac{2}{k} \\sum_{i,j = 1}^n \\abs{ (\\mtx{A})_{ij} }^2 = \\frac{2}{k} \\fnorm{\\mtx{A}}^2  \t\\leq \\frac{2}{k} \\norm{\\mtx{A}} \\trace(\\mtx{A}).  \\end{equation}  The rotational invariance of the standard normal distribution  allows us to characterize the behavior of this estimator in full detail.  Consider a Rademacher random vector $\\vct{\\omega} \\sim \\unif\\{\\pm 1\\}^n$.  The variance of the resulting trace estimator~\\eqref{eqn:trace-est}--\\eqref{eqn:trace-est-avg}  satisfies  $$  \\Var[ \\bar{X}_k ] =  \\frac{4}{k} \\sum_{1 \\leq i < j \\leq n} \\abs{ (\\mtx{A})_{ij} }^2 <  \\frac{2}{k} \\fnorm{\\mtx{A}}^2 \\leq  \\frac{2}{k}  \\norm{\\mtx{A}} \\trace(\\mtx{A}).  $$  This is the minimum variance trace estimator generated by  an isotropic random vector $\\vct{\\omega}$ with statistically independent coordinates.  It also avoids the simulation of normal variables.  \\citeasnoun{Gir89:Fast-Monte-Carlo} also studied the estimator  obtained by drawing $\\vct{\\omega} \\in \\F^n$ uniformly at random from  the sphere $\\sqrt{n} \\, \\mathbb{S}^{n-1}(\\F)$ for $\\F=\\mathbb{R}$. %  When $\\F = \\CC$, this approach has the minimax variance among all  trace estimators of the form~\\eqref{eqn:trace-est}--\\eqref{eqn:trace-est-avg}.  %  We return to this example in Section~\\ref{sec:near-isotropic}.  \\begin{algorithm}[t]  \\begin{algorithmic}[1]  \\caption{\\textit{Trace estimation by random sampling.} \\newline  See Section~\\ref{sec:trace-est-basic}.}  \\label{alg:trace-est}  \\Require\tPsd input matrix $\\mtx{A} \\in \\Sym_n$, number $k$ of samples  \\Ensure\t\tTrace estimate $\\bar{X}_k$ and sample variance $S_k$  \\Statex  \\Function{TraceEstimate}{$\\mtx{A}$, $k$}  \\For{$i = 1, \\dots, k$}  \t\\Comment Compute trace samples  \\State\tDraw isotropic test vector $\\vct{\\omega}_i \\in \\F^n$  \\State\tCompute $X_i = \\vct{\\omega}_i^* (\\mtx{A} \\vct{\\omega}_i)$  \\EndFor  \\State\tForm trace estimator: $\\bar{X}_k = k^{-1} \\sum_{i=1}^k X_i$  \\State\tForm sample variance: $S_k = (k-1)^{-1} \\sum_{i=1}^k (X_i - \\bar{X}_k)^2$  \\Statex  \\Comment\tUse compensated summation techniques for large $k$!  %  \\EndFunction  \\end{algorithmic}  \\end{algorithm}  \\begin{remark}[General matrices]  The assumption that $\\mtx{A}$ is psd allows us  to conclude that the standard deviation  of the randomized trace estimate is smaller  than the trace of the matrix.  The same methods allow us to estimate the  trace of a general square matrix, but the  variance of the estimator may no longer  be comparable with the trace.  \\end{remark}  \\subsection{{A priori} error estimates}  We can use theoretical analysis']"
,,,"['The analysis of the SLQ approximation requires heavy machineryfrom approximation theory.%See~\\citeasnoun{GM10:Matrices-Moments}and~\\citeasnoun{UCS17:Fast-Estimation} for more details.Algorithm~\\ref{alg:slq} contains pseudocode for SLQ.The dominant cost is$O(kq)$ matrix--vector multiplies with $\\mtx{A}$,plus $O(k q^2)$ additional arithmetic.  We recommendusing structured random test vectors to reduce the varianceof the resulting approximation.  The storage cost is$O(qn)$ numbers. <!--TRUNCATED-->']"
,,,"['We can store a sparse embedding using about $O(\\zeta n \\log d)$numbers.  We can apply it to a vector in $\\F^n$ with $O(\\zeta n)$arithmetic operations.  The main disadvantage is that we must use sparsedata structures and arithmetic to achieve these benefits.  Sparse signmatrices have similar performance to Gaussian embeddings.\\citeasnoun{Coh16:Nearly-Tight-Oblivious} has shown that a sparse signmatrix serves as an oblivious subspace embedding with constant distortionfor an arbitrary $k$-dimensional subspace of $\\RR^n$ whenthe embedding dimension $d = O(k \\log k)$ andthe per-column sparsity $\\zeta = O(\\log k)$.It is conjectured that improvements are still possible.\\begin{remark}[History]Sparse random embeddings emerged from the work of\\citeasnoun{Ach03:Database-Friendly-Random}and \\citeasnoun{CCF04:Finding-Frequent}.For randomized linear algebra applications, sparse embeddingswere promoted in~\\cite{CW13:Low-Rank-Approximation,MM13:Low-Distortion-Subspace,NN13:OSNAP-Faster}and~\\cite{Ura13:Fast-Randomized}.Analyses of the embedding behavior of a sparse map appear in \\cite{BDN15:Toward-Unified} and~\\cite{Coh16:Nearly-Tight-Oblivious}.\\end{remark}']"
,,,"['\\subsubsection{Gaussian test matrices}We can obtain precise results for the behaviorof the randomized rangefinder when the testmatrix is (real) standard normal.Let us present a variant of~\\citeasnoun[Thm.~10.1]{HMT11:Finding-Structure}.\\begin{theorem}[Rangefinder: Gaussian analysis] \\label{thm:rrf-gauss}Fix a matrix $\\mtx{B} \\in \\RR^{m \\times n}$ with singular values$\\sigma_1 \\geq \\sigma_2 \\geq \\dots$.Draw a standard normal test matrix $\\mtx{\\Omega} \\in \\F^{n \\times \\ell}$,and construct the sample matrix $\\mtx{Y} = \\mtx{B\\Omega}$.Choose $k < \\ell - 1$, and introduce the random variable$$Z = \\norm{ \\mtx{\\Gamma}^\\pinv }\\quad\\text{where $\\mtx{\\Gamma} \\in \\RR^{k \\times \\ell}$ is standard normal.}$$Then the expected error in the random rangefinder satisfies$$\\Expect \\norm{(\\Id - \\mtx{P}_{\\mtx{Y}}) \\mtx{B}}\t\\leq \\left( 1 + \\sqrt{\\frac{k}{\\ell-k-1}} \\right) \\sigma_{k+1}\t\t+ (\\Expect Z) \\left(\\sum\\nolimits_{j > k} \\sigma_{j}^2\\right)^{1/2}.$$\\end{theorem}In other words, the randomized rangefinder computesan $\\ell$-dimensional subspace that captures as much of theaction of the matrix $\\mtx{B}$ as the best $k$-dimensionalsubspace.  If we think about $k$ as fixed and $\\ell$ as the variable,we only need to choose $\\ell$ slightly larger than $k$ to enjoy this outcome.The error is comparable with $\\sigma_{k+1}$, the error in the best rank-$k$approximation, provided that the tail singular values$\\sigma_{j}$ for $j > k$ have small $\\ell_2$ norm.This situation occurs, for example, when $\\mtx{B}$has a rapidly decaying spectrum. <!--TRUNCATED-->']"
,,,"['There is no guarantee that the rank $k$ (that is, the number of rows or columns involved)will be close to the rank of the truncated SVD.  How close can we get in practice?When the singular values of $\\mtx{A}$ decay rapidly, then the minimal rank attainableby an approximate ID is close to what is attainable with an SVD.  Moreover,the algorithms we will describe for computing an ID produce an answer that is closeto the optimal one.When the singular values decay slowly, however,the difference in rank between the optimal ID and the optimal SVDcan be quite substantial \\cite{gu1996}.On top of that, the algorithms used to compute the ID can result in answers thatare still further away from the optimal value \\cite{2005_martinsson_skel}.<!--TRUNCATED-->']"
,,,"['The randomized Kaczmarz (RK) algorithm uses a probabilistic controlmechanism instead.  This kind of approach also has a long history,and it is useful in cases where cyclic control is ineffective.The RK method has received renewed attention owing to work of\\citeasnoun{SV09:Randomized-Kaczmarz}.They proposed sampling each $j(t)$ independently at random,with the probability of choosing the $i$th equation proportional to thesquared $\\ell_2$ norm of the $i$th column of $\\mtx{A}$.They proved that this version of RK converges linearly with a rate determined by the(Demmel) condition number of the matrix $\\mtx{A}$.  Later,it was recognized that this approach is just a particularinstantiation of SGD for the least-squares problem~\\eqref{eq:Ax=b_kacz}.See \\citeasnoun{2014_needell_randomized_kaczmarz}, which drawson results from \\citeasnoun{2011_bach_moulines}.There are many subsequent papers that have built on the RKapproach for solving inconsistent linear systems.\\citeasnoun{LL10:Randomized-Methods} observed that relatedideas can be used to design randomized Gauss--Seideland randomized Jacobi iterations.\\citeasnoun{2014_tropp_needell_paved} studied a blocked versionof the RK algorithm, which is practically more efficientfor many of the same reasons that other blocked algorithmswork well (Section~\\ref{sec:blocking}).\\citeasnoun{2015_richtarik_randomized_kaczmarz} observedthat the RK algorithm is a particular type of iterativesketching method.  Based on this connection, they proposeda generalization.  At each iteration, we draw anindependent random embedding $\\mtx{S}_t \\in \\F^{\\ell \\times m}$.The next iterate is chosen by solving the least-squares problem\\begin{equation}\\label{eq:Ax=b_kacz_proj0}\\vct{x}_{t} = \\argmin\\nolimits_{\\vct{y}} \\norm{ \\vct{x}_{t-1} - \\vct{y} }^2\\quad\\text{subject to}\\quad\\mtx{S}_t \\mtx{A}^* \\vct{y} = \\mtx{S}_t \\vct{b}.\\end{equation}The idea is to choose the dimension $\\ell$ sufficiently small thatthe sketched least-squares problem can be solved explicitlyusing a direct method (e.g., QR factorization).This flexibility leads to algorithms that converge more rapidly in practicebecause the sketch $\\mtx{S}_t$ can mix equations instead of just sampling.Later, \\citeasnoun{RT17:Stochastic-Reformulations}showed that this procedure can be accelerated to achieverates that depend on the \\emph{square root} of an appropriate conditionnumber; see also \\citeasnoun{GHRS18:Accelerated-Stochastic}.<!--TRUNCATED-->']"
,,,"['\\subsubsection{Stars and cliques}\\label{sec:star-clique}To describe the Cholesky algorithm on a graph, we need tointroduce a few more concepts from graph theory.%Define the \\emph{degree} and the \\emph{total weight} of a vertex $u$ in the multigraph $\\mtx{L}$ to be$$\\operatorname{deg}_{\\mtx{L}}(u) := \\sum\\nolimits_{e=uv \\in \\mtx{L}} 1\\quad\\text{and}\\quadw_{\\mtx{L}}(u) := \\sum\\nolimits_{e = uv \\in \\mtx{L}} w_{\\mtx{L}}(e).$$In other words, the degree of $u$ is the total number of multiedges $e$ that contain $u$.The total weight of $u$ is the sum of the weights of the multiedges $e$ that contain $u$.Let $u$ be a fixed vertex.  The \\emph{star}induced by $u$ is the Laplacian$$\\textsc{star}(u, \\mtx{L}) := \\sum\\nolimits_{e = uv \\in \\mtx{L}} w_{\\mtx{L}}(e) \\, \\mtx{\\Delta}_e.$$In words, the star includes precisely those multiedges $e$ in the multigraph $\\mtx{L}$that contain the vertex $u$.The \\emph{clique} induced by $u$ is defined implicitly as the correctionthat occurs when we take the Schur complement~\\eqref{eqn:schur-complement} of the Laplacian withrespect to the coordinate $u$:$$\\mtx{L} / \\vct{\\delta}_u %\t=: (\\mtx{L} - \\textsc{star}(u, \\mtx{L})) + \\textsc{clique}(u, \\mtx{L}).$$%%Recall that $\\vct{\\delta}_u$ is the standard basis vector in coordinate $u$.By direct calculation, one may verify that$$\\textsc{clique}(u, \\mtx{L}) = \\frac{1}{2 w_{\\mtx{L}}(u)} \\sum\\limits_{e_1 = uv_1 \\in \\mtx{L}} \\sum\\limits_{e_2 = uv_2 \\in \\mtx{L}}\tw_{\\mtx{L}}(e_1) w_{\\mtx{L}}(e_2) \\mtx{\\Delta}_{v_1v_2}.$$Each sum takes place over all multiedges $e$ in $\\mtx{L}$ that contain the vertex $u$.It can be verified that the clique is also the Laplacian of a weighted multigraph. <!--TRUNCATED-->']"
,,,"['Let $\\mtx{\\Omega} \\in \\F^{n \\times s}$ be a random test matrix,with $s > \\ell$.  By performing rank-one updates, we can efficiently maintainthe sample matrices$$\\mtx{Y}_t = \\bar{\\mtx{Z}}_t \\mtx{\\Omega} \\in \\F^{n \\times s}\\quad\\text{for $t = 1, 2, 3, \\dots$.}$$After collecting a sufficient number $r$ of samples, we canapply Algorithm~\\ref{alg:random-nystrom} to $\\mtx{Y}_t$ toobtain a near-optimal rank-$\\ell$ eigenvalue decompositionof the empirical approximation $\\bar{\\mtx{Z}}_r$.It usually suffices to take the sketch size $s$ to be proportionalto the rank $\\ell$ of the truncated eigenvalue decomposition.In this case, the overall approach uses $\\bigO(\\ell n)$ storage.  We can generate andprocess $r$ random features using $\\bigO((d+\\ell)rn)$ arithmetic, where $d$ is the dimension of $\\mathcal{X}$.The subsequent cost of the Nystr{\\""o}m approximation is $\\bigO(\\ell^2 n)$ operations.The streaming random features approach has storage and arithmetic costsroughly $\\ell / r$ times those of the direct random features approach.The streaming method can be combined with dimension reduction techniques(Section~\\ref{sec:fastfood}) for further acceleration.']"
"[""\\section{Introduction}\\label{Chap1sec1}Consider a linear {differential-algebraic equation} DAE of the form\\begin{align}\\label{DAE1}\\Delta: E\\dot{x}=Hx,\\end{align}where \\red{$x\\in \\mathscr X\\cong\\mathbb{R}^n$} is called the ``generalized'' state, $E\\in \\mathbb{R}^{l\\times n}$ and $H\\in \\mathbb{R}^{l\\times n}$. \\red{Throughout}, a linear DAE of form (\\ref{DAE1}) \\red{will be} denoted by $\\Delta_{l,n}=(E,H)$ \\red{or, shortly,} $\\Delta$ and the corresponding matrix pencil of $\\Delta$ by $sE-H$.The first aim of the paper is to find a way to \\red{relate} linear DAEs with linear control systems and find their \\red{geometric} connections. In fact, we will show that \\red{to} any linear DAE, we can attach a class of linear control systems defined up to a coordinates change, a feedback transformation and an output injection. We call this attachment the explicitation of a DAE.The second purpose of the paper is to distinguish two kinds of equivalences in linear DAEs theory, namely, internal equivalence and external equivalence. The external equivalence is widely considered in \\red{the} linear DAEs literature. However, since solutions of a DAE exist only on a constrained (invariant) subspace, sometimes we only need to perform the analysis on \\red{that} constrained subspace. This motivates to introduce the notion of internal equivalence \\red{and} to find normal forms not on the whole space but only on \\red{that} constrained subspace.The paper is organized as follows. In Section \\ref{Chap1sec2}, we introduce the notations and define equivalences. In Section \\ref{Chap1sec3}, we \\red{explain how} to associate \\red{to any} DAE \\red{a class of control systems}. In Section \\ref{Chap1sec4}, we describe \\red{geometric} relations. In Section \\ref{Chap1sec5}, we show the \\red{correspondence} between canonical forms. In Section \\ref{Chap1sec6}, we introduce the \\red{notion of} internal equivalence. Section \\ref{Chap1sec7} contains the proofs and Section \\ref{sec:conclusions} contains the conclusions. \\red{Finally, in the Appendix we recall two basic canonical forms.}<!--TRUNCATED-->""]",,,"['\\section{Preliminaries}\\label{Chap1sec2}We use the following notations in the present paper.\\begin{longtable}[l]{lll}\t$\\mathbb{N}$ & & the set of natural numbers with zero and $\\mathbb{N}^+=\\mathbb{N}\\backslash \\{0 \\}$\\\\\t$\\mathbb{C}$ & & the set of complex numbers\\\\\t${\\mathbb{R}^{n \\times m}}$ & &the set of real valued matrices with $n$ rows and $m$ columns \\\\\t$\\mathbb R[s]$ && the polynomial ring over $\\mathbb R$ with indeterminate $s$ \\\\\t$Gl\\left( {n,\\mathbb{R}} \\right)$ & & the group of nonsigular matrices of $\\mathbb{R}^{n \\times n}$\\\\\t${\\rm rank\\,} A$ & &the rank of \\red{a linear map} $A$\\\\\t${\\rm rank\\,}_{\\mathbb R[s]} (sE-H)$ && the rank of a polynomial \\red{matrix} $sE-H$ over $\\mathbb R[s]$\t\\\\\t$\\ker A$ & &the kernal of \\red{a linear map} $A$\\\\\t\t$\\dim\\, \\mathscr A$ && the dimension of \\red{a linear} space $\\mathscr A$\\\\\t${\\mathop{\\rm Im\\,}\\nolimits} A$ && the image of \\red{a linear map} $A$\\\\\t$\\mathscr A/{\\mathscr B}$ & & the  quotient of a vector space $\\mathscr A$ by a subspace $\\mathscr B\\subseteq \\mathscr A$ \\\\\t$I_n$ & & the identity matrix of size $n\\times n$ for $n\\in \\mathbb{N}^+$\\\\\t$0_{n\\times m}$ & & the zero matrix of size $n\\times m$ for $n,m\\in \\mathbb{N}^+$\\\\\t${A^T}$ & &the transpose of a matrix $A$\\\\\t\t\t${A^{-1}}$ && the inverse of a matrix $A$\\\\\t\t\t${A\\mathscr B}$ &&$\\{Ax\\,|\\,x\\in \\mathscr B\\} $, the image of $\\mathscr B$ under  \\red{a linear map} $A$\\\\\t${A^{-1}\\mathscr B}$ &&$\\{x\\,|\\,Ax\\in \\mathscr B\\} $, the preimage of $\\mathscr B$ under  \\red{a linear map} $A$\\\\\t\t${A^{-T}}\\mathscr B$ & & $(A^T)^{-1}\\mathscr B$\\\\\t$\\mathscr A^{\\bot}$\t&& $\\{ x\\,|\\,\\forall a\\in \\mathscr A: x^Ta\\!=\\!0\\}$\\end{longtable}']"
,,,"['\\begin{defn}\\label{Max invariant subspace}\tA linear subspace $\\mathscr M$ of $\\mathbb{R}^n$,  is called an invariant subspace of $\\Delta_{l,n}=(E,H)$ if for any $x^0\\in \\mathscr M$, there \\red{exists} a solution $x(t,x^0)$ of $\\Delta$ \\red{such that $x(0,x^0)=x^0$} and \\red{$x(t,x^0)\\in \\mathscr M$ for all $t\\in \\mathbb{R}$}. An invariant subspace $\\mathscr M^*$ of $\\Delta_{l,n}=(E,H)$ is called the maximal invariant subspace if for any other invariant subspace $\\mathscr M$ of $\\mathbb{R}^n$, we have ${\\mathscr M}\\subseteq \\mathscr M^*$.\\end{defn}']"
,,"['\\section{Conclusion}\\label{sec:conclusions}In this paper, we propose a procedure named explicitation for DAEs. The explicitation of a DAE is, simply speaking, attaching to the DAE a class of linear control systems defined up to a coordinates change, a feedback and an output injection. We prove that the invariant subspaces of the attached control systems have direct relations with the limits of the Wong sequences of the DAE. We  show that the Kronecker invariants of the DAE have direct relations with the Morse invariants of the attached control systems, and as a consequence, the Kronecker canonical form \\textbf{KCF} of the DAE and the Morse canonical from \\textbf{MCF} of control systems have a perfect correspondence. We also propose a notion named internal equivalence for DAEs and show that the internal equivalence is useful when analyzing the existence and uniqueness of solutions (internal regularity). <!--TRUNCATED-->']","['\\begin{proof}[Proof of Lemma \\ref{subs relationex}]\tWe first show \\red{that} equation (\\ref{RelationV2}) holds. Let independent vectors $v_1=\\left[ {\\begin{matrix}\t\t{{v^1_1}}\\\\\t\t{{v^2_1}}\t\t\\end{matrix}} \\right],...,v_\\alpha=\\left[ {\\begin{matrix}\t\t{{v^1_\\alpha}}\\\\\t\t{{v^2_\\alpha}}\t\t\\end{matrix}} \\right]\\in \\mathbb{R}^n$ form a basis of \t$$\tP\\mathscr V_{i+1}(\\Delta)\\mathop = \\limits^{(\\ref{VWDeltaIm_Delta})}\\mathscr V_{i+1}(\\Delta^{Impl})\\mathop = \\limits^{(\\ref{relation of VbarV})}{\\left[ {\\begin{matrix}\t\t\t{A}&{B}\\\\\t\t\t{C}&{D}\t\t\t\\end{matrix}} \\right]^{ - 1}} \\left[ {\\begin{matrix}\t\t{{\\mathcal{V}_{i}}(\\Lambda)}\\\\\t\t0\t\t\\end{matrix}} \\right], \t$$ \twhere $v^1_j\\in \\mathbb{R}^q, v^2_j\\in \\mathbb{R}^m, j=1,2,..., \\alpha$ (\\red{implying that} $\\dim\\,(\\mathscr V_{i+1}(\\Delta^{Impl}))=\\alpha$). Now without loss of generality, assume  $v^1_j \\ne 0$ for $j=1,...,\\kappa$ and  $v^1_j=0$ for $j=\\kappa+1,...,\\alpha$, \\red{where $\\kappa< \\alpha$ is the number of non-zero vectors $v^1_j$}.  Then from equation (\\ref{VbarVRelationpre}), it can be deduced that $v^1_j$ for $j=1,...,\\kappa$ form a basis of ${\\mathcal{V}_{i+1}({\\Lambda})}$. Moreover, from (\\ref{relation of VbarV}), it is not hard to see that  $v^2_j$ for $j=\\kappa+1,...,\\alpha$ form a basis of $\\mathcal{U}_i(\\Lambda)$.  Let $F_i\\in \\mathbb R^{m\\times \\kappa}$ \\red{be} such that $F_iv_j^1=v_j^2$ for $j=1,...,\\kappa$ (such $F_i$ exists), then $v_1,\\dots,v_\\alpha$ form a basis of \t$\t\\left[ {\\begin{matrix}\t\t{{\\mathcal{V}_{i+1}}(\\Lambda)}\\\\\t\t{F_i  {\\mathcal{V}_{i+1}}(\\Lambda)}\t\t\\end{matrix}} \\right]+\\left[ {\\begin{matrix}\t\t0\\\\\t\t{{\\mathcal{U}_i}(\\Lambda)}\t\t\\end{matrix}} \\right]\t$. \\red{Therefore,}\t$$\t\\left[ {\\begin{matrix}\t\t{{\\mathcal{V}_{i+1}}(\\Lambda)}\\\\\t\t{F_i {\\mathcal{V}_{i+1}}(\\Lambda)}\t\t\\end{matrix}} \\right] +  \\left[ {\\begin{matrix}\t\t0\\\\\t\t{{\\mathcal{U}_{i}}(\\Lambda)}\t\t\\end{matrix}} \\right]= {\\left[ {\\begin{matrix}\t\t\t{A}&{B}\\\\\t\t\t{C}&{D}\t\t\t\\end{matrix}} \\right]^{ - 1}} \\left[ {\\begin{matrix}\t\t{{\\mathcal{V}_{i}}(\\Lambda)}\\\\\t\t0\t\t\\end{matrix}} \\right],\t$$\t\\red{because both spaces have the same basis $v_1,\\dots,v_{\\alpha}$.}\tWe now prove \\red{that for any choice} of $F_i$, \\red{we have} $F_i\\in \\mathbb{F}({{\\mathcal{V}_i}(\\Lambda)})$. Pre-multiply the above equation by $\\left[ {\\begin{matrix}\t\t{A}&{B}\\\\\t\t{C}&{D}\t\t\\end{matrix}} \\right]$ \\red{on the left} \\red{to obtain}\t\\begin{align*}\t\\left[ {\\begin{matrix}\t\t(A+BF_i)\t{{\\']"
,,,"[""The integers $\\varepsilon'_1,...,\\varepsilon'_{a'}\\in \\mathbb{N}$ are the controllability indices of $( A^1,B^1)$.(ii) $A^{2}={\\rm diag}\\{ A^2_{\\rho'_1 },..., A^1_{\\rho'_{b'}}\\}$, where $A^2_{\\rho'}$ is given by $$\\begin{array}{lll}A_{\\rho'}^2  = \\left[ \\begin{smallmatrix}{{\\lambda_{\\rho'}}}&{ 1}&{}&{}\\\\{}& \\ddots & \\ddots &{}\\\\{}&{}& \\ddots &{ 1}\\\\{}&{}&{}&\\lambda_{\\rho'}\\end{smallmatrix} \\right] & {\\rm or} \\ \\A_{\\rho'}^2  = \\left[ \\begin{smallmatrix}{{\\Lambda_{\\rho'}}}&{ I}&{}&{}\\\\{}& \\ddots & \\ddots &{}\\\\{}&{}& \\ddots &{ I}\\\\{}&{}&{}&\\Lambda_{\\rho'}\\end{smallmatrix} \\right],&\\Lambda _{\\rho'} = \\left[ \\begin{smallmatrix}{s-{\\phi _{\\rho'}} }&{{-\\varphi _{\\rho'} }}\\\\{  {\\varphi _{\\rho'} }}&{s-{\\phi _{\\rho'}} }\\end{smallmatrix} \\right],\\end{array}$$where $\\lambda_{\\rho'}, \\varphi _{\\rho'},  \\phi _{\\rho'}\\in\\mathbb R$.<!--TRUNCATED-->""]"
"['\\centerline{\\bf{ $\\S$1.} Introduction and the main results}In the real world, non-smooth phenomena exist in large numbers because of the influence of natural laws and many factors, for more details see, for instance, $\\cite{M2008}$ and the references therein.Piecewise smooth differential system is a kind of important non-smooth system which is based on non-smooth model. Usually, piecewise differential systems have been considered when a straight line separates the plane in two half-planes. In recent years, many authors have studied intensively the number of limit cycles of discontinuous piecewise linear differential systems with two zones separated by a straight line, see for instance [2, 4, 9, 11, 13] and the references quoted in these papers.We are mainly interested in studying the existence of limit cycles for piecewise linear differential systems with two pieces separated by a nonlinear switching curve. In $\\cite{Lb2019}$, the authors considered the family of piecewise linear differential systems in the plane with two pieces separated by a cubic curve. They studied the class of discontinuous piecewise linear differential systems obtained by perturbing up to order 2 in the small parameter $\\epsilon$ the linear center $\\dot{x}=y$, $\\dot{y}=-x$, and they obtained that 7 is a lower bound for the Hilbert number of this family. In $\\cite{LZh 2019}$, the authors studied the discontinuous piecewise differential system in the plane with two pieces separated by a curve of $y=x^{n}$.In the present paper, motivated by the above references, we will study the number of limit cycles for Hamilton system under perturbations of piecewise polynomials of degree $n$ with switching curves $y=x^3$. We consider the following perturbed piecewise smooth differential system$$\\left(  \\begin{array}{c}    \\dot{x} \\\\    \\dot{y} \\\\  \\end{array}\\right)=\\left\\{\\begin{aligned}\\left(  \\begin{array}{c}    y+\\epsilon p^+(x,y) \\\\    -x+\\epsilon q^+(x,y) \\\\  \\end{array}\\right),\\ y\\geq x^3,\\\\\\left(  \\begin{array}{c}    y+\\epsilon p^-(x,y) \\\\    -x+\\epsilon q^-(x,y) \\\\  \\end{array}\\right),\\ y<x^3, \\\\\\end{aligned}\\right.\\eqno(1.1)$$where $$p^{\\pm}(x,y)=\\sum^{n}_{i+j=0}a^{\\pm}_{i,j}x^{i}y^{j},~~q^{\\pm}(x,y)=\\sum^{n}_{i+j=0}b^{\\pm}_{i,j}x^{i}y^{j}$$ are any polynomials of degree $n$. The first integral of system $(1.1)_{\\epsilon=0}$ is$$H(x,y)=\\frac{1}{2}(x^2+y^2)=\\frac{h}{2},~~h\\in(0,+\\infty).\\eqno(1.2)$$System $(1.1)_{\\epsilon=0}$ has a family of periodic orbits$$\\begin{aligned}L_{h}&=\\left\\{(x,y)|H(x,y)=\\frac{h}{2},~h\\in(0,+\\infty),~y\\geq x^3\\right\\}\\\\&\\cup\\left\\{(x,y)|H(x,y)=\\frac{h}{2},~h\\in(0,+\\infty),~y<x^3\\right\\}\\\\:&=L_{h}^{+}\\cup L_{h}^{-}.\\end{aligned}$$Let $H(n)$ denotes the upper bound of the number of limit cycles bifurcating from the period annulus around the origin for all possible polynomials $p^{\\pm}(x,y)$ and $q^{\\pm}(x,y)$ up to the first order Melnikov function, taking into account the multiplicity. Our main results are the following theorem.\\vskip 0.2 true cm\\noindent{\\bf Theorem 1.1.}Consider system (1.1), by using the first order Melnikov function in $\\epsilon$, we have$$H(1)=3;~~H(2)=6;~~H(n)\\leq 6\\left[\\frac{n}{2}\\right]+6~(n\\geq ']","[""\\centerline{\\bf{ $\\S$2}. Preliminaries}\\vskip 0.5 true cmConsider$$\\left(  \\begin{array}{c}    \\dot{x} \\\\    \\dot{y} \\\\  \\end{array}\\right)=\\left\\{\\begin{aligned}\\left(  \\begin{array}{c}    H_{y}^{+}(x,y)+\\epsilon f^+(x,y) \\\\    -H_{x}^{+}(x,y)+\\epsilon g^+(x,y) \\\\  \\end{array}\\right),\\ y\\geq \\phi(x),\\\\\\left(  \\begin{array}{c}    H_{y}^{-}(x,y)+\\epsilon f^-(x,y) \\\\    -H_{x}^{-}(x,y)+\\epsilon g^-(x,y) \\\\  \\end{array}\\right),\\ y<\\phi(x)\\\\\\end{aligned}\\right.\\eqno(2.1)$$where $H^{\\pm}$, $f^{\\pm}$, $g^{\\pm}$ and $\\phi(x)$ are all $C^{\\infty}$ functions satisfying $\\phi(0)=0$, $\\epsilon\\geq0$ is a small parameter. For system $(2.1)_{\\epsilon=0}$ we make the following assumptions:\\noindent{\\bf (A1)}: There exists an open interval $J$ such that for each $h\\in{J}$, there are two points $A(h)$ and $B(h)$ on the curve $y=\\phi(x)$ with $A(h)=(a(h),\\phi(a(h)))$, $B(h)=(b(h),\\phi(b(h)))$ and satisfying$$H^{+}(A(h))=H^{+}(B(h))=h,~~H^{-}(A(h))=H^{-}(B(h)),~~a(h)<0<b(h).$$\\noindent{\\bf (A2)}: There is a family of periodic orbits surrounding the origin with clockwise orientation and denoted by $L_{h}=L_{h}^{+}\\cup L_{h}^{-}$, $h\\in{J}$ where $L_{h}^{+}$ is defined by $H^{+}(x,y)=h$, $y\\geq\\phi(x)$ and starting from $A(h)$, ending at $B(h)$, $L_{h}^{-}$ is defined by $H^{-}(x,y)=H^{-}(A(h))$, $y\\leq\\phi(x)$ and starting from $B(h)$, ending at $A(h)$.\\noindent{\\bf (A3)}: Curve $L_{h}^{\\pm}$, $h\\in{J}$ are not tangent to curve $y=\\phi(x)$ at points $A(h)$ and $B(h)$. In other words, for each $h\\in{J}$,$$H_{x}^{\\pm}(x,y)+H_{y}^{\\pm}(x,y)\\phi^{'}(x)\\neq0$$at points $A(h)$ and $B(h)$.\\vskip 0.2 true cm\\noindent{\\bf Lemma 2.1.} Under assumptions {\\bf (A1)-(A3)}, for the first order Melnikov function of system (2.1), we have$$M(h)=\\int_{L_{h}^{+}}g^{+}dx-f^{+}dy+\\frac{H_{x}^{+}(A)+H_{y}^{+}(A)\\phi^{'}(a(h))}{H_{x}^{-}(A)+H_{y}^{-}(A)\\phi^{'}(a(h))}\\int_{L_{h}^{-}}g^{-}dx-f^{-}dy.\\eqno(2.2)$$\\vskip 0.2 true cm\\noindent{\\bf Proof}. Making the change of variables$$\\left\\{\\begin{aligned}    &x=x,\\\\    &z=y-\\phi(x),\\\\\\end{aligned}\\right.$$system (2.1) becomes the following $C^{\\infty}$ system$$\\left(  \\begin{array}{c}    \\dot{x} \\\\    \\dot{z} \\\\  \\end{array}\\right)=\\left\\{\\begin{aligned}\\left(  \\begin{array}{c}    {\\widetilde H}_{z}^{+}(x,z)+\\epsilon p^+(x,z) \\\\    -{\\widetilde H}_{x}^{+}(x,z)+\\epsilon q^+(x,z) \\\\  \\end{array}\\right)""]",,"['\\vskip 0.2 true cm\\noindent{\\bf Definition 2.2.}\\cite{Grau11} Let $p_0(x),p_1(x),...,p_{n-1}(x)$ be analytic functions on an open interval $J\\subset\\mathbb{R}$. The ordered set $(p_0(x),p_1(x),...,p_{n-1}(x))$ is said to be an ECT-system on $J$ if, for all $k=1,2,...,n$, any nontrivial linear combination$$\\alpha_0p_0(x)+\\alpha_1p_1(x)+...+\\alpha_{k-1}p_{k-1}(x)$$has at most $k-1$ isolated zeros on $J$ counted with multiplicities.\\vskip 0.2 true cm\\noindent{\\bf Lemma 2.3.}\\cite{xiong17} The ordered set $(p_0(x),p_1(x),...,p_{n-1}(x))$ is an ECT-system on $J$ if and only if, for each $k=1,2,...,n,$$$W(p_0,p_1,...,p_{k-1})\\neq0,$$ for all $x\\in J,$where $W(p_0,p_1,...,p_{k-1})$ is the Wronskian of functions $p_0(x),p_1(x),...,p_{k-1}(x).$\\vskip 0.2 true cm\\noindent{\\bf Lemma 2.4.}\\cite{li15} Consider $p+1$ linearly independent analytical functions $f_i:U\\rightarrow \\mathbb{R}, i=0,1,...,p$, where $U \\in \\mathbb{R}$ is an interval. Suppose that there exists $j \\in {0,,1,...,p}$ such that $f_{j}$ has constant sign. Then there exists $p+1$ constants $C_{i} ,i=0,1,...,p$ such that $f(x)=\\sum _{i=0}^{p}C_{i}f_{i}(x)$ has at least $p$ simple zeros in $U$.<!--TRUNCATED-->']"
"[""\\section*{Introduction}Consider, for the time being, a \\emph{stochastic} $d$-by-$d$ matrix $P$. The matrix $P$ represents a finite-dimensional \\emph{Markov chain}, a stochastic model where states transition to one another with some probability at discrete time steps according to the entries in the matrix. The asymptotic properties of the Markov chain, such as what the stationary distribution is (if it exists) and the rate at which the process converges to that distribution, are determined by the spectral theory of the matrix $P$.In the field of dynamical systems, we often start with a map $T$ on some state space $X$, and we want to answer questions such as ``what happens to most of the orbits of $T$ over a long time?'' and ``do regions of $X$ mix together over time, and at what rate?'' These questions are less about looking at individual orbits of points under $T$ and more about looking at what happens \\emph{on average}. Specifically, we can learn much about the dynamical system $(X,T)$ by studying how \\emph{probability densities} on $X$ change over time under the action of $T$. To formalize this process and to lead into the focus of this article, we consider a specific class of piecewise linear maps acting on $[-1,1]$. Define $T_{\\kappa} : [-1,1] \\to [-1,1]$ by a paired tent map. Let $f$ be a probability density on $[-1,1]$. Applying the map $T_{\\kappa}$ stirs the space up, moving the chocolate chips around; there is then a new density, call it $P_{\\kappa}f$, that describes the new locations of the chocolate chips. The operator $P_{\\kappa}$ can be defined on all integrable functions, and is bounded and linear; we call $P_{\\kappa}$ the \\emph{Perron-Frobenius operator associated to} $T_{\\kappa}$.Returning to the questions posed above, we note that $P_{\\kappa}$ is the infinite-dimensional analogue of the transition matrix $P$ for the Markov chain. If we want to find a ``stationary distribution'' for $T_{\\kappa}$, we really are looking for invariant densities, which are eigenvectors of $P_{\\kappa}$ with eigenvalue $1$. If all initial densities converge to an invariant density over time, then we have a good idea of where most of the points in $[-1,1]$ end up in the long run: no matter where they started, points will be distributed over $[-1,1]$ according to the invariant density. Moreover, if there is a gap in modulus between an eigenvalue of $1$ and the rest of the spectrum, this gap describes how quickly this convergence occurs, in the same way as described above for Markov chains.By inspection, if $\\kappa = 0$, then from the graph of $T_{\\kappa}$ it is clear that $T_{\\kappa}$ has \\emph{two} invariant densities: the characteristic functions on $[-1,0]$ and $[0,1]$, respectively. If $\\kappa > 0$, we can see that these two densities are no longer invariant, because there is mixing between the two intervals $[-1,0]$ and $[0,1]$. It is \\emph{a priori} unclear whether or not $T_{\\kappa}$ has an invariant density, and if it does whether it has a spectral gap; however, to answer these questions we can study $P_{\\kappa}$, as described above. <!--TRUNCATED-->""]",,,"['\\section*{Markov Maps and Partitions}The map $T_{\\kappa}$ is \\emph{Markov} when there is a finite collection $\\{R_i\\}_{i=1}^r$ of disjoint open intervals in $[-1,1]$ such that:\\begin{enumerate}\\item $[-1,1] \\setminus \\bigcup_i R_i$ is the collection of endpoints of the intervals $\\{R_i\\}$, and\\item if $R_i$ intersects $T_{\\kappa}(R_j)$, then all of $R_i$ is contained in $T_{\\kappa}(R_j)$.\\end{enumerate} The collection $\\{R_i\\}$ is called a \\emph{Markov partition} for $T_{\\kappa}$, even though it is not a partition, strictly speaking.Suppose that the paired tent map $T_{\\kappa}$ is Markov, with Markov partition $\\{R_i\\}_{i=1}^r$. If $V = \\subspan_{\\C}\\set{\\mathds{1}_{R_i}}{1 \\leq i \\leq r}$ and $P_{\\kappa}$ is the Perron-Frobenius operator for $T_{\\kappa}$, then $V$ is $P_{\\kappa}$-invariant (considered as a subspace of $BV$). The adjacency matrix for $T_{\\kappa}$ is given by the $r$-by-$r$ matrix $A_{\\kappa} = \\left[ a_{ij} \\right]$, where \\[ a_{ij} = \\begin{cases}1, & R_i^o \\subset T(R_j), \\\\0, & \\text{otherwise}.\\end{cases} \\] Define an isomorphism $\\phi_r : V \\to \\C^r$ by $\\phi(\\mathds{1}_{R_i}) = e_i$. Then the restriction of $P_{\\kappa}$ to $V$ can be represented by the $r$-by-$r$ matrix $M_{\\kappa} = (2(1+\\kappa))^{-1}A_{\\kappa}$, with $\\phi_r\\circ P_{\\kappa} = M_{\\kappa}\\circ \\phi_r$. Moreover, \\[ \\sigma(P_{\\kappa})\\setminus \\overline{B(0,(2(1+\\kappa))^{-1})} = (2(1+\\kappa))^{-1}\\sigma(A_{\\kappa})\\setminus \\overline{B(0,(2(1+\\kappa))^{-1})}. \\] <!--TRUNCATED-->']"
,,"['\\section*{Conclusion for Mixing Times}At the beginning of this paper, we asked about mixing times and mixing rates for dynamical systems. We can now answer that question for our two systems, $T_n$ and $\\tilde{T}_n$. For $T_n$, we have shown that the second-largest eigenvalue of the Perron-Frobenius $P_n$ is approximately $1-2\\kappa_n$ (Corollary \\ref{cor:sec-eval} and Lemma \\ref{lem:markov-op}). Thus the mixing time for $T_n$ is, ignoring a scale factor, \\[ \\frac{1}{\\abs{\\log(1-2\\kappa_n)}} \\sim \\frac{1}{2\\kappa_n} \\sim 2^{n-1}, \\] using the fact that $\\kappa_n \\sim 2^{-n}$.On the other hand, for $\\tilde{T}_n$, Corollary \\ref{cor:sec-eval-factor} says that the second-largest eigenvalue of the Perron-Frobenius operator has modulus at most $(1+n^{-1})(2+2\\kappa_n)^{-1}$. By a similar computation, the mixing time for $\\tilde{T}_n$ (with $n\\geq 6$) is $O(1)$, which is much smaller than the mixing time for $T_n$. This result matches our intuition: for $T_n$, taking the perturbation to zero (or $n$ to infinity) leads to no mixing between the two halves, so the mixing time should tend to infinity, whereas for $\\tilde{T}_n$, taking the perturbation to zero leads to a mixing tent map, and hence the mixing time should approach that for the unperturbed map. The difference in the orders of the mixing times indicates significant dynamical information about how these two systems are distinct, and we obtained this information by performing calculations with matrices (without touching the matrices themselves) and some analysis of roots of polynomials.']","['First, note that the action of $A_n$ restricted to $E^+$ can be seen as identifying the vectors $e_{i}$ and $e_{2n+5-i}$ and looking at the action of $A_n$ on the vectors $e_{n+3}$ to $e_{2n+4}$, because $E^+ = \\subspan_{\\C}\\{s_i\\}_{i=1}^{n+2}$ with $s_i = \\frac{1}{2}(e_{n+3-i}+e_{n+2+i})$. The columns of the matrix $A_n$ indicate the images under $T_n$ of the intervals $R_i$ for each $i$, and so the columns of the restriction $C_n$ indicate the images under $T_n$ of the intervals $R_{n+3}$ up to $R_{2n+4}$ under the identification of $R_i$ with $R_{2n+5-i}$ (considered with multiplicity). However, this is exactly what the columns of $B_n$ indicate, because $B_n$ is the adjacency matrix for $\\tilde{T}_n$, taken with the refined partition $\\{S_i\\}_{i=1}^{n+3}$. Since $\\iota$ represents the refinement of the partition, we have $\\iota C_n = B_n \\iota$. From this equality we can obtain the remainder of the results in Proposition \\ref{prop:adj-char-poly-factor}.<!--TRUNCATED-->']"
"['\\section{Introduction}\\label{derivation}The problem we consider is \\begin{equation}\\label{eq:ls}Ay=b\\end{equation}where $A=[a_{ij}] \\in\\Rnn$ and $b=[b_i] \\in\\Rn$ are given, and $y\\in\\Rn$ is the vector of the unknowns. The matrix $A$ is assumed to be nonsingular, so that the problem has a unique solution. We also assume that the problem needs to be solved in a distributed computational framework determined by a set of connected computational nodes which can communicate through a generic sequence of graphs. The considered problem is important as linear systems appear naturally in a number of applications. One important example of application is to Ordinary Kriging, an optimal linear prediction technique of the expected value of a spatial random field $\\Z(s),\\ s\\in\\mathbb{R}^n$. When the semivariogram $\\gamma (h)$ of the random field has a sill, it can be assumed that there is a range $\\bar{h}$ over which the covariance $Cov(\\Z(s),\\Z(s+h))=0$, when $|h|>\\bar{h}$. In this case the matrix $A$ of the Ordinary Kriging linear system becomes sparse.There is a vast literature devoted to solving systems of linear equations in the conventional centralized computational environment, as well as a number of results that cover parallelization of classical iterative methods which are applicable to the case of fully connected distributed computational environment. Our interest in this paper is the class of fixed point methods and their extensions to the distributed framework, as described above.The framework we consider in this paper for solving systems of linear equations, in more detail, assumes a network of computational nodes which communicate through a generic directed graph, which can depend on time. The focus of these algorithms is to ensure convergence of the local approximations to the global solution, in the presence of time-varying communication graphs.In this paper, we propose a novel distributed method to solve \\eqref{eq:ls}, which we refer to as DFIX (Distributed Fixed Point). DFIX assumes the same computational framework but differs significantly from the above mentioned methods. Namely, DFIX is derived starting from an associated (centralized) fixed point method, rather than basing the derivation directly on the initial linear system. With respect to the underlying graph, representing the connection among the computational agents, we consider both the case when the graph is fixed (i.e., the connectivity among the nodes is the same at any time during the execution of the algorithm) and the case when the network changes at every iteration. In the fixed graph case we prove that convergence holds if the network is strongly connected, while in the time-varying graph case we give suitable assumptions over the sequence of networks.Any system of linear equation \\eqref{eq:ls} with symmetric matrix $A$ can be considered as the first order optimality condition of an unconstrained optimization problem with cost function $\\frac{1}{2}x^tAx-b^tx.$ It is therefore of interest to compare the approach of solving \\eqref{eq:ls} applying some distributed optimization method to the minimization of the quadratic function $\\frac{1}{2}x^tAx-b^tx$ with DFIX. We thus compare computational and communication costs of DFIX with the state of the art optimization method and show that the computational costs with DFIX are significantly lower, while the communication costs are comparable or go in favor of DFIX, depending on the connectivity of the underlying graph.<!--TRUNCATED-->']","['\\section{DFIX method}We consider now a generic fixed point method for solving \\eqref{eq:ls} by the fixed point iterative method (\\ref{fpiter}), with $\\ M=[m_{ij}] \\in\\Rnn,\\ d=[d_i] \\in\\Rn$ defined in such a way that node $i$ contains the $i$-th row $M_i\\in\\R^{1\\times n}$ and $d_i\\in\\R.$ Moreover, we assume that the fixed point $y^*$ of \\eqref{fp} is a solution of \\eqref{eq:ls}. The algorithm is designed in such way that each node has its own estimate of the solution $ y^*.$ Thus at iteration $ k $ each node $ i $ has its own estimate $ x_i^k \\in \\mathbb{R}^n $ with components $ x_{ij}^k, \\; j=1,\\ldots,n.$ The DFIX method is presented in the algorithm below.\\\\\\noindent{\\bf Algorithm DFIX}\\begin{itemize}\\item[Step 0] Initialization: Set $k = 0$. Each node chooses $ x_i^0 \\in \\mathbb{R}^n.$ \\item[Step 1] Each node $i$ computes \\begin{equation}\\label{eq:step1}\\begin{aligned}&\\hx{ii}{k+1}=\\sum_{j=1}^n m_{ij}\\x{ij}{k}+d_i,\\\\& \\hx{ij}{k+1} =\\hx{ij}{k},\\ i \\neq j.  \\end{aligned}\\end{equation}\\item[Step 2] Each node $i$ updates its solution estimate\\begin{equation}\\label{eq:step2}\\x{i}{k+1}=\\sum_{j=1}^nw_{ij}\\hx{j}{k+1}\\end{equation} and sets $ k = k+1.$\\end{itemize}Notice that at Step 1 each node $ i $ updates only the $i$-th component of its solution estimate and leaves all other components unchanged, while in Step 2 all nodes perfom a consensus step using the set of vector estimates $ \\hx{j}{k+1}.$ Defining the global variable at iteration $k$ as\\begin{equation*}X^k=\\left( x_1^k;  \\ldots;  x_n^k \\right)\\in\\R^{n^2}, \\end{equation*}  Algorithm DFIX can be stated in a condensed form using $ X^k $ and the following notation\\begin{equation*}\\widehat M_i=\\left(\\begin{array}{ccccc}1& &  & & \\\\ &\\ddots & & &\\\\m_{i1} &\\dots &m_{ii}  &\\dots & m_{in}\\\\ & &  &\\ddots &\\\\ & &  & &1\\\\\\end{array}\\right)\\in\\Rnn, \\phantom{spa}\\widehat d_i=\\left(\\begin{matrix} 0 \\\\ \\vdots\\\\ d_i\\\\ \\vdots\\\\ 0 \\\\\\end{matrix}\\right)\\in\\Rn.\\end{equation*}Now, Step 1 can be rewritten as \\begin{equation*}\\hx{i}{k+1}=\\widehat{M}_i\\x{i}{k}+\\hat{d}_i,\\end{equation*} and we can rewrite the Steps 1-2 in matrix form as\\begin{equation}\\label{eq:globalit}X^{k+1}=(W\\otimes I)(\\mathcal MX^k+\\hat d)\\end{equation}where $\\mathcal M=\\diag\\left(\\widehat M_1,\\dots,\\widehat M_n\\right)\\in\\R^{n^2\\times n^2},$ $\\widehat d=\\left(\\hat d_1;\\dots;\\hat d_n\\right)\\in\\R^{n^2}$ and $\\otimes $ denotes the Kronecker product of matrices. The following theorem shows that for every $i \\in \\{1,\\ldots,n\\} $ the local sequence $\\{\\x{i}{k}\\}$ converges to the fixed point $y^*$ of \\eqref{fp}. Denote \\begin{equation*}X^*=\\left(y^*; \\ldots; y^*\\right) \\in\\R^{n^2}.  \\end{equation*}']","[""\\section{Conclusion}The paper presents DFIX, a novel distributed fixed point method for solving systems of linear equations in networks of computational agents. The method is derived from classical fixed point iterations and extended to operate in distributed environments with both fixed and time-varying communication graphs. Theoretical analysis proves linear convergence under appropriate conditions on the iterative matrix and network connectivity. Numerical results demonstrate DFIX's advantages over existing distributed optimization and linear system solving methods. Specifically, DFIX shows lower computational costs compared to distributed optimization approaches applied to equivalent quadratic problems. Communication costs are also favorable, depending on network connectivity. Comparisons with the projection-based method from \\cite{nedic} further highlight DFIX's efficiency when the iterative matrix has suitable properties.The method's practical relevance is illustrated through applications to kriging problems, where it efficiently handles sparse systems arising from spatial correlation models. Future work may explore extensions to asynchronous implementations and broader problem classes while maintaining the convergence guarantees established here.<!--TRUNCATED-->""]","['\\section{Preliminaries}  Let us first briefly recall the theory of fixed point iterative methods for systems of linear equations. Given a generic method of type (\\ref{fp})\\begin{equation}x^{k+1} = M x^k + d, \\label{fpiter}\\end{equation}we know that the method is convergent if $\\rho(M) < 1$, where $\\rho(M)$ is the spectral radius of $M$. This condition is both necessary and sufficient for convergence. Given any matrix norm $\\|\\cdot\\|$ one can also state the sufficient convergence condition as $\\|M\\| < 1$. There are many ways of transforming (\\ref{eq:ls}) to the fixed point form (\\ref{fp}), depending on the properties of $A$, with Jacobi and Gauss-Seidel methods, as well as their relaxation versions being the most studied methods.Assume that $A$ is a nonsigular matrix with nonzero diagonal entries. Using the splitting $A = D - P$, with $D$ being the diagonal matrix, $D = \\diag(a_{11},\\ldots,a_{nn})$, the Jacobi iterative method is defined by (\\ref{fpiter}) with $M = D^{-1}P:=M_{J}$. The method is linearly convergent for many classes of matrices, for example strictly diagonally dominant matrices, symmetric positive definite matrices etc, and the rate of convergence is determined by $\\rho(M_{J})$. To speed up convergence and extend the class of matrices for which the method is convergent, one can introduce the relaxation parameter $\\omega \\in \\mathbb{R}$ and define $M = \\omega D^{-1}P + (1-\\omega) I$. Let us now define precisely the computational environment we consider. Assume that the network of nodes is a directed network ${\\cal G} = ({\\cal V},{\\cal E})$, where ${\\cal V}$ is the set of nodes and ${\\cal E}$ is the set of all edges. The graph $\\G=(\\V,\\E)$ is strongly connected if for every couple of nodes $i,j$ there exists an oriented path from $i$ to $j$ in $\\G$. We associate with ${\\cal G}$ an $n \\times n$ matrix $W$, such that the elements of $W$ are all nonnegative and each row sums up to one. The matrix $W \\in \\mathbb{R}^{n \\times n}$ is row stochastic with elements $w_{ij}$ such that $w_{ij} > 0$ if $j\\in O_i$, $w_{ij} = 0$ if $j \\notin O_i$. Let us denote by $w_{min}$ a constant such that all nonzero elements of $W$ satisfy $w_{ij} \\geq w_{\\min}>0$. The diameter of a network is defined as the largest distance between two nodes in the graph. Let us denote with $\\delta$ the diameter of $\\G$.<!--TRUNCATED-->']"
,,"['\\section{Conclusions}We proposed a class of novel, iterative, distributed methods for the solution of linear systems of equations, derived upon classical fixed point methods. We proved global convergence in the case when the communication network is strongly connected and we showed that the convergence rate depends on the diameter of the network and on the norm of the underlying iterative matrix. In particular we have that if the graph is strongly connected, the obtained result is analogous to the classical, centralized, case. We extended the presented method to the time-varying case and we proved an analogous convergence result, assuming the networks satisfy suitable joint con\\-nec\\-tivity assumptions, comparable with those required by different methods in literature.Our algorithm was compared with the relevant methods presented in \\cite{harnessing} and \\cite{nedic}. The numerical results showed good performance of DFIX compared with the mentioned methods. In particular, in the vast majority of the considered tests, DFIX outperformed the two methods in terms of both computational cost and communication traffic.']","['\\subsection{Strictly diagonally dominant systems}Let us now  consider a linear system $Ax=b$ of order $n = 100$, where $ A $ and $ b $ are generated as follows. For every index $i$ we take $b_i$ randomly generated with uniform distribution in $(0,1)$, and $A$ is a symmetric diagonally dominant random matrix obtained as follows: take $\\hat{a}_{ij}\\in(0,1)$ with uniform distribution and  then set $\\tilde{A}=\\frac{1}{2}(\\hat{A}+\\hat{A}^T)$ and finally $A = \\hat{A}+(n-1)I$, where we denote with $I$ the identity matrix of order $n$.  As the underlying network we consider an $m$-regular graph with $n$ nodes. For every fixed value of the degree $m$ we generate, as just described, 10 random linear systems, solve all of them using the three methods and compute the average number of iterations necessary to arrive at termination. For each method, the total amount of computation and communication are then obtained multiplying the average number of iterations and the per-iteration computational cost and communication traffic, respectively.The matrix $W$ is defined as in \\eqref{eq:metropolisregular}, the step sizes $\\alpha$ and $\\eta$, the initial guess at each node and the termination condition are as in the previous test.In Figures 5 and 6 we plot the results  for $m$ in in $\\{2, 4, \\dots, 48, 50\\}$.Similarly to the previous test, we have that DFIX outperforms both Harnessing an Projection method in terms of computation and communication. From Figure 6 we can notice that the communication required by the two methods for distributed linear systems, DFIX and Projection, is similar and that the difference with the communication required by Harnessing method increases as the degree of the graph increases. Regarding the computational cost (Figure 5), we have that while DFIX is cheaper than the other two methods, Projection method seems to be more influenced by the connectivity of the network and it is more efficient than Harnessing only for large vaues of the degree.\\\\']"
"['\\section{Introduction}The numerical solution of partial differential equations (PDEs) using adaptive finite element methods has become a standard tool in scientific computing. A key challenge in this context is the development of reliable and efficient error estimators to guide the adaptive refinement process. While substantial progress has been made for elliptic problems, the situation is more complicated for nonlinear and time-dependent problems where multiple error components (spatial, temporal, algebraic) interact.This paper focuses on the numerical analysis of a posteriori error estimation for nonlinear parabolic problems solved by implicit time discretization and adaptive finite element methods. The main difficulties arise from:1) The coupling between spatial and temporal discretization errors2) The nonlinearity of the differential operator3) The iterative nature of the algebraic solver4) The interaction between different error components in the adaptive processWe present a unified framework for a posteriori error estimation that simultaneously accounts for spatial discretization errors, time discretization errors, and algebraic iteration errors. Our approach combines duality techniques with careful analysis of the nonlinear terms, leading to computable error estimators that can be localized for adaptive mesh refinement.The key contributions include:- A general error representation formula that separates different error components- Fully computable error estimators for spatial, temporal and algebraic errors- Analysis of the effect of algebraic solver tolerance on the overall error- Numerical verification of the theoretical results on benchmark problemsThe proposed estimators are shown to be reliable and efficient, providing a solid foundation for adaptive algorithms that can balance different error components. Particular attention is paid to the practical implementation aspects, including the treatment of nonlinear terms and the interaction between adaptive time stepping and spatial mesh refinement. <!--TRUNCATED-->']",,,"['\\section{Notation and Preliminaries}Let $\\Omega \\subset \\mathbb{R}^d$ ($d=2,3$) be a bounded polygonal/polyhedral domain with Lipschitz boundary $\\partial\\Omega$. We consider a family $\\{\\mathcal{T}_h\\}$ of shape-regular triangulations of $\\Omega$ into simplices $K$ with diameter $h_K$. Let $h := \\max_{K\\in\\mathcal{T}_h} h_K$ denote the mesh size. For each element $K\\in\\mathcal{T}_h$, we denote by $\\mathcal{F}_h(K)$ the set of its faces and by $\\mathcal{F}_h := \\bigcup_{K\\in\\mathcal{T}_h} \\mathcal{F}_h(K)$ the set of all faces. The faces are divided into interior faces $\\mathcal{F}_h^I$ and boundary faces $\\mathcal{F}_h^B$. For a given polynomial degree $p\\geq 1$, we define the discontinuous Galerkin space:\\begin{equation*}S_h^p := \\{v_h \\in L^2(\\Omega) : v_h|_K \\in \\mathbb{P}_p(K) \\ \\forall K\\in\\mathcal{T}_h\\}\\end{equation*}where $\\mathbb{P}_p(K)$ denotes the space of polynomials of degree at most $p$ on $K$. The space $S_h^p$ is equipped with the broken Sobolev norms:\\begin{align*}\\|v_h\\|_{1,h}^2 &:= \\sum_{K\\in\\mathcal{T}_h} \\|\\nabla v_h\\|_{0,K}^2 + \\sum_{\\gamma\\in\\mathcal{F}_h} h_\\gamma^{-1} \\|\\llbracket v_h \\rrbracket\\|_{0,\\gamma}^2 \\\\\\|v_h\\|_{2,h}^2 &:= \\|v_h\\|_{1,h}^2 + \\sum_{K\\in\\mathcal{T}_h} h_K^2 |v_h|_{2,K}^2\\end{align*}where $h_\\gamma$ denotes the diameter of face $\\gamma$, and $\\llbracket\\cdot\\rrbracket$ represents the jump operator across faces. <!--TRUNCATED-->']"
"['\\section{Introduction}Let $\\Om \\subset \\IR^d$ be a polygonal domain with the boundary  $\\gom=\\partial\\Om$. We consider an abstract partial differential equation in the form $\\krl \\u = f$, where $\\u:\\Om \\to \\IR$ is the unknown solution, $\\krl$ is a linear differential operator and $f$ is the right-hand side. The problem has to be accompanied by suitable boundary conditions. Further, let $\\Vh$ be a finite-dimensional space of functions where an approximation of $\\u$ is sought. In many practical applications, we are not interested in the solution itself but rather in a quantity of interest, which is the value of a certain target functional $\\J(\\u)$. This gives rise to goal-oriented error estimates, requiring the solution of both primal and dual algebraic systems. We solve both systems simultaneously using the bi-conjugate gradient method (BiCG) which allows control of algebraic errors for both systems. We develop a stopping criterion that is cheap to evaluate and guarantees the algebraic error estimation is smaller than the discretization error estimation. Using this criterion and adaptive mesh refinement, we obtain an efficient and robust method for numerical solution of PDEs, demonstrated by numerical experiments. The approach is applicable to general linear operators discretized by variational formulations, though we demonstrate it for elliptic and convection-diffusion problems solved via $hp$-adaptive discontinuous Galerkin methods. <!--TRUNCATED-->']","[""\\section{Framework for the goal-oriented error estimates}\\label{sec:GO}We briefly recall a general framework for the goal-oriented error estimates. More details can be found, e.g., in \\cite{BeckerRannacher01,GileSuli02}.\\subsection{Primal problem}Let the {\\em weak formulation} of the primal problem be given by $a( \\u, \\vp) = \\ell(\\vp)\\qquad \\forall \\vp\\in \\V$, where $\\u\\in \\V$ is a weak solution, $a(\\cdot,\\cdot):\\V\\times \\V\\to\\IR$ is a bilinear form, $\\ell(\\cdot):\\V\\to\\IR$ is a linear form and $V$ is a Hilbert space. We assume that the weak formulation is well-posed.For numerical approximation, let $\\Vh,\\ h>0$ be a finite element space of piecewise polynomial functions related to a partition of $\\Om$ onto finite elements $\\Th$. The discrete solution $\\uh\\in\\Vh$ satisfies $\\ah(\\uh,\\vp_h) = \\ell_h(\\vp_h)\\qquad \\forall \\vp_h\\in\\Vh$, where $\\ah$ and $\\ell_h$ are discretized forms. We define the residual of the primal problem by $\\res{\\uh}{\\vp}:= \\ell_h(\\vp) - \\ah(\\uh, \\vp) = \\ah(\\u-\\uh,\\vp)$.\\subsection{Quantity of interest and the dual problem}We consider a linear functional $\\J:\\Wh\\to \\IR$ defining the quantity of interest. To estimate the error $\\J(\\u) - J(\\uh)$, we solve the dual problem $\\ah(\\psi_h, \\zh) = \\J(\\psi_h) \\qquad \\forall \\psi_h\\in\\Vh$, where $\\zh$ is the discrete dual solution. The residual of the dual problem is $\\resD{\\zh}{\\psi}:= \\J(\\psi) - \\ah(\\psi,\\zh) = \\ah(\\psi, \\z-\\zh)$.\\subsection{Abstract goal-oriented error estimates}For adjoint consistent discretization, we have the equivalence $\\ell_h(\\z) = \\ah(\\u, \\z) = \\J(\\u)$ and its discrete variant $\\ell_h(\\zh) = \\ah(\\uh, \\zh) = \\J(\\uh)$. This leads to the error equivalence $\\J(\\u -\\uh) = \\ell_h(\\z -\\zh)$. We derive primal and dual error identities using Galerkin orthogonality and residuals.\\subsection{Abstract goal-oriented error estimates including algebraic errors}Since exact discrete solutions $\\uh$ and $\\zh$ are unavailable, we work with approximations $\\uhk$ and $\\zhk$ from iterative solvers. We extend the error identities to include algebraic errors: $\\J(\\u - \\uhk) = \\res{\\uhk}{\\z - \\zhk} + \\res{\\uhk}{\\zhk} = \\errSk + \\errAk$ for the primal problem, and similarly for the dual problem. These identities hold without Galerkin orthogonality.\\subsection{Computable goal-oriented error estimates}The algebraic errors $\\errAk$ are computable, while discretization errors $\\errSk$ require unknown exact solutions. We approximate them using higher-order reconstructions $\\huh=\\krr(\\uhk)$ and $\\hzh=\\krr(\\zhk)$, leading to computable estimates $\\etaSk$ and $\\etaDSk$. The total error estimates become $\\J(\\u - \\uhk) \\approx \\etaSk + \\etaAk$ and $\\ell_h(\\z - \\zhk) \\approx \\etaDSk + \\etaDAk$. Note these approximations don't provide guaranteed bounds.\\subsection{An alternative representation of the algebraic error}We present alternative decompositions: $\\J(\\u - \\uhk) = \\J(\\u - \\uh) + \\J(\\uh - \\uhk)=: \\estSk + \\estAk$, where $\\estAk$ represents algebraic error differently than $\\errAk$. Both representations are algebraically consistent but may converge at different rates. <!--TRUNCATED-->""]",,"['\\section{Solution of primal and dual discretized problems}\\label{sec:algeb}In this section, we introduce the algebraic representation ofthe goal-oriented error estimates from the previous section, present the BiCG methodallowing a simultaneous solution of the primal and dual problems and discussseveral possibilities  algebraic errors estimates andstopping criteria for the iterative solver.\\subsection{Algebraic representation}\\label{sec:alg1}Let $\\{\\vp_i:\\Om\\to\\IR,\\ i=1,\\dots, \\Nh \\}$ be a basis of the finite-dimensional space $\\Vh$.We define the matrix $\\mA\\in\\IR^{\\Nh\\times\\Nh}$ by\\begin{align}  \\label{mA}  \\mA = \\{\\mA_{i,j}\\}_{i,j=1}^{\\Nh},\\quad \\mA_{i,j} := \\ah(\\vp_i,\\vp_j),\\ i,j=1,\\dots, \\Nh,\\end{align}where $\\ah$ is the bilinear form defined by \\eqref{ah}.Then the primal and dual discrete problems \\eqref{eq:PPh} and \\eqref{eq:DPh}are equivalent to the solution oftwo linear algebraic systems\\begin{align}  \\label{AA1}  \\mA\\bx = \\bb\\qquad\\mbox{and}\\qquad \\mA^{\\T}\\by=\\bc,\\end{align}respectively, where $\\bx\\in\\IR^\\Nh$ and  $\\by\\in\\IR^\\Nh$ are the algebraic representationof the primal and dual solutions given by\\begin{align}  \\label{AA2}  & \\bx = (\\x_1,\\dots, \\x_{\\Nh})^{\\T} \\ \\leftrightarrow \\uh =\\sum\\nolimits_{i=1}^\\Nh \\x_i \\vp_i, \\\\  \\quad   \\mbox{and} \\quad &   \\by = (\\y_1,\\dots, \\y_{\\Nh})^{\\T} \\ \\leftrightarrow  \\zh =\\sum\\nolimits_{i=1}^\\Nh \\y_i \\vp_i,  \\notag\\end{align}respectively, and $\\bb\\in\\IR^\\Nh$ and  $\\bc\\in\\IR^\\Nh$ are the algebraic representationof the right-hand sides of the primal and dual problems given by\\begin{align}  \\label{AA3}  & \\bb = (\\b_1,\\dots, \\b_{\\Nh})^{\\T} \\ \\leftrightarrow \\b_i = \\ell_h(\\vp_i),\\ i=1,\\dots,\\Nh,  \\\\    \\mbox{and} \\quad   &  \\bc = (\\c_1,\\dots, \\c_{\\Nh})^{\\T} \\ \\leftrightarrow \\c_i =\\J(\\vp_i),\\quad i=1,\\dots,\\Nh,   \\notag\\end{align}respectively. <!--TRUNCATED-->']"
,,"[""\\section{Summary of the results and outlook}\\label{sec:sum}We developed an efficient technique for the numerical solution ofprimal and dual algebraic systems arising in the goal-orientederror estimation and mesh adaptation.Both algebraic systems are solved simultaneously by BiCG method which allowsto control the algebraic error during the iterative process.The proposed $\\sigma$-stopping criterion is cheap for the evaluation andsignificantly reduce the computational costs.Moreover, it guarantees that the algebraic error estimatebounded by the discretization one.Further natural step is to extend this approach for the solution of nonlinearpartial differential equations. The the dual problem has to be build on alinearization of the primal one. However, employing a Newton-like methodfor the solution of the discretized primal problem,an approximate solution of the dual problem is availableat each Newton step and the technique developed in this paper can be employed.However, it is necessary to balancethe linear algebraic errors, the non-linear algebraic errors andthe discretization errors. This is the subject of the further work.\\paragraph{Acknowledgements} The authors are thankful to their colleagues from the Char\\-les University,namely M.Kub{\\'\\i}nov{\\'a}, T. Gergelits and F. Roskovecfor a fruitful discussion.""]","['In finite precision arithmetic, the first equalities in \\eqref{xi6} and \\eqref{xi7} still hold up to some small inaccuracy.However, if the orthogonality conditions \\eqref{orthog} are not (approximately) satisfiedduring finite precision computations, then the second equalities in \\eqref{xi6} and \\eqref{xi7} do not (approximately) hold; for more details and examples see \\cite{StTi2011}.Note that in our experiments in Section~\\ref{sec:numerF}, the orthogonality conditions \\eqref{orthog} are well preserved and, therefore, the evaluations {\\PPa}--{\\PPc} provide almost the same results.Finally, let us mention that if $\\bx_0=0$ and $\\by_0=0$, the relations  \\eqref{xi6} and \\eqref{xi7} imply\\begin{align}  \\label{xi8}  \\J(\\uhk) =\\ell_h(\\zhk),\\end{align}which together with \\eqref{eq:eqi} imply \\eqref{eq:equi_algB}. We recall that the lastrelation is valid only if $\\uhk$ and $\\zhk$ are obtained by the BiCG method in exact arithmetic.']"
,"[""\\section{Basic methods and algorithms for state estimation}\\label{sec:basic}The goal of data assimilation is to incorporate measured observations into a model of a dynamical system in order to produce estimates of the current system state (and future system states) which are as accurate as possible. In that sense data assimilation can be defined as an approximation of a true state of a physical system at a given time, by combining time-distributed observations with the dynamical system model in some optimal way.One can view the data assimilation problem as a Bayesian inference problem. Let $x\\in\\mathbb{R}^n$ be a model state that we would like to estimate. In Bayesian statistics, we model $x$ as a realisation of a random variable (here a random vector), $X:\\Omega\\rightarrow\\mathbb{R}^n$. If $\\Theta:\\Omega\\rightarrow\\mathbb{R}^p$ is another random variable with mean zero modelling the observational noise, then we can also model the observed variable $Y\\in\\mathbb{R}^p$ as a random variable, defined by\\[Y = h(X)+\\Theta,\\]  where $h:\\mathbb{R}^n\\rightarrow\\mathbb{R}^p$ is a (in general nonlinear) continuous map which models the transformation of the system space to the observation space. $X$ and $\\Theta$ are assumed to be independent. We would like to infer information about states $x$ given realisations $y$ of $Y$, which is a Bayesian inverse problem \\cite{bardsley2018,Stu2010,dashti2016,allmaras2013,calv2007,calv2018,apte2008}.If we assume that $\\Theta\\in\\mathbb{R}^p$ is a random variable with probability density $\\pi$, and $y$ as well as $x$ realisations of the random variable $Y$ and $X$, respectively, then the probability of $y$ given $x$ is given by $\\pi_{Y|X}(y|x) = \\pi(y-h(x))$. This is often referred to as the data likelihood. Further let $\\pi_X(x)$ be the probability density function of $X$, which describes our prior believes about the distribution of $X$. Then, by Bayes' formula, $\\pi_{X|Y}(x|y)$, the \\emph{posterior conditional probability density} function of $x$ given the observations $y$ is given by\\beq\\label{eq:bayes}\\pi_{X|Y}(x|y) = \\frac{\\pi_{Y|X}(y|x)\\pi_X(x)}{\\pi_Y(y)}\\propto\\pi_{Y|X}(y|x)\\pi_X(x),\\eeqwhere $\\pi_Y(y) = \\int_{\\mathbb{R}^n}\\pi_{Y|X}(y|x)\\pi_X(x)dx$ is a normalisation constant depending only on $y$ (see, for example \\cite{Stu2010,LawStuZyg2015,reich2015}). In general it is hard to obtain the entire probability density $\\pi_X(x|y)$, in particular in higher dimensions. However, if we make some assumptions about the probability density functions of the prior and the likelihood, the problem of finding the posterior density can be simplified, which leads to data assimilation algorithms in the traditional sense, which we discuss in the next sections. We will distinguish between variational and sequential data assimilation methods. <!--TRUNCATED-->""]",,"['\\subsection{Variational data assimilation}\\label{sec:var}If the prior density in the Bayesian inference problem (\\ref{eq:bayes}) is Gaussian, that is $X\\sim\\mathcal{N}(x^B,B)$ with mean $x^B\\in\\mathbb{R}^n$ (often called the background vector) and positive definite background error covariance matrix $B\\in\\mathbb{R}^{n\\times n}$, then we have  \\[\\pi_X(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\det{B}}}\\exp\\left(-\\frac{1}{2}(x-x^B)^TB^{-1}(x-x^B)\\right).\\]Here $x\\in\\mathbb{R}^n$ is the state vector we would like to estimate. If, in addition, the observation error is also Gaussian, that is $\\Theta\\sim\\mathcal{N}(0,R)$ (or $Y\\sim\\mathcal{N}(h(X),R)$) with positive definite error covariance matrix $R\\in\\mathbb{R}^{p\\times p}$, then the likelihood function can be written as \\[\\pi_{Y|X}(y|x) = \\frac{1}{\\sqrt{(2\\pi)^p \\det{R}}}\\exp\\left(-\\frac{1}{2}(y-h(x))^T R^{-1}(y-h(x))\\right).\\]and hence \\[\\pi_X(x|y)\\propto\\exp\\left(-\\frac{1}{2}\\|y-h(x)\\|^2_{R^{-1}} -\\frac{1}{2}\\|x-x^B\\|^2_{B^{-1}}\\right),\\]where $\\|z\\|^2_{R^{-1}} := z^T R^{-1}z$ is a weighted norm, the so-called Mahalanobis distance to zero. The maximum a posteriori (MAP) estimator for $x\\in\\mathbb{R}^n$ is then given by\\beq\\label{eq:3dvar}\\underset{x\\in\\mathbb{R}^n}{\\operatorname{argmin }} J(x), \\quad\\text{where}\\quad J(x) =  \\left(\\frac{1}{2}\\|y-h(x)\\|^2_{R^{-1}} +\\frac{1}{2}\\|x-x^B\\|_{B^{-1}}^2\\right),\\eeqwhich is a weighted nonlinear least squares problem. The minimisation problem in (\\ref{eq:3dvar}) is what is known as the three dimensional variational data assimilation problem (\\emph{3D-Var}). If $h:\\mathbb{R}^n\\rightarrow\\mathbb{R}^p$ is a linear operator, represented by a matrix $H\\in\\mathbb{R}^{p\\times n}$, the solution to (\\ref{eq:3dvar}) can be computed immediately using the (sometimes called Kalman gain) matrix $K$ given by\\[K = BH^T(HBH^T+R)^{-1},\\]or $K = (B^{-1}+H^TR^{-1}H)^{-1} H^T R^{-1}$, using the Sherman-Morrison-Woodbury formula \\cite{Golub2012}, and the minimum in \\eqref{eq:3dvar} is then given by\\[x^* = x^B+K(y-H(x^B)).\\] <!--TRUNCATED-->']"
"['\\section{Introduction}Data assimilation is the process of combining observational data with mathematical models to estimate the state of a dynamical system. It plays a crucial role in numerical weather prediction, oceanography, and other geosciences where accurate state estimation is essential. The fundamental challenge lies in optimally merging imperfect observations with imperfect model forecasts to produce an improved estimate of the system state.The mathematical foundations of data assimilation are rooted in estimation theory, control theory, and numerical linear algebra. Two main approaches dominate the field: variational methods (particularly 4D-Var) and sequential methods (notably the Kalman filter and its variants). Both approaches lead to large-scale optimization problems or linear algebra computations that require efficient numerical solutions.Recent advances in numerical linear algebra have enabled more efficient solutions to these data assimilation problems, particularly for high-dimensional systems. Key challenges include handling the curse of dimensionality, developing preconditioners for iterative methods, and exploiting low-rank structures in covariance matrices. The increasing availability of computational resources and novel algorithms has opened new possibilities for more accurate and efficient data assimilation systems.This paper focuses on the interplay between numerical linear algebra techniques and data assimilation methodologies. We survey recent developments in solving the large-scale linear algebra problems arising in both variational and sequential data assimilation, with particular attention to <!--TRUNCATED-->']",,,"['\\section{Basic Concepts of Data Assimilation}The fundamental problem in data assimilation can be formulated as finding the best estimate of the system state given observations and a dynamical model. The mathematical framework typically involves minimizing a cost function that measures the misfit between the model state and observations, weighted by their respective uncertainties. For linear systems with Gaussian errors, this leads to the well-known Kalman filter equations for sequential estimation or the variational (3D-Var/4D-Var) approach for batch processing.The Kalman filter provides recursive formulas for state estimation and error covariance propagation. In its basic form, it requires the propagation of full covariance matrices, which becomes computationally infeasible for high-dimensional systems. Variational methods, on the other hand, solve an optimization problem where the cost function incorporates both background and observation terms. The 4D-Var extension includes temporal information by considering observations distributed over a time window.Both approaches require solving large-scale linear algebra problems - either Riccati equations for the Kalman filter or linear systems arising from the optimality conditions in variational methods. The dimensionality of these problems often exceeds millions or even billions in operational weather forecasting systems, making efficient numerical solutions crucial. Key matrices like background error covariance and observation operators typically exhibit special structures that can be exploited for computational efficiency.']"
,"['\\section{The Kalman filter and low-rank approximations}\\label{sec:kfred}The Kalman filter is impractical for large dimensional systems. It requires the storage and evolution of large covariance matrices $P_i^{A/F}$, which are both not feasible for very high dimensional systems. The propagation of the error covariance requires a number of integrations of the forward model equal to the dimension of the system. Moreover matrix inversion within the Kalman gain computation is expensive. Hence a range of approximate Kalman filters have been developed for large systems, either by using a simplified or reduced order model to propagate the covariance matrices or by using a reduced state space or error space. Many of the approaches rely on low-rank approximations of the error covariance matrices.\\paragraph{Reduced-rank Kalman filters}The singular evolutive extended (SEEK) Kalman filter algorithm is one of the best known reduced rank square root (RRSQRT) filters. It is assumed that the covariance matrices $P$ have low-rank form $P = SS^T$, where $S\\in\\mathbb{R}^{n\\times r}$ with $r\\ll n$. The Kalman filter equations are rewritten using the matrices $S_i^F$ and $S_i^A$, the low rank approximations of the forecast error and analysis error covariance matrix. The equation for the Kalman gain becomes\\[K_i = S_i^F (H_i S_i^F)^T (H_iS_i^F (H_iS_i^F)^T + R_i)^{-1},\\]or using the Sherman-Morrison Woodbury identity. The analysis increment is a linear combination of the columns of $S_i^F$. For the forecast step, the propagation of the error covariance matrix is done via $P_{i+1}^F = \\tilde{S}_{i+1}^F (\\tilde{S}_{i+1}^F)^T + Q_i$.\\paragraph{Ensemble Kalman filter}The ensemble Kalman filter (EnKF) is a Monte Carlo implementation of the Bayesian update. There are variants including the stochastic ensemble Kalman filter (EnKF) and the ensemble transform Kalman filter (ETKF). The forecast error covariance is estimated using the empirical covariance from ensemble members. Each ensemble member is updated using the Kalman gain to obtain a posterior ensemble. The ETKF performs all matrix operations in the lower dimensional ensemble subspace.\\paragraph{Iterative solvers within the Kalman filter}A variational form of the Kalman filter can be formulated where state estimate and posterior covariance are given by minimum and inverse Hessian of a quadratic cost function. The preconditioned CG method leads to a conjugate gradient ensemble Kalman filter. Low-rank approximations to covariance matrices can be obtained by exploiting the connection between conjugate gradient and Lanczos iterations. Limited memory BFGS within the Kalman filter estimate has also been investigated. <!--TRUNCATED-->']",,"['\\paragraph{Reduced-rank Kalman filters}The singular evolutive extended (SEEK) Kalman filter algorithm is one of the best known reduced rank square root (RRSQRT) filters. It is assumed that the covariance matrices $P$ have low-rank form $P = SS^T$, where $S\\in\\mathbb{R}^{n\\times r}$ with $r\\ll n$. The Kalman filter equations are rewritten using the matrices $S_i^F$ and $S_i^A$, the low rank approximations of the forecast error and analysis error covariance matrix. The equation for the Kalman gain becomes\\[K_i = S_i^F (H_i S_i^F)^T (H_iS_i^F (H_iS_i^F)^T + R_i)^{-1},\\]or using the Sherman-Morrison Woodbury identity. The analysis increment is a linear combination of the columns of $S_i^F$. For the forecast step, the propagation of the error covariance matrix is done via $P_{i+1}^F = \\tilde{S}_{i+1}^F (\\tilde{S}_{i+1}^F)^T + Q_i$. <!--TRUNCATED-->']"
"['\\section{Introduction}Variational data assimilation leads to large PDE-constrained optimisation problems. We will first concentrate on 4D-Var and later discuss weak constraint 4D-Var.  Since the full nonlinear minimisation is difficult to solve, special algorithms for optimisation and linear algebra are required. In geoscience applications an incremental approach was proposed, which is merely a Gauss-Newton method for nonlinear least squares problems. Incremental variational data assimilation approximates the solution to the nonlinear optimisation problem by a sequence of minimisations of quadratic cost functions, obtained by linearising both the model and observation operators.  The quadratic cost function within the Gauss-Newton approach can be minimised using conjugate gradient methods. As the problem is usually very large, the inner CG iteration is often applied to a lower resolution system with preconditioning. Two-level preconditioning is commonly used, first employing a change of variables based on the background error covariance matrix, followed by spectral preconditioning using dominant Hessian eigenvalues obtained from the Lanczos procedure.  For problems where the observation space dimension is much smaller than the state space, a dual formulation (PSAS) can be more efficient by solving a smaller system in observation space. Weak constraint 4D-Var requires minimisation over all state variables and leads to a large saddle point system that needs to be solved at each iteration.  Hybrid methods combining variational and ensemble approaches have gained popularity as they avoid adjoint requirements. Inexact methods are needed when gradients or Jacobians are not available exactly. Parallel-in-time approaches and Levenberg-Marquardt methods with regularization have been proposed to address convergence issues in large-scale problems. <!--TRUNCATED-->']","['\\section{Solutions to the optimisation problem arising in variational data assimilation}Variational data assimilation leads to large PDE-constrained optimisation problems. We first concentrate on 4D-Var and later discuss weak constraint 4D-Var.  The incremental approach is a Gauss-Newton method for nonlinear least squares problems. The solution is approximated by a sequence of quadratic cost function minimisations, obtained by linearising model and observation operators. Each iteration computes an increment by solving a large linear system through conjugate gradient methods, with forward/adjoint model evaluations for cost function and gradient computations.  For the inner CG iteration, two-level preconditioning is applied: first a change of variables using the background error covariance Cholesky factor, then spectral preconditioning using dominant Hessian eigenvalues from the Lanczos procedure. When observation space dimension is much smaller than state space, the dual formulation (PSAS) solves a smaller system in observation space via the Sherman-Morrison-Woodbury formula.  Weak constraint 4D-Var requires full state variable minimisation, leading to a saddle point system solved at each iteration. The system is structured with block diagonal covariance matrices and all-at-once model/observation operators over the assimilation window.  Hybrid methods combine variational approaches with ensemble Kalman smoothers to avoid adjoint requirements. For cases with inexact gradients/Jacobians, Levenberg-Marquardt methods with regularization are used. Parallel-in-time approaches and low-rank preconditioners exploiting saddle point structure have been developed for large-scale problems. <!--TRUNCATED-->']",,"['\\paragraph{The inner iteration and preconditioning}The quadratic cost function within the Gauss-Newton (incremental) approach can be minimised using a conjugate gradient (CG) method. As the problem is usually very large, in practice, the inner loop, the CG iteration, is applied to a system with lower spatial resolution and with a preconditioner. One of the earliest approaches is the multilevel setting where the quadratic cost function is replaced by a lower resolution model, in order to obtain a multi-resolution scheme.In addition, two-level preconditioning is usually applied within 4D-Var. The first level preconditioning employs a linear change in variables using the Cholesky factor of the background error covariance matrix. The Hessian of the cost function then becomes $I+L^TH^TR^{-1}HL$, which ensures that the smallest eigenvalue of the transformed Hessian is one. At a second preconditioning level the spectrum of the (symmetric) Hessian is used, obtaining dominant eigenvalues and corresponding eigenvectors using the Lanczos method. After $k$ steps of the CG algorithm, approximate leading eigenpairs of the Hessian are available to construct limited memory preconditioners.The limited memory preconditioners (LMP) are often referred to as spectral LMP, as they use the spectral information of the Hessian approximation. More general versions of LMP were investigated where the authors show the equivalence of a certain reduced version of 4D-Var and the SEEK filter, and use this equivalence to accelerate the convergence of the Gauss-Newton method. <!--TRUNCATED-->']"
,,,
,,,"[""A \\EM{Gelfand--Tsetlin pattern} is a (finite) triangular array of natural numbers$$\\begin{matrix} &  &  &  & u_{1,1} &  &  &  & \\\\ &  &  & u_{2,1} & & u_{2,2} &  &  & \\\\ &  & u_{3,1} & & u_{3,2} & & u_{3,3} &  & \\\\ & u_{4,1} & & u_{4,2} & & u_{4,3} & & u_{4,4} & \\\\\\dots & & \\dots & & \\dots & & \\dots & & \\dots\\\\\\end{matrix}$$where the entries in row $i-1$ are in the following sense``interlaced'' with the entries in row $i$:\\begin{equation}\\label{eq:interlacing}u_{i,1}\\leq u_{i-1,1}<u_{i,2}\\leq u_{i-1,2} < u_{i,3}\\leq\\cdots u_{i-1,i-1}<u_{i,i}.\\end{equation}<!--TRUNCATED-->""]"
"['\\section{Introduction}Let $\\frak g $ be a semi-simple Lie algebra and let $\\hat {\\frak g} = \\frak g [ t , t^{-1} ] \\oplus {\\Bbb C} K $ be the corresponding affine Kac-Moody algebra. It is well-known that a highest weight module over $ \\hat {\\frak g}$ of level $k$ has the structure of a module over the level $k$ vertex algebra $V^k ( {\\frak g})$ associated to $\\hat {\\frak g}$.The purpose of this note is to show that certain level $k$ representations of $\\hat {\\frak g}$ that are not in category ${\\cal O}$, e.g., representations constructed in \\cite{DZ}, has the structure of what we will call a linear topological module over vertex algebra $ V^k ( {\\frak g})$.If $A$ is a ring, a linear topological module over $A$ is a left $A$-module $M$ with a topology that is invariant under translations and admits a fundamental system of neighborhoods of $0$ that consists of submodules of $M$ \\cite{B}. A linear topological module over a vertex algebra $V$ is a complete linear topological module over ${\\Bbb C}$ equipped with an action of vertex operators $ Y ( a , z ) = \\sum a_{(n)} z^{-n-1}$, $a \\in V$.We expect that the vertex algebras will also play a role in the study of representations of ${\\hat {\\frak g}}$ that are not necessarily in category ${\\cal O}$. As the algebra $ U (\\hat {\\frak g } )$ is not large enough as a candidate for invariant differential operators on the loop groups, we need to extend it to include the coefficients of vertex operators from $V^k ( {\\frak g})$ so that the extended algebra still acts on useful classes of ${\\hat {\\frak g}}$-modules such as ones that will appear in the study of automorphic forms on loop groups. We also hope that the notion of a linear topological module over vertex algebras will be useful to extend the work \\cite{BKP} on loop groups over $p$-adic fields to loop groups over ${\\Bbb R}$.<!--TRUNCATED-->']",,,"[""\\section{Definitions}Recall that a linear topology on a vector space $M$ over ${\\Bbb C}$ is a topology on $M$ that is invariant under translations and admits a fundamental system of neighborhoods of $0$ that consists of subspaces of $M$ (e.g. \\cite{B}). If such a topology is given on $M$, $M$ is called a linear topological vector space. A sequence $\\{v_n\\} \\subset V$ is called a Cauchy sequence if for any neighborhood $U$ of $0$, there exists an integer $N$ with the property that\\[ v_i-v_j \\in U,\\;\\;\\text{ for all } i,j\\geq N. \\]We call $M$ complete if every Cauchy sequence converges in $ V$.Here is a simple way to construct a linear topological space. Suppose $U_1 \\supset U_2 \\supset \\dots $ is a sequence of subspaces in a vector space $M_0$ with $\\cap_{i=1}^\\infty U_i = \\{ 0 \\} $. The sequence $U_i$ defines a linear topology on $M_0$ such that $ U_i $'s forms a fundamental system of neighborhoods of $0$. This topology on $M_0$ is not complete in general. The completion $M$ of $M_0$ consists of Cauchy series $v_1 + v_2 + \\dots $, i.e., for every $U_i $, there is $N_i$ such that $ v_j \\in U_i $ for all $ j > N_i $. $ M$ has a fundamental system $\\bar U_i $ of neighborhoods of $0$, where $\\bar U_i$ consists of the equivalence classes of Cauchy series $\\sum v_i$ in $U_i$. For example, $ M_0 = {\\Bbb C} [ z , z^{-1} ] $, $ U_i = {\\Bbb C} [ z ] z^i $, the completion $M$ of $M_0$ with respect to $\\{ U_i\\}$ is $ {\\Bbb C} (( z )) $ with a fundamental system of neighborhoods $ {\\Bbb C} [[ z ]] z^i $, $ i =1 , 2 , \\dots $.""]"
"[""\\section{Introduction}Let us consider the field of rational functions $\\C(x)$, with the derivation $\\partial=\\frac{{\\rm d}}{{\\rm d}x}$, and a linear differential system $\\partial \\vec y=A\\vec y$, where $A$ is a square matrix of order $n$ with coefficients in~$\\C(x)$. One can attach to such an object an algebraic group, called the differential Galois group, whose geometric properties encode the algebraic properties of the solutions of the linear differential system. The problem of calculating explicitly the differential Galois group of $\\partial \\vec y=A\\vec y$ is old and still difficult.Instead of calculating directly the differential Galois group of $\\partial \\vec y=A\\vec y$, one can try to study, or calculate, the Lie algebra of the differential Galois group, called {\\em Galois--Lie algebra} in what follows. The Galois--Lie algebra already contains a significant part of the information. Kolchin and Kovacic have proved that one can transform $\\partial \\vec y=A\\vec y$ into an equivalent system $\\partial \\vec y=B\\vec y$ defined over a finite extension $k$ of $\\C(x)$, such that $B$ belongs to the set of $k$-rational points of the Galois--Lie algebra. The linear differential system $\\partial \\vec y=B\\vec y$ is called {\\em a reduced form} of $\\partial \\vec y=A\\vec y$.{\\bf Main results.}In the present work, we prove three results on reduced forms and their relation to the Galois--Lie algebra. First of all, we prove that a system is in reduced form if and only if any differential module in a construction admits a constant basis. This extends the criterion for reduced form from previous works which concerned only invariant lines.Our second contribution gives an effective characterization of a gauge transformation that transforms a linear differential system into a reduced one. It may be considered as an effective counterpart of our first theorem, based on the ``local'' data of the semi-invariants. Compared to the original result by Kolchin and Kovacic on the existence of reduced forms, we have to perform an algebraic extension of the base field $k$, which may not be optimal, in order to gain the effectivity.Finally, we prove a theorem on the Lie algebra of the intrinsic Galois group, introduced in previous work, where the idea of focusing on the Galois--Lie algebra rather than on the differential Galois group itself is pursued. Indeed, Katz introduces another Galois group for the linear differential system $\\partial \\vec y=A\\vec y$, called the generic or the intrinsic Galois group. Then he considers the Lie algebra of such a group, for which he gives a conjectural description equivalent to a well-known conjecture of Grothendieck on the algebraicity of the solutions of a linear differential system. We will call it {\\em the Katz algebra}. In the last section, we gather material from previous works and show how our criteria for reduced forms, combined with standard Tannakian tools, clarify the structure of the Katz algebra. Namely, for a reductive group, our theorem shows that the Katz algebra is a $k$-form of the Galois--Lie algebra. <!--TRUNCATED-->""]",,,"['\\section{Notation and definitions}\\label{section2}We consider a characteristic zero differential field $(k,\\partial)$, that is a characteristic zero field $k$ with a derivation $\\partial\\colon k\\to k$, such that $\\partial(a+b)=\\partial(a)+\\partial(b)$ and$\\partial(ab)=\\partial(a)b+a\\partial(b)$, for any $a,b\\in k$. We suppose that the subfield of constants $C:=k^\\partial=\\{f\\in k\\colon \\partial f=0\\}$ is algebraically closed.\\subsection{Differential modules}\\label{section2.1}A differential module $\\cM=(M,\\nabla)$ over $k$ (of rank $n$) is a $k$-vector space $M$ of dimension $n$, with a $C$-linear map $\\na\\colon M\\to M$ such that $\\na(fm)=\\partial(f)m+f\\nabla(m)$ for any $f\\in k$ and any $m\\in M$. For a detailed exposition on differential modules, see \\cite[Section~2.2]{vdPutSingerDifferential}. We denote by~$M^\\na$ or~$\\ker \\na$ the set of {\\em horizontal elements} of~$\\cM$, that is elements $m \\in M$ satisfying $\\na(m) =0$. This is a $C$-vector space of dimension at most~$n$.{\\bf Main properties of differential modules.}Given a basis (denoted as a row) $\\ul e :=(e_1, \\dots, e_n)$ of $M$ over $k$,the action of $\\nabla$ with respect to the basis $\\ul e$is described by a square matrix $A\\in M_n(k)$ as follows\\begin{gather*}\\na\\ul e=-\\ul e A.\\end{gather*}For any $\\vec y\\in k^n$ such that $\\ul e \\vec y$ represents an element of $M$, we have$\\na(\\ul e \\vec y)=\\ul e(\\partial \\vec y-A \\vec y)$.{Thus horizontal elements of $\\cM$ correspond to solutions over $k$ of the differential system}\\begin{gather*}[A]\\colon \\ \\partial \\vec y=A\\vec y.\\end{gather*}We say that $[A]\\colon \\partial \\vec y=A\\vec y$ is the linear differential system associated to $\\cM$ with respect to the basis~$\\ul e$. <!--TRUNCATED-->']"
,,,"[""Each space in $\\Constr(V)$ which is invariant under $\\mathfrak{g}$ is in (Tannakian) correspondence with a~submodule of $\\Constr(M)$, invariant under $\\nabla$ and hence under $\\gK$. A reciprocal property would be to characterize, among all subspaces in any~\\Constr(M) that are stable under~$\\gK$, which ones are stable under $\\nabla$ (and hence are in Tannakian correspondence with a $\\mathfrak{g}$-module in $\\Constr(V)$. Proposition~\\ref{stable-sous-gk} shows that this it is possible to do that at the cost of extending scalars. When we study the subspaces of $\\Constr(M)\\otimes_k k'$ which are invariant under $\\gK\\otimes_k k'$, the ones which are invariant under $\\nabla$ (and hence in tannakian correspondence with a $\\mathfrak{g}$-module) are exactly those which admit a constant basis.""]"
"['\\section{Introduction}\\label{sec:int}Let $X$ be a projective variety over an algebraically closed field $k$. The automorphism group functor of $X$, which assigns to any $k$-scheme $S$ the group of relative automorphisms $\\Aut_S(X \\times S)$, is represented by a $k$-group scheme $\\Aut_X$, locally of finite type. Thus, the reduced subscheme of $\\Aut_X$ is equipped with a structure of a smooth $k$-group scheme $\\Aut(X)$. But $\\Aut(X)$ is not necessarily an algebraic group; equivalently, it may have infinitely many components, e.g. when $X$ is a product of two isogenous elliptic curves. Still, the automorphism group is known to be a linear algebraic group for some interesting classes of varieties including smooth Fano varieties, complex almost homogeneous manifolds and normal almost homogeneous varieties in arbitrary characteristics.\\begin{theorem}\\label{thm:rel}Let $X$ be a normal projective variety, $G \\subset \\Aut(X)$ a linear algebraic subgroup, $K := k(X)^G$ the field of $G$-invariant rational functions on $X$, and $\\Aut_K(X)$ the subgroup of $\\Aut(X)$ fixing $K$ pointwise. Then $\\Aut_K(X)$ is a linear algebraic group.\\end{theorem}By a theorem of Rosenlicht, the rational functions in $K$ separate the $G$-orbit closures of general points of $X$. Thus, $\\Aut_K(X)$ is the largest subgroup of $\\Aut(X)$ having the same general orbit closures as $G$. Also, note that $K = k$ if and only if $X$ is almost homogeneous; then $\\Aut_K(X)$ is just the full automorphism group. \\begin{theorem}\\label{thm:full}Let $X$ be a normal projective variety, $G \\subset \\Aut(X)$ a linear algebraic subgroup, and $K := k(X)^G$. If $K$ has transcendence degree $1$ over $k$, then $\\Aut(X)$ is an algebraic group. If in addition $G$ is connected and $K$ is not the function field of an elliptic curve, then $\\Aut(X)$ is linear.\\end{theorem}\\begin{corollary}\\label{cor:surface}Let $X$ be a normal projective surface having a non-trivial action of a connected linear algebraic group. Then $\\Aut(X)$ is an algebraic group. Moreover, $\\Aut(X)$ is linear unless $X$ is birationally equivalent to $Y \\times \\bP^1$ for some elliptic curve $Y$.\\end{corollary}<!--TRUNCATED-->']",,,"['\\section{Proof of Theorem \\ref{thm:rel}}\\label{sec:rel}It proceeds via a sequence of reduction steps and lemmas.We begin with two easy and useful observations:\\begin{lemma}\\label{lem:closed}\\begin{enumerate}\\item[{\\rm (i)}] $\\Aut_K(X)$ is a closed subgroup of $\\Aut(X)$.\\item[{\\rm (ii)}] If $G$ is connected, then $K$ is algebraically closed in $k(X)$.\\end{enumerate}\\end{lemma}\\begin{proof}(i) It suffices to show that the stabilizer $\\Aut_f(X)$ is closed in $\\Aut(X)$ for any non-zero $f \\in k(X)$. Let\\[ \\Aut_{(f)}(X) := \\{ g \\in \\Aut(X) ~\\vert~ g^*(f) = \\lambda(g) f \\text{ for some } \\lambda(g) \\in k^* \\}. \\]Then $\\Aut_f(X) \\subset \\Aut_{(f)}(X) \\subset \\Aut(X)$. Denote by $D_0$ (resp.~$D_{\\infty}$) the scheme of zeroes (resp.~poles) of $f$, and by $\\Aut(X,D_0,D_{\\infty}) \\subset \\Aut(X)$ the common stabilizer of these subschemes of $X$. We claim that $\\Aut_{(f)}(X) = \\Aut(X,D_0,D_{\\infty})$. Indeed, the inclusion $\\Aut_{(f)}(X) \\subset \\Aut(X,D_0,D_{\\infty})$ is obvious, and the opposite inclusion follows from the fact that every $g \\in \\Aut(X,D_0,D_{\\infty})$ satisfies $\\div(g^*(f)) = \\div(f)$.By the claim, $\\Aut_{(f)}(X)$ is closed in $\\Aut(X)$. Moreover, $\\Aut_{(f)}(X)$ stabilizes the open subset $U := X \\setminus (D_0 \\cup D_{\\infty}) \\subset X$, and $\\Aut_f(X)$ is the stabilizer of $f \\in \\cO(U)$. So $\\Aut_f(X)$ is closed in $\\Aut_{(f)}(X)$.(ii) Let $f \\in k(X)$ be algebraic over $K$. Then the stabilizer of $f$ in $G$ is a closed subgroup of finite index, and hence is the whole $G$. So $f \\in K$.\\end{proof}']"
,,,"['Since $G$ is a normal subgroup of $\\Aut(X)$, the action of $\\Aut(X)$ on $k(X)$ stabilizes $K$.This yields an exact sequence\\[ 1 \\longrightarrow \\Aut_K(X) \\longrightarrow \\Aut(X) \\longrightarrow \\Aut_k(K) = \\Aut(Y). \\]Also, recall from Lemma \\ref{lem:closed} that$\\Aut_K(X)$ is closed in $\\Aut(X)$. <!--TRUNCATED-->']"
"['\\section{Introduction}  The Galois group of a polynomial over a field is a finite group. The inverse problem in the Galois theory of polynomials asks to determine, for a given field, which finite groups occur. For example, for the field $\\C(x)$ of rational functions over $\\C$, it is known that every finite group occurs.The Galois group of a linear differential equation over a differential field is a linear algebraic group. The inverse problem in the Galois theory of linear differential equations asks to determine, for a given differential field, which linear algebraic groups occur. For example, for the field $\\C(x)$ with derivation $\\frac{d}{dx}$, it is known that every linear algebraic group occurs.A difference-differential field is a field equipped with two commuting operators, a derivation and an endomorphism, usually denoted with $\\s$. The $\\s$-Galois group of a linear differential equation over a difference-differential field is a linear difference algebraic group. The problem we are concerned with in this article is the inverse problem in the $\\s$-Galois theory of linear differential equations. It asks to determine, for a given difference-differential field, which difference algebraic groups occur. We are mainly interested in the difference-differential field $\\C(x)$ with derivation $\\frac{d}{dx}$ and endomorphism $\\s\\colon \\C(x)\\to \\C(x),\\ f(x)\\mapsto f(x+1)$.As we will show, not every difference algebraic group occurs as a $\\s$-Galois group of a linear differential equation over $\\C(x)$. In fact, constant subgroups of unipotent linear algebraic groups do not occur and moreover, we isolate two properties that any $\\s$-Galois group over $\\C(x)$ must have. On the positive side, our main result is the following: Every linear algebraic group, considered as a difference algebraic group, occurs as a $\\s$-Galois group over $\\C(x)$. Our main tool for the proof is patching. In fact, we establish a general patching result for $\\s$-Picard-Vessiot rings over difference-differential fields that we deem of independent interest. This patching result is analogous to known patching results in the Galois theories of linear differential equations and parameterized linear differential equations that turned out to be very useful in the study of the corresponding inverse problems. We therefore expect our patching theorem to have further applications in the study of the inverse problem in the $\\s$-Galois theory of linear differential equations.  To put our results into perspective, let us review the state of the art of the inverse problem in the various Galois theories. The three most relevant Galois theories for us are the following: 1. The Galois theory of linear differential equations, where the Galois groups are linear algebraic groups.2. The Galois theory of parameterized linear differential equations, where the Galois groups are differential algebraic groups.3. The $\\s$-Galois theory of linear differential equations, where the Galois groups are difference algebraic groups.  The inverse problem in this Galois theory is wide open. It appears that beyond some initial observations nothing is known. In this paper, for the first time, a significant class of difference algebraic groups is shown to occur as Galois groups.  The direct problem in the above Galois theories is to compute the Galois group of a given (parameterized) linear differential equation. We note that progress in the inverse problem can be helpful for the direct problem. For example, if it is already known that the Galois group of a given (parameterized) differential equation is non-trivial and contained in a certain group $G$, the information, that no non-trivial subgroup of $G$ is a Galois group would already imply that the searched for Galois group equals $G$.  We conclude the introduction with an overview of the article. In the first section we recall and establish some basic preparatory definitions and results concerning difference algebraic groups and the $\\s$-Galois theory of linear differential equations. The main patching result is established in Section 3. It follows from this result that to realize all linear algebraic groups (considered as difference algebraic groups) as Galois groups it suffices to realize certain building blocks, namely, the multiplicative group, the additive group and finite cyclic groups. These building blocks are then dealt with in Section 4. Our main result, that all']",,,"['\\section{Basics on difference Galois theory}In this section we recall the necessary definitions concerning difference algebraic groups and $\\s$-Picard-Vessiot theory. We also establish some results of a preparatory nature.All rings are assume to be commutative and unital.\\subsection{Difference algebraic groups}We begin by recalling some basic notions from difference algebra. Standard references for difference algebra are \\cite{Cohn:difference} and \\cite{Levin:difference}. For more background on difference algebraic groups see \\cite[Appendix A]{DiVizioHardouinWibmer:DifferenceGaloisTheoryOfLinearDifferentialEquations} or \\cite{Wibmer:FinitenessProperties}.A \\emph{difference ring}, or \\emph{$\\s$-ring} for short, is a ring $R$ together with an endomorphism $\\s\\colon R\\to R$. A morphism $\\psi\\colon R\\to S$ of $\\s$-rings is a morphism of rings such that$$\\xymatrix{R \\ar^\\psi[r] \\ar_\\s[d] & S \\ar^\\s[d] \\\\R \\ar^\\psi[r] & S\t}$$commutes. A $\\s$-ring is a \\emph{$\\s$-field} if the underlying ring is a field.Let $k$ be a $\\s$-ring. A \\emph{\\ks-algebra} is a $\\s$-ring $R$ together with a morphism $k\\to R$ of $\\s$-rings. A morphism of \\ks-algebras is a morphism of $k$-algebras that is also a morphism of $\\s$-rings.For a subset $B$ of $R$, the smallest \\ks-subalgebra of $R$ that contains $B$ is denoted by $k\\{B\\}$. Note that $k\\{B\\}$ is generated by $B,\\s(B),\\s^2(B),\\ldots$ as a $k$-algebra. If $R=k\\{B\\}$ for a finite subset $B$ of $R$ then $R$ is called \\emph{finitely $\\s$-generated}. If $R$ and $S$ are \\ks-algebras, then $R\\otimes_k S$ is a \\ks-algebra with $\\s$ defined by $\\s(r\\otimes s)=\\s(r)\\otimes \\s(s)$ for $r\\in R$ and $s\\in S$.An ideal $\\ida$ of a $\\s$-ring $R$ is a \\emph{$\\s$-ideal} if $\\s(\\ida)\\subseteq \\ida$. In this case $R/\\ida$ is naturally a $\\s$-ring. The \\emph{$\\s$-polynomial ring} $k\\{y\\}=k\\{y_1,\\ldots,y_n\\}$ over $k$ in the $\\s$-variables $y_1,\\ldots,y_n$ is the polynomial ring over $k$ in the variables $\\s^i(y_j)$ ($1\\leq j\\leq n$, $0\\leq i$) with $\\s\\colon k\\{y\\}\\to k\\{y\\}$ extending $\\s\\colon k\\to k$ as suggested by the naming of the variables. If $f\\in k\\{y_1,\\ldots,y_n\\}$ is a $\\s$-polynomial and $x=(x_1,\\ldots,x_n)\\in R^n$ for some \\ks-algebra $R$, then the element $f(x)\\in R$ is obtained from $f$ by substituting $\\s^i(y_j)$ with $\\s^i(x_j)$.For a \\ks-algebra $R$ and $F\\subseteq k\\{y_1,\\ldots,y_n\\}$ we set$\\V_R(F)=\\{x\\in R^n\\ |\\  \\text{for all }f\\in F: f(x)=0 \\}.$ Note that $R\\rightsquigarrow \\V_R(F)$ is naturally a functor from the category of \\ks-algebras to the category of sets.Let $k$ be a $\\s$-field. A \\emph{$\\s$-variety $X$ over $k$} is a functor $R\\rightsquigarrow X(R)$ from the category of \\ks-algebras to the category of sets that']"
,,,"['\\begin{prop}\\label{prop: 123}\tLet $m\\in \\N$ and let $\\H$ be either $\\Gm$, $\\Ga$ or a finite cyclic group. Then there exists a $\\s$-Picard-Vessiot ring $R$ over $F=\\C(x)$ with $\\s$-Galois group isomorphic to $[\\s]_\\C\\H$ such that $R\\subseteq L(m)$ as an $F$-$\\ds$-subalgebra.\\end{prop}\\begin{proof}\tWe begin with $\\H=\\Gm$. Define $a=-(x-mi)^2 \\in F$. Then $y=\\exp(\\frac{1}{x-mi})$ solves the differential equation $\\de(y)=ay$ and $y$ is holomorphic on $\\mathbb P^1_\\C\\backslash\\{mi\\}\\supseteq V_{1,m}$. Hence $y \\in F_{V_{1,m}}\\subseteq L(m)$. Define $R=F\\{y,y^{-1}\\}\\subseteq L(m)$. Then $R/F$ is a $\\s$-Picard-Vessiot ring for $\\de(y)=ay$ by Lemma \\ref{lemma: crit}, since $L(m)^\\de=\\C=F^\\de$. Using Proposition \\ref{prop: Gm} together with Lemma \\ref{lemma: 1}, we conclude that its $\\s$-Galois group is isomorphic to $[\\s]_\\C\\Gm$.\t\t\\medskip\t\tWe next treat the case $\\H=\\Ga$. Recall that the complex logarithm is a holomorphic function on $\\C\\backslash \\R_-$, where $\\R_-$ denotes the interval $(-\\infty,0]$. As $x\\mapsto \\frac{1}{x-mi}+1$ defines a holomorphic function $V_{1,m}\\to \\C\\backslash \\R_-$, we conclude that $y=\\log\\left(\\frac{1}{x-mi}+1 \\right)$ is holomorphic on $V_{1,m}$ and in particular, $y\\in F_{V_{1,m}}\\subseteq L(m)$. Also note that $\\de(y)$ is contained in $\\C(x)=F$. Hence $R=F\\{y\\}\\subseteq L(m)$ is a $\\s$-Picard-Vessiot ring over $F$ with $\\s$-Galois group isomorphic to $[\\s]_\\C\\Ga$ by Proposition \\ref{prop: Ga} together with Lemma \\ref{lemma: 2}. \t\t\\medskip\t\tFinally we treat the case that $\\H$ is a finite cyclic group. Let $d\\in \\N$ be the order of $\\H$. \tRecall that the complex $d$-th root $\\sqrt[d]{x}=\\exp(\\log(x)/d)$ is a holomorphic function on $\\C\\backslash \\R_-$, where $\\R_-$ denotes the interval $(-\\infty,0]$. As $x\\mapsto \\frac{1}{x-mi}+1$ defines a holomorphic function $V_{1,m}\\to \\C\\backslash \\R_-$, we conclude that $y=\\sqrt[d]{\\frac{1}{x-mi}+1}$ is holomorphic on $V_{1,m}$ and in particular, $y\\in F_{V_{1,m}}\\subseteq L(m)$. Hence $R=F\\{y,y^{-1}\\}\\subseteq L(m)$ is a $\\s$-Picard-Vessiot ring over $F$ with $\\s$-Galois group $[\\s]_\\C\\H$ by Proposition \\ref{prop: cyclic} together with Lemma \\ref{lemma: 3}. \\end{proof}']"
,,,"['Recall that every element of $\\C(x)$ has a unique partial fraction decomposition $$g(x)+\\sum_{j=1}^{r} \\frac{\\alpha_j}{x+\\beta_j}+\\sum_{j=1}^{r_2} \\frac{\\alpha_{2j}}{(x+\\beta_{2j})^2}+\\dots+\\sum_{j=1}^{r_m} \\frac{\\alpha_{mj}}{(x+\\beta_{mj})^m} $$with $g\\in \\C[x]$, $m,r,r_2,\\dots,r_m \\in \\N$ and $\\alpha_j,\\alpha_{ij},\\beta_j,\\beta_{ij} \\in \\C$. The term $\\sum_{j=1}^{r} \\frac{\\alpha_j}{x+\\beta_j}$ is called the \\textit{logarithmic part}. An element in $\\C(x)$ has an antiderivative inside $\\C(x)$ if and only if its logarithmic part vanishes.']"
,,,"['\\section{1. A new proof of Cayley-Hamilton Theorem}The main result concerning the eigen polynomial $f_1(x)=:{\\rm det}(xE-A)$ of a square matrix $A$ is the  Cayley-Hamilton theorem:\\begin{mthm} Let $A$ be a square matrix over a commutative ring $S$. Then the eigen polynomial $f_1(x)=:{\\rm det}(xE-A)$ of $A$ annihilates $A$, i.e., $f_1(A)=0$.\\end{mthm}A typical approach to this theorem is found in many text books, e.g.,  \\cite[Page 297]{Shi}, \\cite[Theorem 2.5.1]{2001Serre}, or \\cite[Theorem 6.4.1]{Guo}. See also \\cite[Theorem 2.4.3.2, Ex. 2.4 P3]{HB 2013} for other treatments without using Jordan theorem. <!--TRUNCATED-->']"
"['\\section{Introduction}For a complex number $\\tau$ from the upper complex half plane $\\mathbb{H}$, the theta functions are defined as follows;$$\\theta_2(\\tau)=2\\sum_{n=0}^{\\infty} q^{{(n+1/2)}^2} \\,,\\qquad \\theta_3(\\tau)=1+2\\sum_{n=1}^\\infty q^{n^2}\\,,  \\quad\\mbox{~~and~~}\\quad\\theta_4(\\tau)=1+2\\sum_{n=1}^\\infty (-1)^nq^{n^2}\\,, $$where $q=e^{i\\pi\\tau}$. For the sake of brevity we sometimes write $\\theta_i$ instead of $\\theta_i(\\tau)$, $i=2,3,4$.The motivation of this article comes from the following sources: In $2018$, C.Elsner and Y.Tachiya \\cite{elsner} proved that for distinct integers $\\ell, m$ and$n$, the functions $\\theta_3(\\ell\\tau), \\theta_3(m\\tau)$ and $\\theta_3(n\\tau)$ are {\\em algebraically\\/} dependent over $\\mathbb{Q}$. In 2019, C.\\,Elsner, F.\\,Luca and Y.\\,Tachiya \\cite{luca} proved the following: let $\\tau$ be any complex number with Im$(\\tau) >0$ such that $e^{i\\pi\\tau}$ is algebraic. Let $m,n\\geq 1$ be distinct positive integers. Then the numbers $\\theta_3(m\\tau)$ and $\\theta_3(n\\tau)$ are {\\em algebraically} independent over $\\mathbb{Q}$.  Naturally the following two questions arise.\\noindent{\\bf Question 1.~}Let $m\\geq 2$ and let $a_1, a_2, \\ldots, a_m$ be distinct positive integers. Are the functions $$\\theta_3(a_1 \\tau),\\,\\theta_3(a_2\\tau),\\,\\ldots,\\,\\theta_3(a_m\\tau)$${\\em linearly\\/} independent over  $\\mathbb{C(\\tau)}$? \\noindent{\\bf Question 2.~}What are the values of $\\tau$ and $\\alpha_0,\\alpha_1,\\alpha_2$ such that the linear form  $$L \\,:=\\, \\alpha_0\\theta_3(\\tau) + \\alpha_1\\theta_3(m\\tau) + \\alpha_2\\theta_3(n\\tau)$$ does not vanish?In this article, we give the complete answer to the Question\\,1 and answer the Question\\,2 in the following way: We consider linear forms withintegers $n>m>1$, certain numbers $\\tau \\in \\mathbb{H}$, and algebraic numbers $\\alpha_0,\\alpha_1, \\alpha_2$, and give conditions on $\\alpha_0,\\alpha_1, \\alpha_2$ such that $L\\not= 0$. <!--TRUNCATED-->']",,"['\\section{Concluding remarks.}  In the case when $\\tau\\in\\mathbb{H}$  such that $e^{i\\pi\\tau}$ is algebraic, the number $\\theta_3(\\tau)$ is transcendental due to the algebraicindependence of the values $\\theta_3(m\\tau)$  and $\\theta_3(n\\tau)$ for distinct positive integers $m, n$. By our Theorem 2, we know that at least two of the numbers among $\\theta_3(\\tau), \\theta_3(m\\tau)$ and $\\theta_3(n\\tau)$ are transcendental for any $\\tau\\in \\mathbb{H}$ such that either $\\tau$ is algebraic of degree $\\geq 3$ or $e^{i\\pi \\tau}$ is algebraic.  In this context, it is interesting to consider the following problem:... <!--TRUNCATED-->']","['\\section{The results}\\label{Sec2}\\subsection{The linear independence over ${\\C}(\\tau)$ of the functions $\\theta_3(a_1 \\tau),\\,\\ldots,\\,\\theta_3(a_m\\tau)$ in $\\tau$} \\[\\]We begin with the following result on the linear independence over ${\\C}(\\tau)$ of Jacobi-theta constants.\\begin{theorem}\\label{maintheorem}{\\em (i)\\/}\\,\\,Let $a_1, a_2, \\ldots, a_m$ be distinct positive rational numbers. Then the $m$ functions $$\\theta_3(a_1 \\tau), \\quad \\theta_3(a_2\\tau),\\ldots, \\theta_3(a_m\\tau)$$ in $\\tau \\in {\\H}$ are linearly independent over $\\mathbb{C(\\tau)}$. \\\\{\\em (ii)}\\/\\,\\,Let $\\alpha_1, \\alpha_2, \\ldots, \\alpha_m$ be distinct positive real numbers. Then the $m$ functions $$\\theta_3(\\alpha_1 \\tau), \\quad \\theta_3(\\alpha_2\\tau),\\ldots, \\theta_3(\\alpha_m\\tau)$$ are linearly independent over ${\\C}$.\\end{theorem}']"
,,,"[""\\noindent{\\bf Problem 1.~} Let $\\tau$ and $m, n$ be  as in Theorem 2.  Then $1,\\theta_3(\\tau), \\theta_3(m\\tau)$ and $\\theta_3(n\\tau)$ are $\\overline{\\mathbb{Q}}$- linearlyindependent.\\smallskipAs a consequence of this problem, one can conclude the transcendence of $\\theta_3(\\tau)$ for algebraic $\\tau$ of degree $\\geq 3$, which is not known in  this case.\\bigskipIn this paper we considered linear forms in three  values of theta constant $\\theta_3$. It is natural to consider the following more general problem: \\bigskip\\noindent{\\bf Problem 2.~}  Let $a_1, a_2,\\ldots, a_m$ be distinct positive integers.  Let $\\alpha_0,\\ldots,\\alpha_m$ be non-zero algebraic numbers. Under what values of $\\tau\\in\\mathbb{H}$, the linear form$$L:=\\alpha_1 \\theta_3(a_1\\tau)+\\cdots+\\alpha_m\\theta_3(a_m \\tau)$$does not vanish.In the case  $\\alpha_i$'s are rational and $\\tau=\\frac{i\\log b}{2\\pi}$, where  $b\\geq 2$ is an integer, $L\\neq 0$ by the result from  \\cite{veekesh} and \\cite{tachiya}.<!--TRUNCATED-->""]"
"['\\section{Introduction}We consider large parameter-dependent systems of equations\\begin{equation} \\label{eq:initialproblem}\\bA(\\mu)\\bu(\\mu)= \\bb(\\mu),~\\mu \\in \\mathcal{P},\\end{equation}where $\\bu(\\mu)$ is a solution vector, $\\bA(\\mu)$ is a parameter-dependent {matrix}, $\\bb(\\mu)$ is a parameter-dependent right hand side and $\\mathcal{P}$ is a parameter set. Parameter-dependent problems are considered for many purposes such as {design,} control, optimization, uncertainty quantification or inverse problems.{Solving~\\textup {(\\ref {eq:initialproblem})} for many parameter values can be computationally {unfeasible}. Moreover, for real-time applications, a quantity of interest ($\\bu(\\mu)$ or a function of $\\bu(\\mu)$) has to be estimated on the fly in highly limited computational time for a certain value of $\\mu$. Model order reduction (MOR) methods are developed for efficient approximation of the quantity of interest for each parameter value. They typically consist of two stages.} In the first so-called offline stage a reduced model is constructed from the full order model. This stage usually involves expensive computations such as evaluations of $\\bu(\\mu)$ for several parameter values, computing multiple high-dimensional matrix-vector and inner products, etc., but this stage is performed only once. Then, for each given parameter value, the precomputed reduced model is used for efficient approximation of the solution or an output quantity with a computational cost independent of the dimension of the initial system of equations~\\textup {(\\ref {eq:initialproblem})}. For {a detailed presentation of the classical MOR methods} such as Reduced Basis (RB) method and Proper Orthogonal Decomposition (POD) the reader can refer to~\\cite{morbook2017}. In the present work the approximation of the solution shall be obtained with a minimal residual (minres) projection on a reduced (possibly parameter-dependent) subspace.  The minres projection can be interpreted as a Petrov-Galerkin projection where the test space is chosen to minimize some norm of the residual~\\cite{bui2008model,amsallem2013}. Major benefits over the classical Galerkin projection include an improved stability (quasi-optimality) for non-coercive problems and more effective residual-based error bounds of an approximation (see e.g.~\\cite{bui2008model}). In addition, minres methods are better suited to random sketching as will be seen in the present article.In recent years randomized linear algebra (RLA) became a popular approach in the fields such as data analysis, machine learning, compressed sensing, etc.~\\cite{woodruff2014sketching,mahoney2011randomized,tropp2007signal}. This {probabilistic approach for numerical linear algebra} can yield a drastic computational cost reduction in terms of classical metrics of efficiency such as complexity (number of flops) and memory consumption. Moreover, it can be highly beneficial in extreme computational environments that are typical in contemporary scientific computing. For instance, RLA can be essential when data has to be analyzed only in one pass (e.g., when it is streamed from a server) or when it is distributed on multiple workstations with expensive communication costs.  Despite their indisputable success in fields closely related to MOR, the aforementioned techniques only recently started {to be {extensively} used in MOR community.} One of the earliest works considering RLA in the context of MOR is~\\cite{zahm2016interpolation}, where the authors proposed to use RLA for interpolation of (implicit) inverse of a parameter-dependent matrix. In~\\cite{buhr2017randomized} the RLA was used for approximating the range of a transfer operator and for computing a probabilistic bound for {the} approximation error.  {In~\\cite{cao2004,homescu2005error,smetana2018randomized} the authors developed probabilistic error estimators based on random projections and the adjoint method. These approaches can also be formulated in RLA framework. {A randomized singular value decomposition (see~\\cite{halko2011finding,woodruff2014sketching}) was used for computing the POD vectors in~\\cite{hochman2014reduced}.} Efficient algorithms for Dynamic Mode De']","[""\\section{Minimal residual projection} \\label{minres} In this section we first present the standard minimal residual projection in a form that allows an easy introduction of random sketching. Then we introduce the sketched version of the minimal residual projection and provide conditions to guarantee its {quality}.     \\subsection{Standard minimal residual projection} \\label{minresproj} Let $U_r \\subset U$ be a subspace of $U$ (typically obtained with a greedy algorithm or approximate POD). The minres approximation $\\bu_r(\\mu) \\in U_r$ of $\\bu(\\mu)$ can be defined by\\begin{equation} \\label{eq:minresproj}\\bu_r(\\mu) = \\arg \\min_{\\bw \\in U_r} \\|\\br(\\bw; \\mu)\\|_{U'}.\\end{equation} For linear problems it is equivalently characterized by the following (Petrov-)Galerkin orthogonality condition:\\begin{equation} \\label{eq:minresproj2} \\langle \\br(\\bu_r(\\mu);\\mu), \\bw \\rangle=0, ~\\forall \\bw \\in V_r(\\mu),  \\end{equation}{where $V_r(\\mu) :=\\{ \\bR^{-1}_U\\bA(\\mu)\\bx : \\bx \\in U_r \\}$.}If the operator $\\bA(\\mu)$ is invertible then~\\textup {(\\ref {eq:minresproj})} is well-posed. In order to characterize the quality of the projection $\\bu_r(\\mu)$ we define the following parameter-dependent constants \\begin{subequations} \\label{eq:zetaiota_r}    \\begin{align}     &\\zeta_{r}(\\mu):=  \\min_{ \\bx  \\in \\left (\\mathrm{span} \\{ \\bu(\\mu) \\}+ U_r \\right ) \\backslash \\{ \\bnull \\}} \\frac{\\| \\bA(\\mu) \\bx  \\|_{U'}}{\\| \\bx \\|_U}, \\label{eq:alphar} \\\\     &\\iota_{r}(\\mu):=  \\max_{ \\bx  \\in \\left (\\mathrm{span} \\{ \\bu(\\mu) \\}+ U_r \\right ) \\backslash \\{ \\bnull \\}}  \\frac{ \\| \\bA(\\mu) \\bx \\|_{U'}}{\\| \\bx \\|_U}.\\label{eq:betar}    \\end{align} \\end{subequations}Let $\\bP_{W}:U \\rightarrow W$ denote the orthogonal projection from $U$ on a subspace $W \\subset U$, defined for $\\bx \\in U$ by \\begin{equation*}\\bP_{W} \\bx = \\arg\\min_{\\bw \\in W} \\| \\bx- \\bw \\|_{U}.\\end{equation*}\\begin{proposition}  \\label{thm:cea}    If~$\\bu_r(\\mu)$ satisfies~\\textup {(\\ref {eq:minresproj})} and $\\zeta_{r}(\\mu)>0$, then    \\begin{equation} \\label{eq:quasi-opt}    \\| \\bu(\\mu)- \\bu_r(\\mu) \\|_{U} \\leq  \\frac{\\iota_{r}(\\mu)}{\\zeta_{r}(\\mu)} \\| \\bu(\\mu)- \\bP_{U_r} \\bu(\\mu) \\|_{U}.     \\end{equation}    \\begin{proof}        See Appendix B.     \\end{proof}\\end{proposition}The constants $\\zeta_{r}(\\mu)$ and $\\iota_{r}(\\mu)$ can be bounded by the minimal and maximal singular values of $\\bA(\\mu)$:\\begin{subequations} \\label{eq:beta}    \\begin{align}     \\alpha(\\mu) &:=  \\underset{ \\bx  \\in U \\backslash \\{ \\bnull \\}} \\min \\frac{\\| \\bA(\\mu) \\bx  \\|_{U'}}{\\| \\bx \\|_U} \\leq \\zeta_r(\\mu), \\\\    \\beta(\\mu)  &:=  \\underset{ \\""]",,"[""\\section{Random sketching} \\label{randsk}A framework  for using random sketching (see~\\cite{halko2011finding,woodruff2014sketching}) in the context of MOR was introduced in~\\cite{balabanov2019galerkin}. The sketching technique is seen as a modification of {the} inner product in a given subspace (or a collection of subspaces). The modified inner product is an estimation of the original one and is much easier and more efficient to operate with. {Next, we briefly recall the basic preliminaries from~\\cite{balabanov2019galerkin}.}Let $V$ be a subspace of $U$. The dual of $V$ is identified with a subspace $V':=\\{ \\bR_{U} \\bx : \\bx \\in V \\}$ of $U'$. For a matrix $\\bTheta \\in \\mathbb{K}^{k\\times n}$ with $k\\leq n$ we define the following semi-inner products on $U$:\\begin{equation} \\label{eq:thetadef}\\langle \\cdot, \\cdot \\rangle^{\\bTheta}_{U}:= \\langle \\bTheta \\cdot, \\bTheta \\cdot \\rangle, \\textup{ and }\\langle \\cdot, \\cdot \\rangle^{\\bTheta}_{U'} := \\langle \\bTheta \\bR_U^{-1} \\cdot,  \\bTheta \\bR_U^{-1} \\cdot \\rangle,\\end{equation}and we let $\\| \\cdot \\|^{\\bTheta}_{U}$ and $\\| \\cdot \\|^{\\bTheta}_{U'}$ denote the associated semi-norms. \\begin{definition} \\label{def:epsilon_embedding}A matrix $\\bTheta$ is called a $U \\to \\ell_2$ $\\varepsilon$-subspace embedding (or simply an $\\varepsilon$-embedding) for $V$, if it satisfies\\begin{equation} \\label{eq:epsilon_embedding}\\forall \\bx, \\by \\in V, \\ \\left | \\langle \\bx, \\by \\rangle_U - \\langle \\bx, \\by \\rangle^{\\bTheta}_{U} \\right |\\leq \\varepsilon \\| \\bx \\|_U \\| \\by \\|_U.\\end{equation}\\end{definition}Here $\\varepsilon$-embeddings shall be constructed as realizations of random matrices that are built in an oblivious way without any a priori knowledge of $V$. \\begin{definition} \\label{def:oblepsilon_embedding}A random matrix $\\bTheta$ is called a $( \\varepsilon, \\delta, d)$ oblivious $U \\to \\ell_2$ subspace embedding if it is an $\\varepsilon$-embedding for an arbitrary $d$-dimensional subspace {$V \\subset U$} with probability at least $1-\\delta$.\\end{definition}Oblivious $\\ell_2 \\to \\ell_2$ subspace embeddings (defined by~Definition\\nobreakspace \\ref {def:epsilon_embedding} with $\\langle \\cdot, \\cdot \\rangle_{U} := \\langle \\cdot, \\cdot \\rangle$) include the rescaled Gaussian distribution, the rescaled Rademacher distribution, the Subsampled Randomized Hadamard Transform (SRHT), the Subsampled Randomized Fourier Transform (SRFT), CountSketch matrix, SRFT combined with sequences of random Givens rotations, and others~\\cite{balabanov2019galerkin,halko2011finding,woodruff2014sketching,rokhlin2008fast}. In this work we shall rely on the rescaled Gaussian distribution and SRHT.An oblivious $U \\to \\ell_2$ subspace embedding for a general inner product $\\langle \\cdot, \\cdot \\rangle_{U}$ can be constructed as \\begin{equation}\\bTheta = \\bOmega \\bQ, \\end{equation}where $\\bOmega$ is a $\\ell_2 \\to \\ell_2$  subspace embedding and $\\bQ \\in \\mathbb{K}^{s\\times n}$ is an easily computable (possibly rectangular) matrix such that $\\bQ^{\\mathrm{H}} \\bQ= \\bR_U$ (see~\\cite[Remark 2.7""]"
,"[""\\subsection{Sparse minimal residual approximation} \\label{sminres}Here we assume to be given a dictionary $\\mathcal{D}_K$ of $K$ vectors in $U$. Ideally, for each $\\mu$, $\\bu(\\mu)$ should be approximated by orthogonal projection onto a subspace $W_r(\\mu)$ that minimizes \\begin{equation} \\label{eq:Vmproj}\\|\\bu(\\mu) - \\bP_{W_r(\\mu)} \\bu(\\mu)\\|_U\\end{equation}over the library $\\mathcal{L}_r(\\mathcal{D}_K)$. The selection of the optimal subspace requires operating with the exact solution $\\bu(\\mu)$ which is prohibited. Therefore, the reduced approximation space $U_r(\\mu) \\in \\mathcal{L}_r(\\mathcal{D}_K)$ and the associated approximate solution $\\bu_r(\\mu) \\in U_r(\\mu)$ are obtained by residual norm minimization. The minimization of the residual norm, i.e., solving\\begin{equation} \\label{eq:Vmminresprojopt}\\min_{W_r \\in \\mathcal{L}_r(\\mathcal{D}_K)} \\min_{\\bw \\in W_r } \\|\\br(\\bw; \\mu)\\|_{U'},\\end{equation}is a combinatorial problem that can be intractable in practice. From a practical perspective we will assume that only a suboptimal solution can be obtained.\\subsection{Sketched sparse minimal residual approximation} \\label{ssminres}Let $\\bTheta \\in \\mathbb{K}^{k \\times n}$ be a certain $U \\to \\ell_2$ subspace embedding. A sparse minres approximation defined by~\\textup {(\\ref {eq:Vmminresproj})}, associated with dictionary $\\mathcal{D}_K$, can be estimated by solving the following problem: find $\\bu_r(\\mu) \\in U_r(\\mu) \\in \\mathcal{L}_r(\\mathcal{D}_K)$, such that\\begin{equation} \\label{eq:skVmminresproj}\\|\\br(\\bu_r(\\mu); \\mu)\\|^\\bTheta_{U'}   \\leq D \\min_{W_r \\in \\mathcal{L}_r(\\mathcal{D}_K)} \\min_{\\bw \\in W_r } \\|\\br(\\bw; \\mu)\\|^\\bTheta_{U'} + \\tau \\|\\bb(\\mu)\\|^\\bTheta_{U'}.\\end{equation}\\subsection{Dictionary generation} \\label{Dgen}The simplest way is to choose the dictionary as a set of solution samples (snapshots) associated with a training set $\\mathcal{P}_{\\mathrm{train}}$, i.e., \\begin{equation} \\mathcal{D}_K = \\{ \\bu(\\mu): \\mu \\in  \\mathcal{P}_{\\mathrm{train}} \\}.\\end{equation}A better dictionary may be computed with the greedy procedure presented in~Algorithm\\nobreakspace \\ref {alg:sk_greedy_online}, recursively enriching the dictionary with a snapshot at the parameter value associated with the maximal error at the previous iteration.\\section{A posteriori certification of the sketch and solution} \\label{sketchcert}Here we provide a simple, yet efficient procedure for a posteriori verification of the quality of a sketching matrix and describe a few scenarios where such a procedure can be employed. The proposed a posteriori certification of the sketched reduced model and its solution is probabilistic. It does not require operating with high-dimensional vectors but only with their small sketches.\\subsection{Verification of an $\\varepsilon$-embedding for a given subspace}Let $\\bTheta$ be a $U \\to \\ell_2$ subspace embedding and $V \\subset U$ be a subspace of $U$. We propose to verify the accuracy of $\\langle \\cdot, \\cdot \\rangle^{\\bTheta}_{U}$ simply by comparing it to an inner product $\\langle \\cdot, \\cdot \\rangle^{\\bTheta^*}_{U}$ associated with a new random embedding $\\bTheta^* \\in \\mathbb{K}^{k^*\\times n}$.\\subsection{Certification of a sketch of a reduced model and its solution}The results can be employed for certification of a sketch of a reduced model and its solution. They can also be used for adaptive selection of the number of rows of a random sketching matrix""]",,"[""Here we assume to be given a dictionary $\\mathcal{D}_K$ of $K$ vectors in $U$. Ideally, for each $\\mu$, $\\bu(\\mu)$ should be approximated by orthogonal projection onto a subspace $W_r(\\mu)$ that minimizes \\begin{equation} \\|\\bu(\\mu) - \\bP_{W_r(\\mu)} \\bu(\\mu)\\|_U\\end{equation}over the library $\\mathcal{L}_r(\\mathcal{D}_K)$. The selection of the optimal subspace requires operating with the exact solution $\\bu(\\mu)$ which is prohibited. Therefore, the reduced approximation space $U_r(\\mu) \\in \\mathcal{L}_r(\\mathcal{D}_K)$ and the associated approximate solution $\\bu_r(\\mu) \\in U_r(\\mu)$ are obtained by residual norm minimization. The minimization of the residual norm, i.e., solving\\begin{equation} \\min_{W_r \\in \\mathcal{L}_r(\\mathcal{D}_K)} \\min_{\\bw \\in W_r } \\|\\br(\\bw; \\mu)\\|_{U'},\\end{equation}is a combinatorial problem that can be intractable in practice. From a practical perspective we will assume that only a suboptimal solution can be obtained. Let $\\bTheta \\in \\mathbb{K}^{k \\times n}$ be a certain $U \\to \\ell_2$ subspace embedding. A sparse minres approximation defined by the residual norm minimization, associated with dictionary $\\mathcal{D}_K$, can be estimated by solving the following problem: find $\\bu_r(\\mu) \\in U_r(\\mu) \\in \\mathcal{L}_r(\\mathcal{D}_K)$, such that\\begin{equation} \\|\\br(\\bu_r(\\mu); \\mu)\\|^\\bTheta_{U'} \\leq D \\min_{W_r \\in \\mathcal{L}_r(\\mathcal{D}_K)} \\min_{\\bw \\in W_r } \\|\\br(\\bw; \\mu)\\|^\\bTheta_{U'} + \\tau \\|\\bb(\\mu)\\|^\\bTheta_{U'}.\\end{equation}The simplest way to choose the dictionary is as a set of solution samples (snapshots) associated with a training set $\\mathcal{P}_{\\mathrm{train}}$. A better dictionary may be computed with a greedy procedure, recursively enriching the dictionary with snapshots at parameter values associated with maximal errors. Here we provide a simple procedure for a posteriori verification of the quality of a sketching matrix. The proposed certification is probabilistic and operates only on small sketches rather than high-dimensional vectors. We verify the accuracy of the sketched inner product by comparing it to a new random embedding $\\bTheta^*$. The results can be used for certification of reduced models and adaptive selection of sketching matrix sizes. <!--TRUNCATED-->""]"
,,,"['\\begin{figure}[h!]    \\centering    \\begin{subfigure}[b]{.4\\textwidth}        \\centering        \\includegraphics[width=\\textwidth]{Ex1_minres_res.pdf}        \\caption{minres, $\\Delta_{\\mathcal{P}}$}        \\label{fig:Ex1_1a}    \\end{subfigure} \\hspace{.01\\textwidth}    \\begin{subfigure}[b]{.4\\textwidth}        \\centering        \\includegraphics[width=\\textwidth]{Ex1_Galerkin_res.pdf}        \\caption{Galerkin, $\\Delta_{\\mathcal{P}}$}        \\label{fig:Ex1_1b}    \\end{subfigure}\t    \\begin{subfigure}[b]{.4\\textwidth}        \\centering        \\includegraphics[width=\\textwidth]{Ex1_minres_exact.pdf}        \\caption{minres, $e_{\\mathcal{P}}$}        \\label{fig:Ex1_1c}    \\end{subfigure} \\hspace{.01\\textwidth}    \\begin{subfigure}[b]{.4\\textwidth}        \\centering        \\includegraphics[width=\\textwidth]{Ex1_Galerkin_exact.pdf}        \\caption{Galerkin, $e_{\\mathcal{P}}$}        \\label{fig:Ex1_1d}    \\end{subfigure}    \\begin{subfigure}[b]{.4\\textwidth}        \\centering        \\includegraphics[width=\\textwidth]{Ex1_minres_quantity.pdf}        \\caption{minres, $e^s_{\\mathcal{P}}$}        \\label{fig:Ex1_1e}    \\end{subfigure} \\hspace{.01\\textwidth}    \\begin{subfigure}[b]{.4\\textwidth}        \\centering        \\includegraphics[width=\\textwidth]{Ex1_Galerkin_quantity.pdf}        \\caption{Galerkin, $e^s_{\\mathcal{P}}$}        \\label{fig:Ex1_1f}    \\end{subfigure}    \\vspace*{-0.5em}    \\caption{ \\small The errors of the classical minres and Galerkin projections and quantiles of probabilities $p=1, 0.9, 0.5$ and $0.1$ over 20 realizations of the errors of the sketched minres and Galerkin projections, versus the number of rows of $\\bTheta$.  (a) Residual error $\\Delta_\\mathcal{P}$ of standard and sketched minres projection. (b) Residual error $\\Delta_\\mathcal{P}$ of standard and sketched Galerkin projection. (c) Exact error $e_\\mathcal{P}$ (in $\\| \\cdot \\|_U$) of standard and sketched minres projection. (d) Exact error $e_\\mathcal{P}$ (in $\\| \\cdot \\|_U$) of standard and sketched Galerkin projection. (e) Quantity {of interest} error $e^s_\\mathcal{P}$ of standard and sketched minres projection. (f) Quantity {of interest} error $e^s_\\mathcal{P}$ of standard and sketched Galerkin projection.}    \\label{fig:Ex1_1}\\end{figure}']"
,,"[""\\section{Conclusion} \\label{concl}\t In this article we have extended the methodology from~\\cite{balabanov2019galerkin} to minres methods and proposed a novel nonlinear approximation method to tackle problems with a slow decay of Kolmogorov $r$-width.  The main ingredient of our approach is the approximation of the reduced model's solution from a random sketch, which entails drastic reduction of the computational costs and improvement of numerical stability. Precise conditions on thesketch to yield the (approximate) preservation of the quasi-optimality constants of the reduced model's solution are provided.  These conditions do not depend on theoperator's properties, which implies robustness for ill-conditioned and non-coerciveproblems. Moreover, theseconditions can be ensured with SRHT orGaussian matrices of sufficiently large sizes depending only logarithmically on the probability of failure and on the cardinality of the dictionary (for dictionary-based approximation).We here also proposed efficient randomized methods for extraction of the quantity of interest {(see Appendix A)} and a posteriori certification of the reduced model's sketch. These results along with the sketched minres projection can be used as a remedy of the drawbacks revealed in~\\cite{balabanov2019galerkin}.The applicability of the proposed methodology was realized on two benchmark problems difficult to tackle with standard methods. The experiments on {the} invisibility cloak benchmark {confirmed} that random sketching {can indeed provide} high computational savings in both offline and online stages, and more numerical stability compared to the standard minres method while preserving the quality of the output.  It was {illustrated} experimentally that the random sketching technique {may be better suited} to minres methods than to Galerkin methods. Furthermore, the proposed procedure for the a posteriori certification of the sketch's quality was also experimentally tested. It yielded bounds for the dimension of random projections an order of magnitude less than the theoretical ones from~\\cite{balabanov2019galerkin}.\t\tIn the advection-diffusion benchmark a slow-decay of the Kolmogorov $r$-width was revealed, which implied the necessity to use the dictionary-based approximation. It was verified that for this problem the dictionary-based approximation provided an {enhancement} of the online stage in more than an order of magnitude in complexity, in about $2$ times in terms of memory, and in about $4$ times in terms of runtime (compared to the sketched minres projection). Moreover, even higher computational savings could be obtained by representing the sketch of the dictionary in a more favorable format (e.g., as in~\\cite{le2016flexible,rubinstein2009double}), which we leave for future research.""]","[""The experiments on {the} invisibility cloak benchmark {confirmed} that random sketching {can indeed provide} high computational savings in both offline and online stages, and more numerical stability compared to the standard minres method while preserving the quality of the output.  It was {illustrated} experimentally that the random sketching technique {may be better suited} to minres methods than to Galerkin methods. Furthermore, the proposed procedure for the a posteriori certification of the sketch's quality was also experimentally tested. It yielded bounds for the dimension of random projections an order of magnitude less than the theoretical ones from~\\cite{balabanov2019galerkin}.\t\tIn the advection-diffusion benchmark a slow-decay of the Kolmogorov $r$-width was revealed, which implied the necessity to use the dictionary-based approximation. It was verified that for this problem the dictionary-based approximation provided an {enhancement} of the online stage in more than an order of magnitude in complexity, in about $2$ times in terms of memory, and in about $4$ times in terms of runtime (compared to the sketched minres projection). Moreover, even higher computational savings could be obtained by representing the sketch of the dictionary in a more favorable format (e.g., as in~\\cite{le2016flexible,rubinstein2009double}), which we leave for future research.""]"
,,,"['\\begin{proof} [Proof of Proposition\\nobreakspace \\ref{thm:omegaUB}]\t\t\t\t\tObserve that \t\t\t\t$$\\omega = \\max \\left \\{ 1- \\min_{\\bx \\in V / \\{ \\bnull \\}} \\left (\\frac{\\| \\bx\\|^{\\bTheta}_U}{\\| \\bx\\|_U} \\right )^2, \\max_{\\bx \\in V / \\{ \\bnull \\}} \\left (\\frac{\\| \\bx\\|^{\\bTheta}_U}{\\| \\bx\\|_U}\\right )^2 -1  \\right \\}.$$\t\t\t\tLet us make the following assumption: $$   1- \\min_{\\bx \\in V / \\{ \\bnull \\}} \\left (\\frac{\\| \\bx\\|^{\\bTheta}_U}{\\| \\bx\\|_U} \\right )^2 \\geq \t\\max_{\\bx \\in V / \\{ \\bnull \\}} \\left (\\frac{\\| \\bx\\|^{\\bTheta}_U}{\\| \\bx\\|_U}\\right )^2 -1. $$ For the alternative case the proof is similar. \t\t\t\t\t\t\t\tNext, we show that $\\bar{\\omega}$ is an upper bound for $\\omega$ with probability at least $1-\\delta^*$. Define $\\bx^*:= \\arg \\min_{\\bx \\in V / \\{ \\bnull \\}, \\| \\bx \\|_U=1} \\| \\bx\\|^{\\bTheta}_U$. By definition of $\\bTheta^*$, \t\t\t\t\\begin{equation} \\label{eq:obldef}\t\t\t\t1-\\varepsilon^* \\leq \\left( \\|\\bx^*\\|^{\\bTheta^*}_{U} \\right )^2\t\t\t\t\\end{equation} \t\t\t\tholds with probability at least $1-\\delta^*$. If~\\textup {(\\ref {eq:obldef})}<!--TRUNCATED-->']"
"['\\section{Introduction}In \\cite{BKT}, S. Baba, S. Kotyada and R. Teja demonstrate how to define an approximate one-way function FACTOR in a non-Abelian group. As examples of a platform for realization of FACTOR they suggest one of groups like GL$_n$($\\mathbb{F}_q$), UT$_n$($\\mathbb{F}_q$), or Braid Groups $B_n$, $n \\in \\mathbb{N}.$ Here $\\mathbb{F}_q$ denotes the finite field of order $q.$ They believe that the function FACTOR is one-way. It means that the inverse to the FACTOR is easy to compute, while the function itself is hard to compute.Then, using FACTOR function as a primitive the authors of \\cite{BKT} therefore define a public key cryptosystem which is comparable to the classical El-Gamal system based on the discrete logarithm problem. Recall, that the El-Gamal system can be described as follows: Let $G$ be a public finite cyclic group with generator $g$, and let $x \\in \\mathbb{Z}$ is Alices private key. The element $g^x$ is public. To send a message $m \\in G,$ Bob picks a random integer $y$ and sends the cipher text $c = (g^y,g^{xy}m)$ to Alice. To decrypt, Alice calculates $(g^y)^x=g^{xy}$ and inverts it to retrieve $m.$In \\cite{BKT}, the authors also propose a key exchange, analagous to the Diffie-Hellman key exchange protocol in a non-Abelian setting using FACTOR. Recall, that the classical Diffie-Hellman protocol can be described as follows: Let $G$ be a public finite cyclic group with generator $g$, and let $x \\in \\mathbb{Z}$ is Alices private key, as well as $y\\in \\mathbb{Z}$ is Bobs private key. Alice publishes $g^x$ and Bob publishes $g^y.$ Then each of them computes the exchanged key $g^{xy} = (g^x)^y= (g^y)^x.$In this paper, we apply and compare two methods of algebraic cryptanalysis via linear algebra, namely, the linear decomposition method invented and developed by the author in \\cite{R1} - \\cite{R3} and in \\cite{RM} (with A. Myasnikov), and the span-method invented and developed by B. Tsaban in \\cite{T} and in \\cite{TBK} (with A. Ben-Zvi and A. Kalka), to show vulnerability of the proposed in \\cite{BKT} cryptosystem and protocol. <!--TRUNCATED-->']","[""\\section{The ElGamal-type cryptosystem   based on FACTOR \\cite{BKT}}  Let $G$ be any public   group. Let $g, h \\in G$ be two private elements of Alice,   and let $<g>$  and $<h>$ be the cyclic subgroups generated by these elements, respectively.   In order to define the FACTOR problem one assume that $< g > \\cap  < h >= \\{1\\}$. Let $f: <g> \\times <h> \\rightarrow G$  be a function defined as follows: $f(g^x,h^y) = g^x \\cdot h^y, $ where $x, y \\in \\mathbb{Z}.$  Obviously,  that $f$ is injective.  Then  FACTOR($g^xh^y$) $= f^{-1}(g^xh^y).$ {\\bf Cryptosystem.} Let $G$ be a non-Abelian group and let $g,h \\in G$ be two non commuting elements. We assume that $<g>\\cap <h>=\\{1\\}.$ We  suppose that Alice is the recipient of the messages and Bob is communicating with Alice. Let $m \\in  G$ be the message. Alice picks arbitrary  integers $x,y\\in \\mathbb{Z}$ and sets a public key  $(G,g,h,g^xh^y).$  Alice has a private key $(g^x, h^y)$ for decryption.To send the message $m$, Bob picks arbitrary   integers $x',y'$ and sends cipher text$$ c = (g^{x+x'}h^{y+y'},g^{x'}h^{y'}m)$$\\noindent to Alice. To decrypt  the text, Alice uses her private key and  calculates $$(g^x)^{-1}(g^{x+x'}h^{y+y'})(h^y)^{-1}=g^{x'}h^{y'}.$$  Then she  inverts it to retrieve $m.$ The authors of this scheme hoped  that the security of the crypto system described above reduces to solving FACTOR problem in the underlying group. Below we'll show that the system is vulnerable against linear algebra attacks. {\\bf Cryptanalysis.}We will show that any intruder can efficiently compute $g^{x'}h^{y'}$ and then retrieve $m.$ I. First we will use  the Tsaban's span-method. We suppose that $G$ is a finite group presented as a matrix group over a finite field. So, let  $G \\leq$ M$_n$($\\mathbb{F}_q$). Let $V=$ Lin$_{\\mathbb{F}_q}(<g>)$ be the linear subspace of M$_n$($\\mathbb{F}_q$) generated by all matrices of the form $g^i, i \\in \\mathbb{Z}.$ Then dim($V$) $\\leq n-1.$ In fact, the matrices $1, g, g^2, ..., g^n$  are linearly dependent, since $g$ is the root of its characteristic polynomial of degree $n$. Obviously, if $g^{k+1}$ lies in Lin$_{\\mathbb{F}_q}$($1, g, g^2, ..., g^k$), then $g^{k+t}, g^{1-t} \\in$ Lin$_{\\mathbb{F}_q}$($1, g^2, ..., g^k$) for every $t = 2, 3, ...$. We can efficiently construct a basis $1, g, g^2, ..., g^k$ of $V$ by checking for every succesive $l = 1, 2, ...$ either  $g^{l+1}$ lies in Lin$_{\\mathbb{F}_q}$($1, g, g^2, ..., g^l$), or not. Then $k$ is the least $l$ such that this happens. Such verification is carried out by the Gauss elimination method which is known as efficient.  Consider the equation \\begin{equation}\\label{eq:1}f(g^xh^y)h = hf(g^xh^y) \\sim fg^xh = h""]",,"[""\\section{The Diffie-Hellman-type key exchange protocol   based on FACTOR \\cite{BKT}}   Suppose Alice and Bob want to exchange keys. Suppose $G,g,h$ are as in FACTOR. Let Alice pick a pair of integers $(x_1,y_1),$ and Bob pick two integers $(x_2,y_2)$.   Then  Alice sends the element $g^{x_1}h^{y_1}$  to Bob.   Independently  Bob sends the element $g^{x_2}h^{y_2}$  to Alice.    Both Alice and Bob can recover the element $K=g^{x_1+x_2}h^{y_1+y_2}$. This is their private key. {\\bf Cryptanalysis.}Now we will apply and describe only the author's linear decomposition method. Let $G \\leq$ M$_n$($\\mathbb{F}$) be a matrix group over arbitrary (constructive) field $\\mathbb{F}.$ Let $V=$ Lin$_{\\mathbb{F}}(<g><h>)$ be the linear subspace of M$_n$($\\mathbb{F}$) generated by all matrices of the form $g^ih^j, i, j \\in \\mathbb{Z}.$ Then dim($V$) $\\leq (n-1)^2.$ Let $e_1, e_2, ..., e_r$ be a basis of $V$ that can be efficiently obtained  in the same way as described above. Let $e_i = g^{u_i}h^{v_i}, u_i, v_i \\in \\mathbb{Z}, i = 1, ..., r.$ Since, $g^{x_1}h^{y_1} \\in V,$ we can efficiently obtain a presentation of the form\\begin{equation}\\label{eq:5}g^{x_1}h^{y_1} = \\sum_{i=1}^r\\alpha_ie_i, \\ \\alpha_i \\in \\mathbb{F}, \\  i = 1, ..., r.\\end{equation}Then \\begin{equation}\\label{eq:6}\\sum_{i=1}^r\\alpha_ig^{u_i}(g^{x_2}h^{y_2})h^{v_i}= g^{x_2}( \\sum_{i=1}^r\\alpha_ie_i)h^{y_2}=K. \\end{equation}We succeeded again. Of course, the Tsaban's span-method can be applied too.   The described cryptanalysis has many analogues, presented in \\cite{R1}-\\cite{TBK}. In \\cite{R4}, a general scheme based on multiplications is presented. It corresponds to a number of cryptographic systems known in the literature, which are also vulnerable to attacks by the linear decomposition method. Note that the Tsaban's span-method allows him to show the vulnerability of the well-known schemes of Anshel et al. \\cite{AAG}, and   the Triple Decomposition Key Exchange Protocol of Peker \\cite{Kurt}.    A protection against linear algebra attacks is invented in \\cite{R5}. It is described in the case of the Anshel et al. cryptographic scheme but can be applied to the Diffie-Hellman-type and some other schemes too. <!--TRUNCATED-->""]"
"[""\\section{Introduction}Ternary paths are cousins of Dyck paths, but with up-steps $(1,1)$ and down-steps $(1,-2)$, starting at the origin and never going below the $x$-axis. In most cases, one is interested in such paths that also end at the $x$-axis, but also at paths ending at level $i$ after $n$ steps.Ternary paths are of interest at least for the following reasons:\\begin{itemize}    \\item They are no longer symmetric with respect to left $\\leftrightarrow$ right, i.e., arguments using symmetry as often employed when dealing with Dyck paths are no longer possible.        \\item Although some underlying generating functions are cubic, they are still manageable due to a substitution which allows to separate one factor and deal with a quadratic equation only.        \\item Although one can look at more general classes of paths, the ternary case is of a nature to allow for explicit results.        \\item Knuth based his popular christmas talk on the related concept of ternary trees, showing long-standing interest in the subject.        \\item Ternary paths form a large portion of N. Cameron's thesis but some answers were formulated as conjectures.\\end{itemize}Our method of choice is to find generating functions for ternary paths bounded by $h$ and then letting $h\\to\\infty$. This has the advantage of dealing only with finite linear systems solvable via Cramer's rule. An alternative kernel method approach exists but would double the paper's length.We address two main questions: 1) Enumeration of ternary paths from left to right, starting at level 0 and ending at level $i$  2) Enumeration from right to left under the same conditions. The second question proves more challenging, requiring analysis of multiple roots of a cubic equation.As a corollary, we compute the (cumulated) area summed over all ternary paths of length $3n$, confirming a conjecture from Cameron's thesis. For a path $(0,c_0),...,(3n,c_{3n})$, the area is defined as $c_0+...+c_{3n}$.Banderier and Gittenberger studied the area in more general settings, but explicit results like ours are restricted to binary (Dyck) and ternary cases. Our main findings include combinatorial formulas for:- Left-to-right paths ending at level $3K+i$- Right-to-left paths ending at level $i$  - Total area of length $3N$ paths\\textbf{Remarks.}  $\\heartsuit$ Left-to-right enumeration was known to Cameron but rediscovered multiple times  $\\diamondsuit$ Right-to-left enumeration appeared in different forms in prior work  $\\spadesuit$ Equivalent lattice path models exist but cannot express area naturally  $\\clubsuit$ Height-restricted enumeration provides essential generating functions for future analyses of average heights, following seminal work on Dyck paths by de Bruijn et al. <!--TRUNCATED-->""]","[""\\section{Enumeration of ternary paths from left to right}Our method involves working with generating functions for ternary paths bounded by height $h$, then taking the limit as $h\\to\\infty$. The key steps are:1. Define $a_{n,k}$ as the number of height-bounded ternary paths ending at $(n,k)$2. Establish the recursion: $a_{n,k} = a_{n-1,k-1} + a_{n-1,k+2}$ with boundary conditions3. Introduce generating functions $f_k(z) = \\sum_{n\\geq0}a_{n,k}z^n$4. Formulate the system as a matrix equation solvable via Cramer's rule5. Analyze the determinant $d_h$ through its characteristic equation $\\lambda^3 - \\lambda^2 + z^3 = 0$6. Apply the substitution $z^3 = t(1-t)^2$ to obtain explicit roots7. Compute the limit as $h\\to\\infty$ to get $f_k(z) = z^k/(1-t)^{k+1}$\\section{Enumeration of ternary paths from right to left}The more challenging right-to-left enumeration requires:1. Switching to $b_{n,k}$ counts and generating functions $g_k(z)$2. Establishing a modified matrix equation3. Analyzing determinants $\\Delta_{i,j}$ through their characteristic equation $Y^3 - zY + z^3 = 0$4. Using the same substitution $z^3 = t(1-t)^2$5. Expressing solutions in terms of roots $\\mu_1, \\mu_2, \\mu_3$6. Applying the Girard-Waring formula for simplification7. Obtaining the final generating function form through careful limit analysis\\section{The area computation}The area calculation leverages previous results by:1. Viewing each path as a left-right/right-left split at level $i$2. Taking the product of corresponding generating functions3. Performing contour integration with the substitution4. Arriving at the explicit binomial sum formula for total area""]",,"[""\\section{A warm-up: the equation for ternary trees}We start with the equation $X=1+xX^3$ for ternary trees. The combinatorially relevant solution is $\\mathcal{B}_3(x)$ and satisfies the equation $\\mathcal{B}_3(x)=1+x\\mathcal{B}_3(x)^3$, since a ternary tree is either the empty tree or a root followed by 3 (ordered) subtrees. For $t$-ary trees, one has $\\mathcal{B}_t(x)=1+x\\mathcal{B}_t(x)^t$, but explicit results are only expected for $t=2$ and $t=3$.Using the substitution $x=t(1-t)^2$, the other two solutions are:\\begin{equation*}\\sigma_{1}= -\\frac1{2(1-t)}-\\frac{\\sqrt{4t-3t^2}}{2t(1-t)},\\quad\\sigma_{2}= -\\frac1{2(1-t)}+\\frac{\\sqrt{4t-3t^2}}{2t(1-t)}.\\end{equation*}The first part $(-\\frac1{2(1-t)}=-\\frac12\\mathcal{B}_3(x))$ is well understood, so we focus on $-\\frac{\\sqrt{4t-3t^2}}{2t(1-t)}$ and use:\\begin{equation*}t^k=\\sum_{n\\ge k}\\binom{3n-k-1}{n-k}\\frac knx^n,\\end{equation*}which follows from Lagrange inversion. We compute:\\begin{align*}-x^{1/2}\\frac{\\sqrt{4t-3t^2}}{2t(1-t)}&=-\\frac{\\sqrt{4t-3t^2}}{2t^{1/2}}=-\\sqrt{1-\\tfrac34t}=\\sum_{k\\ge0}(-1)^{k-1}(\\tfrac34)^k\\binom{1/2}{k}t^k\\\\&=\\sum_{k\\ge0}(-1)^{k-1}(\\tfrac34)^k\\binom{1/2}{k}\\sum_{n\\ge k}\\binom{3n-k-1}{n-k}\\frac knx^n\\\\&=\\sum_{n\\ge0}x^n \\sum_{1\\le k\\le n}(-1)^{k-1}(\\tfrac34)^k\\binom{1/2}{k}\\binom{3n-k-1}{n-k}\\frac kn\\end{align*}The equation factors as:\\begin{equation*}x(X-\\mathcal{B}_3(x))(X-\\sigma_1)(X-\\sigma_2)=xX^3+1-X.\\end{equation*}Thus $x\\mathcal{B}_3(x)\\sigma_1\\sigma_2=-1$, leading to:\\begin{align*}\\mathcal{B}_3(x)=\\mathcal{B}_{3/2}(x^{1/2})^{1/2}\\mathcal{B}_{3/2}(-x^{1/2})^{1/2}.\\end{align*}\\begin{remark}This factorization was obtained by Bousquet-M\\'elou and Petkovsek using different methods. Our approach uses the substitution $x=t(1-t)^2$ which is central for later sections.\\end{remark} <!--TRUNCATED-->""]"
,,,"['Let $A$ be a complex associative algebra and let $\\C$ be a class of representations of $A$.  %In particular, we will use $\\R$ (resp.\\ $\\I$) for the class of all finite-dimensional (resp.\\ finite-dimensional irreducible) representations of $A$.%For a finite-dimensional vector space $V$ we denote by $\\endo(V)$ the set of all endomorphism of $V$.We say that the elements $p_1,\\ldots,p_k \\in A$ are $\\C$-\\textit{locally linearly dependent} (abbreviated as $\\C$-LLD) if for everyrepresentation $\\pi:A\\to \\endo(V_\\pi)$ in $\\C$ we have that $\\pi(p_1),\\ldots,\\pi(p_k)$ are linearly dependent. %(in $\\endo(V_\\pi)$). We say that elements $p_1,\\ldots,p_k \\in A$ are $\\C$-\\textit{locally directionally linearly dependent} (abbreviated as $\\C$-LDLD) if for everyrepresentation $\\pi:A\\to \\endo(V_\\pi)$ in $\\C$ and every vector $v \\in V_\\pi$ we have that $\\pi(p_1)v,\\ldots,\\pi(p_k)v$ are linearly dependent. %(in $V_\\pi$). Clearly, linear dependence implies $\\C$-LLD which implies $\\C$-LDLD. The opposite implications are false in general. %{\\color{blue} referenca.}The motivation for this terminology comes from \\cite{bk}.<!--TRUNCATED-->']"
,,,"['\\begin{proposition} \\label{nonzero-proposition}\tFor every $d,m_1,m_2 \\in \\NN_0$ with $m_1 \\ge d$ and $m_2 \\ge d$, the vectors\t\t$v_{k,\\ell,m}$ with $k,\\ell,m\\in \\NN_0$ such that $k+\\ell+m\\leq d$ are linearly independent.\\end{proposition}\\begin{proof}Denote $S_d:=\\{(k,\\ell,m)\\in \\NN_0^3\\colon k+\\ell+m\\leq d\\}$ and assume that \t\\begin{equation}\\label{eq-lin-ind}\t\t\\sum_{(k,\\ell,m)\\in S_d} \\alpha_{k,\\ell,m} v_{k,\\ell,m}=0\t\\end{equation}for some $\\alpha_{k,\\ell,m}\\in \\RR$.We have to prove that each $\\alpha_{k,\\ell,m}$ is zero. After a short computation which depends on the formula\t\\begin{equation*}\t\t\\pi_{m_1,m_2}(Y_i^j)(u_1 \\otimes u_2) =\\sum_{q=0}^j \\binom{j}{q} \\psi_1(Y_i^q)u_1 \\otimes \\psi_2(Y_i^{j-q})u_2\t\\end{equation*}for each $i$ and $j$ we get that\t\\begin{equation}\\label{allveceq}\t\tv_{k,\\ell,m}=\t\t\t\\sum_{t=0}^m \\sum_{s=0}^k \\beta_{s,t}^{k,\\ell,m} e_1^{m_1-s-t} e_2^s e_3^t \\otimes f_1^{m_2+t-\\ell-k} f_2^{\\ell-k+s} f_3^{m+k-s-t} \t\t\\end{equation}where $\\beta_{s,t}^{k,\\ell,m}\\in \\RR$ and in particular $\\beta_{k,m}^{k,\\ell,m}=\\binom{m_1}{m}\\binom{m_2}{\\ell}\\binom{m_1-m}{k}\\neq 0.$For $a,b,c\\in \\NN_{0}$ we denote by $P_{a,b,c}$ the projection to the linear subspace $\\Lin\\{e_{1}^{i_1}e_2^be_3^a\\otimes f_1^{i_2}f_2^{c}f_3^{i_3}\\colon i_j\\in \\NN_0\\}.$Applying projections $P_{a,b,c}$ repeatedly in the lexicographic ordering of indices $(a,b,c)$ where $(b,c,a)\\in S_d$ on \\eqref{eq-lin-ind} and using \\eqref{allveceq} we deduce inductivelythat each $\\alpha_{k,\\ell,m}$ in \\eqref{eq-lin-ind} is zero. Namely, first\t$$0=P_{d,0,0}\\Big(\\sum_{(k,\\ell,m)\\in S_d} \\alpha_{k,\\ell,m} v_{k,\\ell,m}\\Big)=\\alpha_{0,0,d}\\cdot \\beta_{0,d}^{0,0,d} e_1^{m_1-d}e_3^d\\otimes f_1^{m_2}$$implies that $\\alpha_{0,0,d}=0$ (since $\\beta_{0,d}^{0,0,d}\\neq 0$).Now fix $(a_0,b_0,c_0)$ and assume that $\\alpha_{b,c,a}=0$ for all $(a,b,c)\\succ_{\\mathrm{lex}} (a_0,b_0,c_0)$. Then\t\t\\begin{eqnarray*}\t\t0\t&=&\tP_{a_0,b_0,c_0}\\Big(\\sum_{(k,\\ell,m)\\in S_d} \\alpha_{k,\\ell,m} v_{k,\\ell,m}\\Big)\\\\\t\t\t&=&\t\\alpha_{b_0,c_0,a_0}\\cdot \\beta_{b_0,a_0}^{b_0,c_0,a_0} e_1^{m_1-b_0-a']"
"[""\\section{Introduction}Difference dimension polynomials, first introduced in \\cite{Levin1} and \\cite{Levin2}, can be viewed as difference algebraic counterparts of Hilbert polynomials in commutative algebra and algebraic geometry, as well as of differential dimension polynomials in differential algebra. Difference dimension polynomials and their invariants are power tools for the study of difference and inversive difference field extensions, systems of algebraic difference equations, difference and inversive difference rings and modules.Moreover, difference dimension polynomials play a significant role in the qualitative theory of difference equations, because the difference dimension polynomial of a system of algebraic difference equations expresses the Einstein's strength of the system. In this paper we present a method of characteristic sets for inversive difference polynomials and apply it for the computation of difference dimension polynomials associated with difference schemes for some systems of quasi-linear algebraic PDEs.Hitherto, algorithmic methods for computing difference dimension polynomials (and therefore for determining the Einstein's strength) have been developed just for systems of linear difference equations. This work provides methods of computation of difference dimension polynomials for essentially wider class of systems of difference equations. We prove the results on characteristic sets of difference ideals generated by quasi-linear difference polynomials that allow one to determine the Einstein's strength of important non-linear systems.In particular, we determine the strengths of systems of partial difference equations that arise from such schemes for reaction-diffusion PDEs. These equations play the key role in the theoretical foundation of the main methods for accurate, and rapid determination of biologically active organic carboxylic acids in objects such as infusion solutions and blood preservatives.""]","['\\section{Characteristic Sets and Difference Dimension Polynomials}Let $K$ be an inversive difference ($\\sigma^{\\ast}$-) field with a basic set $\\sigma = \\{\\alpha_{1},\\dots, \\alpha_{m}\\}$ and let $\\Gamma$ denote the free commutative group generated by $\\sigma$. If $\\gamma = \\alpha_{1}^{k_{1}}\\dots \\alpha_{m}^{k_{m}}\\in \\Gamma$ ($k_{1},\\dots, k_{m}\\in \\mathbb{Z}$), the order of the element $\\gamma$ is defined as $\\ord\\gamma = \\D\\sum_{i=1}^{m}|k_{i}|$. For any $r\\in\\mathbb{N}$, we set $\\Gamma(r) = \\{\\gamma\\in\\Gamma\\,|\\, \\ord\\gamma\\leq r\\}$.Let $K\\{y_{1},\\dots, y_{n}\\}^{\\ast}$ be the algebra of $\\sigma^{\\ast}$-polynomials in $\\sigma^{\\ast}$-indeterminates $y_{1},\\dots, y_{n}$ over $K$ and let $\\Gamma Y$ denote the set $\\{\\gamma y_{i} | \\gamma \\in \\Gamma, 1\\leq i\\leq m \\}$ whose elements are called terms. By the order of a term $u = \\gamma y_{j}$ we mean the order of the element $\\gamma \\in \\Gamma$.A well-ordering of the set of terms $\\Gamma Y$ is called a ranking of the family of $\\sigma^{\\ast}$-indeterminates $y_{1},\\dots, y_{n}$ if it satisfies the following conditions: (i) If $u\\in (\\Gamma Y)_{j}$ and $\\gamma \\in \\Gamma_{j}$, then $u\\leq \\gamma u$. (ii) If $u, v\\in (\\Gamma Y)_{j}$, $u\\leq v$ and $\\gamma \\in \\Gamma_{j}$, then $\\gamma u \\leq \\gamma v$.If $A\\in K\\{y_{1},\\dots, y_{n}\\}^{\\ast}$, then the greatest (with respect to the ranking $\\leq$) term that appears in $A$ is called the leader of $A$; it is denoted by $u_{A}$. If $u = u_{A}$ and $d=\\deg_{u}A$, then the $\\sigma^{\\ast}$-polynomial $A$ can be written as $A = I_{d}u^{d} + I_{d-1}u^{d-1}+\\dots + I_{0}$ where $I_{k} (0\\leq k\\leq d)$ do not contain $u$. The $\\sigma^{\\ast}$-polynomial $I_{d}$ is called the initial of $A$; it is denoted by $I_{A}$.A set $\\mathcal{A}\\subseteq K\\{y_{1},\\dots, y_{n}\\}^{\\ast}$ is said to be autoreduced if either it is empty or $\\mathcal{A}\\bigcap K = \\emptyset$ and every element of $\\mathcal{A}$ is reduced with respect to all other elements of the set $\\mathcal{A}$.\\begin{thm}Let $\\mathcal{A} = \\{A_{1},\\dots, A_{p}\\}$ be an autoreduced subset in the ring of $\\sigma^{\\ast}$-polynomials $K\\{y_{1},\\dots, y_{n}\\}^{\\ast}$ and let $D\\in K\\{y_{1},\\dots, y_{n}\\}^{\\ast}$. Furthermore, let $I(\\mathcal{A})$ denote the set of all $\\sigma^{\\ast}$-polynomials $B\\in K\\{y_{1},\\dots, y_{n}\\}^{\\ast}$ such that either $B =1$ or $B$ is a product of finitely many polynomials of the form $\\gamma(I_{A_{i}})$ where $\\gamma \\in \\Gamma,\\, i=1,\\dots, p$. Then there exist $\\sigma^{\\ast}$-polynomials $J\\in I(\\mathcal{A})$ and $D_{0}\\in K\\{y_{1},\\dots, y_{n}\\}^{\\ast}$ such that $D_{0}$ is reduced with respect to $\\mathcal{A}$ and $JD\\equiv D_{0} (mod\\, [\\mathcal{A}])$.\\end{thm}\\begin']",,"[""\\section{Preliminaries}Throughout the paper, $\\mathbb{N}, \\mathbb{Z}$, $\\mathbb{Q}$, and $\\mathbb{R}$ denote the sets of all non-negative integers, integers, rational numbers, and real numbers, respectively. The number of elements of a set $A$ is denoted by $\\Card A$. As usual, $\\mathbb{Q}[t]$ denotes the ring of polynomials in one variable $t$ with rational coefficients. All fields considered in the paper are supposed to be of characteristic zero. Every ring homomorphism is unitary (maps unity onto unity), every subring of a ring contains the unity of the ring.If $B = A_{1}\\times\\dots\\times A_{k}$ is a Cartesian product of $k$ ordered sets with orders $\\leq_{1},\\dots \\leq_{k}$, respectively ($k\\in \\mathbb{N}$, $k\\geq 1$), then by the product order on $B$ we mean a partial order $\\leq_{P}$ such that $(a_{1},\\dots, a_{k})\\leq_{P}(a'_{1},\\dots, a'_{k})$ if and only if $a_{i}\\leq_{i}a'_{i}$ for $i=1,\\dots, k$. In particular, if $a = (a_{1},\\dots, a_{k}), \\,a'=(a'_{1},\\dots, a'_{k})\\in \\mathbb{N}^{k}$, then $a\\leq_{P}a'$ if and only if $a_{i}\\leq a'_{i}$ for $i=1,\\dots, k$. We write $a <_{P}a'$ if $a\\leq_{P}a'$ and $a\\neq a'$. The lexicographic order on $\\mathbb{N}^{k}$ is denoted by $\\leq_{lex}$. If it is strict, we use the symbol $<_{lex}$.In this section we present some background material needed for the rest of the paper. <!--TRUNCATED-->""]"
,,,"[""Consider the symmetric difference scheme for the diffusion equation (4.1) obtained by replacing the partial derivatives $\\D\\frac{\\partial^{2} u(x,t)}{\\partial x^{2}}$ and $\\D\\frac{\\partial u(x,t)}{\\partial t}$ with $\\D\\frac{u(x + h, t) - 2u(x, t) + u(x-h, t)}{h^{2}}$ and $\\D\\frac{u(x, t + h) - u(x, t-h)}{2h}$, respectively. It leads to the equation in finite differences \\begin{equation} u(x, t+h) - u(x, t-h) = a(u(x+h, t) - 2u(x, t) + u(x-h, t)) \\end{equation} where $a = 2c/h$. As in the case of the forward difference scheme, let $K$ be an inversive difference functional field with basic set $\\sigma = \\{\\alpha_{1}:f(x, t)\\mapsto f(x+h, t),\\, \\alpha_{2}:f(x, t)\\mapsto f(x, t+h)\\}$ ($f(x, t)\\in K$) and let $K\\{y\\}^{\\ast}$ be the ring of $\\sigma^{\\ast}$-polynomials in one $\\sigma^{\\ast}$-indeterminate $y$ over $K$ ($y$ is treated as the unknown function $u(x, t)$;  we also assume that $a\\in K$). Then the equation (4.4) can be written as \\begin{equation} a\\alpha_{1}y + a\\alpha_{1}^{-1}y - \\alpha_{2}y  + \\alpha_{2}^{-1}y - 2ay = 0. \\end{equation} By Proposition 3.11, the characteristic set of the $\\sigma^{\\ast}$-ideal generated by the $\\sigma^{\\ast}$-polynomial $B = a\\alpha_{1}y + a\\alpha_{1}^{-1}y - \\alpha_{2}y  + \\alpha_{2}^{-1}y - 2ay$ is $\\{B, \\alpha_{1}^{-1}B\\}$. The leaders of $B$ and $\\alpha_{1}^{-1}B$ are $\\alpha_{1}y$ and $\\alpha_{1}^{-2}y$, respectively. Now Theorem 3.12 shows that the strength of the equation (4.5) is expressed by the dimension polynomial $\\phi_{E}(t)$ of the set $E = \\{(1, 0), (-2, 0)\\}\\subseteq\\mathbb{Z}^{2}$ (see Theorem 2.5). By Theorem 2.6, this polynomial coincides with the dimension polynomial $\\omega_{E'}(t)$ of the set $$E' = \\{(1, 0, 0, 0), (0, 0, 2, 0), (1, 0, 1, 0), (0, 1, 0, 1)\\}\\subseteq\\mathbb{N}^{4}.$$ Applying formula (2.3) we obtain that the strength of the equation (4.5), which expresses the symmetric difference scheme for (4.1), is represented by the $\\sigma^{\\ast}$-dimension polynomial $$\\psi_{Symm}(t) = 4t.$$<!--TRUNCATED-->""]"
"['\\section{Introduction}\\label{generalapproach}A cluster algebra is an algebra with a distinguished set of generators, cluster variables, that appear in sets called seeds, \\cite{clusteri}. Each seed is obtained recursively, by a process called mutation, from an initial seed. In our work there is a quiver associated with each seed, with $N+1$ vertices, without loops or $2-$cycles. It is a theorem of \\cite{clusterii} that there are finitely many cluster variables if and only if the quiver associated to one of the seeds is of simply laced Dynkin type. From a quiver without cycles we construct a frieze \\cite{frises} by a sequence of mutations at only sinks or sources. These mutations give a coupled system of $N+1$ recurrences\\begin{equation}\\label{generalfrieze}X^k_{n+1}X^k_n=1+\\left(\\prod_{i\\rightarrow k}X_n^i\\right)\\left(\\prod_{i\\leftarrow k}X_{n+1}^i\\right)\\end{equation}which define the frieze sequence. Here the products are taken over neighbours with arrows in to and out of $k$ respectively in the initial quiver. The superscript runs over each $k=1,\\ldots,N+1$, and the subscript $n$ denotes the iterates of the recurrence. We consider the frieze variables with superscript $k$ to be ``living"" at vertex $k$. To see the frieze as a discrete dynamical system we set initial variables $X_0^k$ for each vertex $k$, and define recursively $X_{n+1}^k$ by Equation \\ref{generalfrieze}. The map\\begin{equation}\\label{generalrecursion}\\varphi:\\begin{pmatrix}X^1_n \\\\X^2_n \\\\\\vdots \\\\X^{N+1}_n\\end{pmatrix}\\mapsto\\begin{pmatrix}X^1_{n+1} \\\\X^2_{n+1} \\\\\\vdots \\\\X^{N+1}_{n+1}\\end{pmatrix}\\end{equation}is called the cluster map. The form of cluster mutation means that each new variable can be written as a birational function of the previous ones, and since (\\ref{generalfrieze}) is obtained as a composition of mutations the map (\\ref{generalrecursion}) is birational. This induces an automorphism $\\varphi^*$, a ``shift"", on the field of rational functions\\[\\mathbb{C}(\\{X^k_0\\}_{k=1,\\ldots,N+1})\\] by $\\varphi^*(X^k_n)=X^k_{n+1}$ for each $n\\in\\mathbb{Z}$. We say an element of this field is period $p$ if it is fixed by $(\\varphi^*)^p$. Nonconstant period one elements are called invariants or first integrals. It was proved in \\cite{frises} that if there are linear relations between the variables in the frieze sequence then the corresponding diagram is either affine or Dynkin and is simply laced. These are shown in Figure \\ref{affinediagrams}\\begin{figure}\\begin{align*}&\\tilde{A}_N &&\\begin{tikzpicture}[start chain,node distance=1ex and 2em]\\mnode{1}\\mnode{1}\\dydots\\mnode{1}\\mnode{1}\\begin{scope}[start chain=br going above]\\chainin(chain-3);\\node[ch,join=with chain-1,join=with chain-5,label={above:{1}}] {};\\end{scope}\\end{tikzpicture}\\\\%&\\tilde{D}_N (N \\ge 4) &&\\begin{tikzpicture}\\begin{scope}[start chain]\\mnode{1}\\mnode{2}\\mnode{2}\\dydots\\mnode{2}\\mnode{1}\\end{scope}\\begin{scope}[start chain=br going above]\\chainin(chain-2);\\mnoder{1};\\end{scope}\\begin{scope}[start chain=br2 going above]\\chainin(chain-5);\\mnoder{1};\\end{scope']","[""\\section{Method of proof for the $\\tilde{D}$ and $\\tilde{E}$ quivers}\\label{methodofproofsection}Here we'll discuss our general process for finding periodic quantities for the frieze sequences. We show how to find constant coefficient linear relations from these. We orient the $\\tilde{D}$ and $\\tilde{E}$ quivers such that every vertex is either a sink or a source. This allows us to construct the bipartite belt frieze described in Subsection \\ref{bipartitebeltsubsection}.The key to our results, for $\\tilde{D}$ type, is to write each of the mutation relations (\\ref{friezemutations}) as a $2\\times 2$ matrix with determinant $-1$:\\begin{equation}\\label{clustermutationdeterminantform}-1=\\begin{vmatrix}\\star & X^k_{n+1}  \\\\X^k_n &\\star\\end{vmatrix},\\end{equation}where the product of the stars will give the product inside the brackets in (\\ref{friezemutations}). For the $\\tilde{E}$ quivers we have taken the determinant in (\\ref{clustermutationdeterminantform}) to be $+1$, but the procedure is equivalent, up to swapping matrix columns. We construct a $3\\times 3$ matrix such that each connected $2\\times 2$ determinant inside is of the form (\\ref{clustermutationdeterminantform}). By the following lemma, part of the Dodgson condensation algorithm \\cite{dodgson}, the $3\\times 3$ matrix will have determinant zero. \\begin{lemma}\\label{dodgsonlemma}For general $x_{ij}$ we have \\begin{equation}\\label{dodgson}\\begin{vmatrix}x_{0,0} & x_{0,1} & x_{0,2} \\\\x_{1,0} & x_{1,1} & x_{1,2} \\\\x_{2,0} & x_{2,1} & x_{2,2}  \\end{vmatrix}x_{1,1}=\\begin{vmatrix}x_{0,0} & x_{0,1} \\\\x_{1,0} & x_{1,1}\\end{vmatrix}\\begin{vmatrix}x_{1,1} & x_{1,2} \\\\x_{2,1} & x_{2,2}\\end{vmatrix}-\\begin{vmatrix}x_{0,1} & x_{0,2} \\\\x_{1,1} & x_{1,2} \\end{vmatrix}\\begin{vmatrix}x_{1,0} & x_{1,1} \\\\x_{2,0} & x_{2,1}\\end{vmatrix}.\\end{equation}\\end{lemma}In our cases the $3\\times 3$ matrix will have a kernel vector of the form $(1,-\\alpha,1)^T$. We then add rows to the bottom (or top) of this matrix that annihilate this vector. Our main method for doing this will be such that, using (\\ref{clustermutationdeterminantform}), the new row will create two new connected $2 \\times 2$ matrices with equal determinant. By the following lemma this newly formed $4\\times 3$ matrix will have the same kernel vector.\\begin{lemma}\\label{kernelpreservation}Let $M$ be a $3 \\times 3$ matrix \\[M:=\\begin{pmatrix}x_{0,0} & x_{0,1} & x_{0,2} \\\\x_{1,0} & x_{1,1} & x_{1,2} \\\\x_{2,0} & x_{2,1} & x_{2,2}  \\end{pmatrix}\\]such that $(1,-\\alpha,1)^T$ is in the kernel of the first two rows. We define the determinants $\\delta_{i,j}$ by\\[\\delta_{i,j}:=\\begin{vmatrix}x_{i,j} & x_{i,j+1} \\\\x_{i+1,j} & x_{i+1,j+1}\\end{vmatrix}.\\] If $x_{1,1}\\""]",,"[""\\section{Background material on cluster algebras and integrability}\\label{backgroundmaterial}Here we briefly recall some notions from cluster algebras and discrete integrable systems. The following definitions largely follow \\cite{fominintroduction} and \\cite{fordy}. Here by a cluster algebra we mean a cluster algebra without coefficients or frozen variables. A quiver $Q$ is a directed graph where multiple edges are allowed. We disallow loops or 2-cycles. Quiver mutation $\\mu_k$ at some vertex $k$ is defined in $3$ steps: \\begin{enumerate}\\item For each length two path $i\\rightarrow k \\rightarrow j$ add a new arrow $i\\rightarrow j$.\\item Reverse the direction of all arrows entering or exiting $k$.\\item Delete all 2-cycles that have appeared.\\end{enumerate}The exchange matrix for a quiver $Q$ is the skew-symmetric matrix $B$ with entries $b_{ij}$ which is the number of arrows from $i$ to $j$, with $b_{ji}=-b_{ij}$. In addition to this, we will also attach a cluster variable $x_i$ at each vertex $i$. Cluster mutation at vertex $k$, also denoted $\\mu_k$, fixes all variables $x_i$ with $i\\neq k$ but transforms $x_k$ as follows:\\begin{equation}\\label{clustermutationformula}x'_k:=\\mu_k(x_k):=\\frac{1}{x_k}\\left(\\prod_{i\\rightarrow k} x_j+\\prod_{i\\leftarrow k}x_i\\right).\\end{equation}Here the two products run over the vertices with arrows into and out of $k$ respectively. We consider $\\mu_k$ as a mutation both of the quiver and of the cluster variables. One can see that if there is no edge between vertices $k$ and $k'$ then $\\mu_k$ and $\\mu_{k'}$ commute.In the special case where the vertex $k$ is a sink or a source, the mutation simply reverses the arrows at $k$ and (\\ref{clustermutationformula}) takes a simpler form, with one of the products being empty.<!--TRUNCATED-->""]"
,,,
"['\\section*{Introduction}It is well-known that the bihamiltonian property proved to be very useful in the study of integrable systems. Since its invention by Magri in the context of the KdV equation a second hamiltonian structure was discovered for the majority of important examples. By a bihamiltonian property we mean the existence of second Poisson structure, compatible with the first one (canonically defined), such that the vector field defining the evolution of the system is hamiltonian in two ways, i.e. with respect to both the Poisson structures.Recall that Poisson bivectors $\\pi_1$ and $\\pi_2$ on a smooth manifold $M$ are called \\emph{compatible} if $\\pi_1+\\pi_2$ is a Poisson bivector, or, equivalently, $[\\pi_1,\\pi_2]=0$, where $[,]$ stands for the Schouten bracket. Any pair $(\\pi_1,\\pi_2)$ of compatible Poisson bivectors generates a two-dimensional linear space of Poisson bivectors $\\{\\la_1\\pi_1+\\la_2\\pi_2\\}_{(\\la_1,\\la_2)\\in\\K^2}$ which is called a \\emph{Poisson pencil} (here $\\K$ is the ground field).By linear, quadratic etc. polyvectors on a vector space we mean polyvectors with {\\em homogeneous} linear, quadratic etc. coefficients with respect to some linear system of coordinates.We will say that a Poisson pencil is \\emph{linear-quadratic} if its generators are linear and quadratic bivectors on some vector space respectively. The cases of ``linear-constant\'\' and ``linear-linear\'\' Poisson pencils are already very important, see for instance \\cite{bols\'}, \\cite{pBi-Lie} and references therein. The linear-quadratic Poisson pencils are the main objects of this paper.There are basically two classes of linear-quadratic Poisson pencils known in the literature. The first one can be described as follows (\\cite{doninGurevich}, \\cite{gurevichPanyushev}, \\cite{gurevichRubtsov}). Let $r\\in\\g\\wedge\\g$ be a skew-symmetric solution to the classical Yang--Baxter equation on a Lie algebra $\\g$, i.e. an ""algebraic Poisson bivector"" on $\\g$. Assume that a linear representation of $\\g$ in a vector space $V$ is given and $\\xi_1,\\ldots,\\xi_n$ are the fundamental vector fields of this representation corresponding to a basis $e_1,\\ldots,e_n$ of $\\g$. Then, if $r=r^{ij}e_i\\wedge e_j$, the bivector $\\pi_2:=r^{ij}\\xi_i\\wedge \\xi_j$ is a quadratic Poisson bivector on $V$. If, moreover, another bivector $\\pi_1$ on $V$ is given, invariant under the action of $\\g$, the bivectors $\\pi_1$ and $\\pi_2$ are compatible. In particular, if $V=\\g^*$ with the canonical Lie--Poisson bivector $\\pi_1$, and the representation is the coadjoint one, we get a linear-quadratic Poisson pair $(\\pi_1,\\pi_2)$, where $\\pi_2=r^{ij}\\pi_1(x_i)\\wedge \\pi_1(x_j)$ (here $x_i$ are the corresponding coordinates on $\\g^*$).<!--TRUNCATED-->']",,,"[""\\section{Preliminaries on Poisson pencils}\\label{PP}If $M$ is a real or complex analytic manifold, ${\\cal E}(M)$ will standfor the space of  analytic  functions on $M$ in the corresponding category.We say that a Poisson pencil  $\\{\\la_1\\pi_1+\\la_2\\pi_2\\}$ on a  manifold $M$ is of \\emph{Gelfand--Zakharevich type} if $\\rank(\\la_1\\pi_1+\\la_2\\pi_2)|_x<\\dim M$ for any $(\\la_1,\\la_2)$ and any $x\\in M$.  In this paper we consider only Poisson pencils of Gelfand--Zakharevich type with the following additional property: there exist an open dense set $U \\subset M$ and a collection of functions $C_1,\\ldots,C_r\\in{\\cal E}(M)$, where $r=\\min_{x\\in M}\\corank(\\la_1\\pi_1+\\la_2\\pi_2)|_x$, which are functionally independent on $U$ and are global Casimir functions of the bivector $\\pi_1$, i.e. $\\pi_1(C_i):=[\\pi_1,C_i]=0$ on $M$ (cf. footnote \\ref{fn}). Such a property holds for instance for the Lie--Poisson structure $\\pi_1$ on $\\g^*$, where $\\g$ is a semisimple or reductive Lie algebra.\\begin{defi}\\label{defML} \\rm Let $\\{\\la_1\\pi_1+\\la_2\\pi_2\\}$ be a Poisson pencil.An infinite sequence of functions $f_0,f_1,\\ldots\\in{\\cal E}(M)$, is called a \\emph{Magri--Lenard chain} if it satisfies the relations\\begin{equation}\\label{eq04} \\pi_1(f_0)=0,\\pi_1(f_1)+\\pi_2(f_0)=0,\\ldots,\\pi_1(f_{i+1})+\\pi_2(f_i)=0,\\ldots.\\end{equation}\\end{defi}Note that  $f_0,f_1,\\ldots$ is a Magri--Lenard chain if and only if  the formal series $f(\\la)=f_0+\\la f_1+\\cdots$ is a formal Casimir function of the bivector $\\pi_1+\\la\\pi_2$ (cf. the notions of ``anchored formal $\\la$-family'' and ``anchored Lenard scheme'' in \\cite[Sect. 10]{gz4}). Obviously, if $f_0,f_1,\\ldots$ is a Magri--Lenard chain such that $f_k\\not=0$ and $f_i=0$ for $i>k$, then $f(\\la)$ is a Casimir function of $\\pi_1+\\la\\pi_2$ and $f_k$ is a Casimir function of the bivector $\\pi_2$.\\begin{rema}\\label{invv}\\rm By \\cite{gz4}[Prop. 10.7], given two Magri--Lenard chains $f_0,f_1,\\ldots$ and $g_0,g_1,\\ldots$, all their members commute with each other with respect to the Poisson bracket corresponding to any $\\la_1\\pi_1+\\la_2\\pi_2$.\\end{rema}<!--TRUNCATED-->""]"
,,,"['\\section{10-parametric family of CL quadratic bivectors on $\\gl(3)^*$}\\label{ser}Let $P$ be the canonical Lie--Poisson bivector on $\\sl(3,\\K)^*$, $\\K=\\R,\\C$.\\begin{theo}\\label{mainId}Let $X_0,\\ldots,X_{9}$ be the generators of the weight spaces of 10-dimensional irreducible representation $W^*$ described in Section \\ref{s10} and Appendix. Then for any $b=(b_0,\\ldots,b_{9})\\in\\K^{10}$ there exists a quadratic polynomial $Q_b\\in U$ such that\\begin{equation}\\label{id1}[[X_b,P],[X_b,P]]=2P(Q_b)\\wedge P,\\end{equation}where $X_b:=b_0X_0+\\cdots+b_{9}X_{9}$. The explicit formula for $Q_b$ is given in Appendix\\footnote{Although we defined the modules $W^*$ and $U$ over $\\C$, all the coefficients in the formulas in Appendix are real, hence the theorem is valid over $\\R$ as well.}.\\end{theo}']"
"['\\section{Introduction}Interest to Lie algebras of vector fields goes back to Sophus Lie in his study of differential operators. The importance of Lie algebras of vector fields in geometry comes from the classical result of Shanks and Pursell that the smooth structure on a manifold is determined by the Lie algebras of smooth vector fields on it. Lie algebras of algebraic vector fields were studied extensively throughout the years. These are the Lie algebras of vector fields which are modules over the corresponding rings of functions. Well known four Cartan type Lie algebras are important examples of $\\mathbb Z$-graded infinite dimensional Lie algebras of finite growth.In this paper we focus on a class of Lie algebras of linear vector fields. They are realized by differential operators of degree at most 1. For any Lie algebra which admits a biorthogonal system (e.g. any separable locally convex Hausdorff Lie algebra) we construct an embedding to Lie algebras of linear vector fields, the same can be done for any suitable representation. The constructed embedding into the Lie algebra of linear vector fields resembles the classical Jordan-Schwinger map. We provide various examples of Lie algebras of linear vector fields which arise via such construction. In particular, a Lie algebra of linear vector fields can be associated with any Riemannian manifold $M$ and a Hilbert space of square integrable vector fields. The case $M=\\mathbb{R}$ leads to the well known class of twisted Heisenberg-Virasoro algebras. We use the construction to obtain a class of representations by linear vector fields for the Schr\\""odinger-Virasoro Lie algebras. Finally, we generalize our construction for arbitrary locally convex topological algebra and their homotopes and obtain an embedding into the Cuntz algebra. We also give an explicit representation by linear vector fields for any convex topological finite dimensional algebra using the representation of Cuntz algebra constructed by Dutkay. <!--TRUNCATED-->']","['\\section{Preliminaries}All vector spaces are considered over the field $\\mathbb{R}$ of real numbers. Assume that $V$ and $W$ are topological vector spaces in duality with pairing $<\\cdot,\\cdot>_{V,W}$, $V$ is separable and there exists a biorthogonal system $\\{e_k,f_k\\}_{k=1}^{\\infty}$, where $\\{e_k\\}_{k=1}^{\\infty}\\subset V$, $\\{f_k\\}_{k=1}^{\\infty}\\subset W$. Let $\\mathbb{R}[\\overline{x}]$ be the space of real polynomials in infinitely many variables. Denote by $\\mathcal A$ the Weyl algebra with generators $x_1, x_2, \\ldots$ and $\\partial_1, \\partial_2, \\ldots$ subject to the relations $\\partial_i x_j- x_j \\partial_i=\\delta_{ij}$. Let $\\widehat{\\mathcal A}_l$ be the subspace of $\\widehat{\\mathcal A}$ consisting of linear differential operators.\\section{Linear differential operators}Define key operators $D=D_V:\\mathcal{L}(V,W)\\to \\widehat{\\mathcal A}_l$, $\\partial:V\\to \\widehat{\\mathcal A}_l$, $\\bar{\\partial}:W\\to \\widehat{\\mathcal A}_l$ as:\\begin{equation}D(A):= \\sum\\limits_{\\alpha,\\beta\\in \\mathbb{N}}<Ae_{\\alpha},f_{\\beta}> x_{\\alpha}\\frac{\\partial}{\\partial x_{\\beta}}\\end{equation}\\begin{equation}\\partial(h):= \\sum\\limits_{\\alpha\\in \\mathbb{N}}<h,f_{\\alpha}> \\frac{\\partial}{\\partial x_{\\alpha}}\\end{equation}\\begin{equation}\\bar{\\partial}(r):= \\sum\\limits_{\\alpha\\in \\mathbb{N}}<e_{\\alpha},r> x_{\\alpha}\\end{equation}These operators satisfy important commutation relations:\\begin{eqnarray}&[D(A),D(B)] = D([B,A])\\\\&[\\partial(f),D(A)] = \\partial(Af)\\\\&[\\partial(f),\\partial(g)] = 0\\\\&[D(A),\\bar{\\partial}(r)] = \\bar{\\partial}(A^*r)\\end{eqnarray}For any Lie algebra $\\mathfrak g$ satisfying the assumptions, there exists an embedding of ${\\mathfrak g}/Cent({\\mathfrak g})$ into the semidirect product $\\tilde{D}({\\mathfrak g})\\ltimes \\partial({\\mathfrak g})\\subset End(\\mathbb{R}[\\overline{x}])$ of linear differential operators.\\section{Generalization to homotopes}For topological algebras, define mappings to Cuntz algebra $\\mathcal{O}_{\\infty}$:\\begin{equation}D(A):=\\sum\\limits_{\\alpha,\\beta\\in I}L^{\\beta}(Ae_{\\alpha}) s_{\\alpha}s_{\\beta}^*\\end{equation}\\begin{equation}\\partial(h):=\\sum\\limits_{\\alpha\\in I}L^{\\alpha}(h) s_{\\alpha}^*\\end{equation}\\begin{equation}\\bar{\\partial}(f):=\\sum\\limits_{\\alpha\\in I}<e_{\\alpha},f> s_{\\alpha}\\end{equation}These satisfy analogous commutation relations in the Cuntz algebra framework. For finite dimensional convex topological algebras, we construct explicit representations using wavelet representations of Cuntz algebras. <!--TRUNCATED-->']",,"[""\\section{Examples}\\subsection{Operators on $L^2(0,2\\pi)$}\\begin{trivlist}\\item[(1)]Let $H=\\{\\phi\\in L^2(0,2\\pi)\\| \\phi(0)=\\phi(2\\pi)=0\\}$ be a Hilbert space with scalar product $(f,g)_H=\\frac{1}{\\pi}\\int\\limits_{0}^{2\\pi}f(x)g(x)\\,dx,f,g\\in H$ and orthogonal basis $\\{e_n=\\sin (n\\cdot)\\}_{n=1}^{\\infty}$;$A: \\mathcal{D}(A)\\subset H\\mapsto H$, $\\mathcal{D}(A)=\\{\\phi\\in W^{2,2}(0,2\\pi)\\cap H  | \\ \\phi''(0)=\\phi''(2\\pi)=0\\}$, $Af=\\lambda\\frac{\\partial^2}{\\partial_x^2}+(1-\\lambda)v\\frac{\\partial}{\\partial_x},v\\in H$.Since $v\\in H$ we have that $v=\\sum\\limits_{m=1}^{\\infty}c_m e_m$ with$|v|_H=\\sum\\limits_{m=1}^{\\infty}c_m^2<\\infty$. Now we can deduce that\\[A e_m=-\\lambda m^2e_m+(1-\\lambda)\\sum\\limits_{n=1}^{\\infty}c_n e_n\\partial_x e_m\\]Consequently, we have\\[(Ae_m,e_k)_H=-\\lambda m^2\\delta_{mk}+(1-\\lambda)\\sum\\limits_{n=1}^{\\infty}c_n (e_n\\partial_x e_m,e_k)_H.\\]We can calculate that \\begin{eqnarray}(e_n\\partial_x e_m,e_k)_H &= \\frac{m}{\\pi}\\int\\limits_0^{2\\pi}\\sin nx\\cos mx\\sin kx,dx=\\frac{m}{2}(\\delta_{n,k-m}+\\delta_{n,k+m}-\\delta_{n,-k-m}-\\delta_{n,m-k})\\nonumber\\\\&=\\frac{m}{2}(\\delta_{n,k-m}+\\delta_{n,k+m}-\\delta_{n,m-k}), n,m,k\\in\\mathbb{N}.\\end{eqnarray}Hence,\\[(Ae_m,e_k)_H=-\\lambda m^2\\delta_{mk}+\\frac{m(1-\\lambda)}{2}(c_{m+k}+c_{k-m}-c_{m-k}),m,k\\in\\mathbb{N}\\]where we use notation $c_k:=0,k\\leq 0$.Now we can conclude that \\begin{eqnarray}D(A) &= \\sum\\limits_{m=1}^{\\infty}\\left(\\frac{(1-\\lambda)}{2}mc_{2m}-\\lambda m^2\\right)y_m\\partial_{y_m}\\nonumber\\\\&+\\frac{(1-\\lambda)}{2}\\sum\\limits_{n<m,m,n\\in\\mathbb{N}}c_{m+n}(my_m\\partial_{y_n}+n y_n \\partial_{y_m})\\nonumber\\\\&+c_{m-n}(n y_n \\partial_{y_m}-m y_m \\partial_{y_n})\\end{eqnarray}""]"
,,,"['\\bibitem{Billig2003} Y.Billig, {\\em Representations of the twisted Heisenberg-Virasoro algebra at level zero}, Canad. Math. Bull., 46 (2003), no.4, 529-537.\\bibitem{BilligFutorny2016} Y. Billig, V. Futorny, {\\em Classification of irreducible representations of Lie algebra of vectorfields on a torus}, J. Reine Angew. Math., 2016 (2016), 199-216.\\bibitem{BilligFutorny2018}  Y. Billig, V. Futorny, {\\em Lie algebras of vector fields on smooth affine varieties}, Commun. in Algebra 48 (2018), 3413-3429.\\bibitem{BilligNilsson2019} Y. Billig, J. Nilsson, {\\em Representations of the Lie algebra of vector fields on a sphere},Journal of Pure and Applied Algebra, 223 (2019),  3581-3593.']"
"['\\section{Introduction}Numerical nonlinear algebra is applied to maximum likelihood estimation for Gaussian models defined by linear constraints on the covariance matrix. We examine the generic case as well as special models (e.g.~Toeplitz, sparse, trees) that are of interest in statistics. We study the maximum likelihood degree and its dual analogue, and we introduce a new software package {\\tt LinearCovarianceModels.jl} for solving the score equations. All local maxima can thus be computed reliably. In addition we identify several scenarios for which the estimator is a rational function.In many statistical applications, the covariance matrix $\\Sigma$ has a special structure. A natural setting is that one imposes linear constraints on $\\Sigma$ or its inverse $\\Sigma^{-1}$. We here study models for Gaussians whose covariance matrix~$\\Sigma$ lies in a given linear~space. Such linear Gaussian covariance models were introduced by Anderson~\\cite{andersonLinearCovariance}. He was motivated by the Toeplitz structure of $\\Sigma$ in time series analysis. Recent applications of such models include repeated time series, longitudinal data, and a range of engineering problems~\\cite{pourahmadi1999joint}. Other occurrences are Brownian motion tree models \\cite{sturmfels2019brownian}, as well as pairwise independence models, where some entries of $\\Sigma$ are set to zero.We are interested in maximum likelihood estimation (MLE) for linear covariance models. This is a nonlinear algebraic optimization problem over a spectrahedral cone, namely the convex cone of positive definite matrices $\\Sigma$ satisfying the constraints. The objective function is not convex and can have multiple local maxima. Yet, if the sample size is large relative to the dimension, then the problem is essentially convex. This was shown in \\cite{ZUR14}. In general, however, the MLE problem is poorly understood, and there is a need for accurate methods that reliably identify all local maxima.Nonlinear algebra furnishes such a method, namely solving the score equations \\cite[Section 7.1]{Sul} using numerical homotopy continuation \\cite{SW}. This is guaranteed to find all critical points of the likelihood function and hence all local maxima. A key step is the knowledge of the maximum likelihood degree (ML degree). This is the number of complex critical points. The ML degree of a linear covariance model is an invariant of a linear space of symmetric matrices which is of interest in its own right. <!--TRUNCATED-->']","['\\section{Maximum likelihood estimator and its dual}Numerical nonlinear algebra is applied to maximum likelihood estimation for Gaussian models defined by linear constraints on the covariance matrix. We examine the generic case as well as special models (e.g.~Toeplitz, sparse, trees) that are of interest in statistics. We study the maximum likelihood degree and its dual analogue, and we introduce a new software package {\\tt LinearCovarianceModels.jl} for solving the score equations.Finding all the critical points of the log-likelihood function amounts to solving the following system of linear and quadratic equations in $2 \\cdot \\binom{n+1}{2}$ unknowns:\\begin{equation}\\label{eq:opt_system} \\Sigma\\in \\mathcal L, \\qquad\tK\\Sigma=I_n,\\qquad KSK-K\\in \\mathcal L^\\perp.\\end{equation}The dual maximum likelihood estimator is obtained by minimizing the Kullback-Leibler divergence with respect to $\\Sigma_0$ with $\\Sigma_1=S$. Equivalently, we set $W=S^{-1}$ and maximize:$$ \\ell^\\vee(\\Sigma)\\;=\\;\\log\\det\\Sigma-{\\rm tr}(W\\Sigma).$$The dual MLE requires solving the following system of equations:\\begin{equation}\\label{eq:opt_system2} \\Sigma\\in \\mathcal L, \\qquad\tK\\Sigma=I_n,\\qquad   K-W\\in \\mathcal L^\\perp.\\end{equation}The dual maximum likelihood estimator of a Gaussian linear covariance model is consistent, asymptotically normal, and first-order efficient. The advantage of the dual MLE is that it results in a unique maximum, since $\\Sigma \\mapsto \\ell^\\vee(\\Sigma)$ is a convex function on the positive definite cone $\\mathbb{S}^n_+$. The equations for the dual MLE are linear rather than quadratic, making them easier to solve with fewer solutions. <!--TRUNCATED-->']",,"['\\section{General Linear Constraints}The \\emph{maximum likelihood degree} of a linear covariance model $\\mathcal{L}$ is, by definition, the number of complex solutions to the likelihood equations \\eqref{eq:opt_system} for generic data $S$. This is abbreviated {\\em ML degree}; see \\cite[Section 7.1]{Sul}. To compute the ML degree, take $S$ to be a random symmetric $n \\times n$ matrix and count all complex critical points of the likelihood function $\\ell(\\Sigma)$ for $\\Sigma \\in \\mathcal{L}$. Equivalently, the ML degree of the model $\\mathcal{L}$ is the number of complex solutions $(\\Sigma, K)$ to the polynomial equations in~(\\ref{eq:opt_system}). We also consider the complex critical points of the dual likelihood function $\\ell^\\vee(\\Sigma)$. Their number, for a generic matrix $S \\in \\S^n$, is the {\\em dual ML degree} of $\\mathcal{L}$. It coincides with the number of complex solutions $(\\Sigma, K)$ to the polynomial equations in~(\\ref{eq:opt_system2}).Our ML degrees can be computed symbolically in a computer algebra system that rests on Gr\\""obner bases. However, this approach is limited to small instances. To get further, we use the methods from numerical nonlinear algebra described in Section~\\ref{sec5}.We here focus on a generic $m$-dimensional linear subspace $\\mathcal{L}$ of $\\S^n$. In practice this means that a basis for $\\mathcal{L}$ is chosen by sampling $m$ matrices at random from $\\S^n$. <!--TRUNCATED-->']"
,,,"['For each model $\\mathcal{L}$, we generated $100$ sample covariance matrices $S$,and we solved the likelihood equations (\\ref{eq:opt_system}) using our software \\ {\\tt LinearCovarianceModels.jl}.The row \\emph{max} denotes the largest number of local maxima that was observed in these $100$ experiments. The row\\emph{multiple} gives the fraction of instances which resulted in two or more local maxima. These two numbers pertain to local maxima in $\\mathbb{S}^5$.The rows \\emph{max pd} and \\emph{multiple pd} are the analogues restricted to the positive definite cone $\\mathbb{S}^5_+$.<!--TRUNCATED-->']"
"[""\\section{Introduction}A topology on a set $X$ is the same as a projection (i.e. an idempotent linear operator)$cl:2^X\\to 2^X$ satisfying $A\\subset cl(A)$ for all $A\\subset X$. That's a good way to summarize Kuratowski's closure operator.Basic geometry on a set $X$ is a dot product $\\cdot:2^X\\times 2^X\\to \\{0,\\infty\\}$. Its equivalent form is an orthogonality relation on subsets of $X$. The optimal case is when the orthogonality relation satisfies a variant of parallel-perpendicular decomposition from linear algebra. Dot products are a special case of forms which act on arbitrary vectors based on a given set $X$.We show that this concept unifies small scale (topology, proximity spaces, uniform spaces) and large scale (coarse spaces, large scale spaces). Using forms we define large scale compactifications that generalize all well-known compactifications:Higson corona, Gromov boundary, the visual boundary of CAT(0)-spaces, \\v Cech-Stone compactification, Samuel-Smirnov compactification, and Freudenthal compactification. This allows to generalize many results in coarse topology from proper metric spaces to arbitrary metric spaces or even to arbitrary large scale spaces.\\begin{Example} (see \\ref{GActingOnMetricSpaces})Let $X$ be a metric space and let $G$ be a finite group acting isometrically on X. Then $X/G$ has the same asymptotic dimension as $X$.In case of proper metric spaces $X$, Theorem \\ref{GActingOnMetricSpaces}was proved by Daniel Kasprowski \\cite{Kasp}.\\end{Example}\\begin{Example} (see \\ref{GeneralCoarselyNTo1Thm})If $n\\ge 1$ and $f:X\\to Y$ is a coarsely $n$-to-$1$ bornologous map of large scale spaces, then $asdim(Y)\\leq asdim(Y)+n-1$.Theorem \\ref{GeneralCoarselyNTo1Thm} was proved by Austin-Virk in \\cite{AV} for proper metric spaces $X$ and $Y$.\\end{Example}<!--TRUNCATED-->""]",,,"[""\\section{Multilinear forms on sets}\\begin{Definition}The semi-group $\\{0,\\infty\\}$ has the following binary operation:\\\\1. $0+0=0$,\\\\2. $0+\\infty=\\infty+0=\\infty+\\infty=\\infty$.\\end{Definition}Recall that a \\textbf{bornology} $\\mathcal{B}$ on a set $X$ is any family of subsets closed under finite unions so that $B\\subset B'\\in\\mathcal{B}$ implies $B\\in\\mathcal{B}$.\\begin{Remark}Be aware that Wikipedia \\cite{WikiBornology} assumes additionally that each point of $X$ belongs to a bornology.\\end{Remark}Notice that bornologies $\\mathcal{B}$ on a set $X$ are identical with kernels of basic linear operators $\\omega:2^X\\to \\{0,\\infty\\}$, i.e. functions satisfying\\\\1. $\\omega(\\emptyset)=0$,\\\\2. $\\omega(C\\cup D)=\\omega(C)+\\omega(D)$ for all $C,D\\in 2^X$.That observation leads to the following generalization:\\begin{Definition}A \\textbf{$k$-vector} in $X$ ($k\\ge 1$) is a $k$-tuple $(C_1,\\ldots,C_k)$ of subsets in $X$. The set of all vectors in $X$ will be denoted by $Vect(X)$.The \\textbf{concatenation} $V_1\\ast V_2$ of a $k$-vector $V_1=(C_1,\\ldots,C_k)$and an $m$-vector $V_2=(D_1,\\ldots,D_m)$ is the $(k+m)$-vector$(C_1,\\ldots,C_k,D_1,\\ldots,D_m)$.A \\textbf{basic multilinear form} $\\omega$ on $X$ is a symmetric function on all $k$-vectors of $X$ ($k\\ge 1$) to $\\{0,\\infty\\}$ satisfying the following properties:\\\\1. $\\omega((C_1\\cup C_2)\\ast V)= \\omega(C_1\\ast V)+ \\omega(C_2\\ast V)$for any $k$-vector $V$ and any two $1$-vectors $C_1$, $C_2$,\\\\2. $\\omega(\\emptyset)=0$,\\\\3. $\\omega(C\\ast V)=\\omega(V)$ if one of the coordinates of $V$ is contained in $C$.\\end{Definition}<!--TRUNCATED-->""]"
,,,"[""\\begin{Definition}An \\textbf{orthogonality relation} on subsets of a set $X$ is a symmetric relation $\\perp$ satisfying the following properties:\\\\1. $\\emptyset \\perp X$,\\\\2. $A\\perp (C\\cup C')\\iff A\\perp C$ and $A\\perp C'$.\\end{Definition}\\begin{Observation}One can reduce the number of axioms by dropping symmetry and replacing Axiom 2 by\\\\2'. $A\\perp (C\\cup C')\\iff C\\perp A$ and $C'\\perp A$.\\end{Observation}\\begin{Example}\\label{OrthInducedByBornology}For every bornology $\\mathcal{B}$ on a set $X$ the relation $A\\perp C$ defined as $A\\cap C\\in \\mathcal{B}$ is an orthogonality relation.\\end{Example}<!--TRUNCATED-->""]"
,,,"['\\begin{Proposition}If $\\omega_{ss}$ is the basic topological form of $(X,d)$ and $\\omega_h$ is the hyperbolic form of $X$, then $\\omega_h$ and $\\omega_{ss}$ are compatible. \\end{Proposition}\\begin{proof}By \\ref{CharHypPerpendicularity}, $C\\perp X$ means $C\\subset X\\setminus N(X,r)$ for some $r > 0$. Notice $N(X,r)\\supset X\\setminus B(p,2r)$, so $C$ is $\\omega_h$-bounded if and only if $C$ is $d$-bounded, so $C$ clearly can be enlarged to an open $d$-bounded set.Also, $N(cl(C),r)=N(C,r)$ for each $r > 0$.\\end{proof}<!--TRUNCATED-->']"
,,,"['Suppose $\\omega(N_p)=\\omega(N_q)$ for all $p,q\\in X$ and $\\{[p,x_n]\\}_{n=1}^\\infty$ is a sequence of geodesics converging to a geodesic ray $l$ at $p$.If $\\{[q,x_n]\\}_{n=1}^\\infty$does not converge to a geodesic ray at $q$,then (by a double diagonal process) we can find two subsequences $C=\\{c_n\\}_{n=1}^\\infty$ and $D=\\{d_n\\}_{n=1}^\\infty$ of $\\{x_n\\}_{n=1}^\\infty$ such that $\\{[q,c_n]\\}_{n=1}^\\infty$ converges to a geodesic ray $l_C$ at $q$, $\\{[q,d_n]\\}_{n=1}^\\infty$ converges to a geodesic ray $l_D$ at $q$, and $l_C\\ne l_D$. Notice $\\omega(N_q)(l_C,l_D)=0$ but $\\omega(N_p)(l_C,l_D)=\\infty$.']"
"[""\\section{Introduction and the results}\\label{sec:1}In transcendental number theory, various authors have investigated necessary and sufficient conditions for the values of analytic functions at algebraic numbers to be algebraically independent. The earliest such result is the famous Lindemann-Weierstrass theorem, which asserts that the values $e^{\\alpha_1},\\ldots,e^{\\alpha_n}$ of the exponential function at algebraic numbers $\\alpha_1,\\ldots,\\alpha_n$ are algebraically independent if and only if $\\alpha_1,\\ldots,\\alpha_n$ are linearly independent over the rationals (cf. Shidlovskii \\cite{Shid}).Some complex or $p$-adic entire functions are known to have the notable property that their values and their derivatives of any order at any nonzero distinct algebraic numbers are algebraically independent. As the first such result, Nishioka established Theorem \\ref{thm:factorial} below. Before stating the theorem, we introduce some notation used throughout this paper.One of the main purpose of this paper is to construct an entire function of two variables which possesses the notable algbraic independence property such as the functions stated in Theorems \\ref{thm:factorial}--\\ref{thm:G} even for its partial derivatives. In what follows, we consider not only the complex case but also the $p$-adic case. Again let $p$ be $\\infty$ or a prime number and fix an algebraic number $a$ with $0<|a|_p<1$. Let $\\{R_k\\}_{k\\geq0}$ be a linear recurrence of nonnegative integers satisfying \\eqref{eq:LRS} and $\\Phi(X)$ the polynomial defined by \\eqref{eq:Phi(X)}. We note that the degree $n$ of $\\Phi(X)$ is greater than $1$. We define a two-variable function $\\Theta(x,y)$, the main object in this paper, by$$\\Theta(x,y):=\\sum_{k=0}^\\infty a^{R_k}x^k\\prod_{\\substack{j=0\\\\j\\neq k}}^\\infty\\left(1-a^{R_j}y\\right).$$By Remark \\ref{rem:Rk}, $\\Theta(x,y)$ is an entire function on $\\C_p\\times\\C_p$. We assert that $F(x)$ and $-G'(y)$ are specializations of $\\Theta(x,y)$. The following theorem, which establishes the algebraic independence of the ``direct product'' of the infinite sets treated in Theorems \\ref{thm:F} and \\ref{thm:G}, is the main theorem of the present paper. \\begin{thm}\\label{thm:main}Let $p$ be $\\infty$ or a prime number. Suppose that $\\{R_k\\}_{k\\geq0}$ satisfies the condition {\\rm (R)}$_p$. Then the infinite subset $$\\left\\{\\frac{\\partial^{l+m}\\Theta}{\\partial x^l\\partial y^m}(\\alpha,\\beta)\\relmiddle| \\alpha\\in\\Qbar^\\times,\\ \\beta\\in\\Qbar,\\ l\\geq0,\\ m\\geq N_\\beta\\right\\}{\\textstyle\\bigcup}\\left\\{G^{(N_\\beta)}(\\beta)\\relmiddle|\\beta\\in\\Qbar^\\times\\right\\}$$of $\\Qbar_p$ is algebraically independent over $\\Q$.\\end{thm}<!--TRUNCATED-->""]",,,"[""\\section{Proof of Theorem \\ref{thm:main}}\\label{sec:2}In this section, we deduce Theorem \\ref{thm:main} from Theorem \\ref{thm:main2}.\\begin{proof}[Proof of Theorem \\ref{thm:main}]Since$$G'(y)=\\prod_{j=0}^\\infty\\left(1-a^{R_j}y\\right)\\times\\sum_{k=0}^\\infty \\frac{-a^{R_k}}{1-a^{R_k}y}=G(y)(-H(1,y)),$$we see inductively that, for any $m\\geq1$,$$G^{(m)}(y)=G(y)A_m\\left(H(1,y),\\ldots,\\frac{\\partial^{m-1}H}{\\partial y^{m-1}}(1,y)\\right),$$where $A_m(X_1,\\ldots,X_m)\\in\\Z[X_1,\\ldots,X_m]$. Hence, by$$\\Theta(x,y)=\\prod_{j=0}^\\infty(1-a^{R_j}y)\\times\\sum_{k=0}^\\infty\\frac{a^{R_k}x^k}{1-a^{R_k}y}=G(y)H(x,y),$$we have\\begin{align}\\label{eq:Theta_H}&\\frac{\\partial^{l+m}\\Theta}{\\partial x^l\\partial y^m}(x,y)\\nonumber\\\\=\\ &G(y)\\frac{\\partial^{l+m}H}{\\partial x^l\\partial y^m}(x,y)\\nonumber\\\\&+G(y)B_m\\left(H(1,y),\\ldots,\\frac{\\partial^{m-1}H}{\\partial y^{m-1}}(1,y),\\frac{\\partial^l H}{\\partial x^l}(x,y),\\ldots,\\frac{\\partial^{l+m-1} H}{\\partial x^l\\partial y^{m-1}}(x,y)\\right)\\end{align}for any $l,m\\geq0$, where $B_m(X_1,\\ldots,X_m,Y_1,\\ldots,Y_m)\\in\\Z[X_1,\\ldots,X_m,Y_1,\\ldots,Y_m]$. Then, for any $l,m\\geq0$, we see that\\begin{align}\\label{eq:H_Theta}&\\frac{\\partial^{l+m}H}{\\partial x^l\\partial y^m}(x,y)\\nonumber\\\\=\\ &\\frac{1}{G(y)}\\frac{\\partial^{l+m}\\Theta}{\\partial x^l\\partial y^m}(x,y)\\nonumber\\\\+&\\ C_m\\left(\\frac{\\Theta(1,y)}{G(y)},\\ldots,\\frac{1}{G(y)}\\frac{\\partial^{m-1}\\Theta}{\\partial y^{m-1}}(1,y),\\frac{1}{G(y)}\\frac{\\partial^l\\Theta}{\\partial x^l}(x,y),\\ldots,\\frac{1}{G(y)}\\frac{\\partial^{l+m-1} \\Theta}{\\partial x^l\\partial y^{m-1}}(x,y)\\right),\\end{align}where $C_m(X_1,\\ldots,X_m,Y_1,\\ldots,Y_m)\\in\\Z[X_1,\\ldots,X_m,Y_1,\\ldots,Y_m]$. In particular, substituting $y=0$ into both sides of \\eqref{eq:Theta_H} and \\eqref{eq:H_Theta}, we see that, for any $l,m\\geq0$,\\begin{equation}\\label{eq:Theta_F_m}\\frac{\\partial^{l+m}\\Theta}{\\partial x^l\\partial y^m}(x,0)=F_{m}^{(l)}(x)+B_m\\left(F_0(1),\\ldots,F_{m-1}(1),F_0^{(l)}(x),\\ldots,F_{m-1}^{(l)}(x)\\right)\\end{equation}and\\begin{align}\\label{eq:F_m_Theta}&F_{m}^{(l)}(x)\\nonumber\\\\=\\ &\\frac{\\partial^{l+m}\\Theta}{\\partial x^l\\partial y^m}(x,0)+C_m\\left(\\Theta(1,0),\\ldots,\\frac{\\partial^{m-1}\\Theta}{\\partial y^{m-1}}(1,0),\\frac{\\partial^l\\Theta""]"
,,,"[""\\begin{lem}\\label{lem:prime}$V(\\bm{\\tau})$ is a prime ideal of $K[\\bt]$.\\end{lem}For the proof we use the following\\begin{lem}[Skolem-Lech-Mahler, cf. Nishioka \\cite{N}]\\label{lem:slm}Let $C$ be a field of characteristic $0$. Let $\\gamma_1,\\dots,\\gamma_s$ be nonzero distinct elements of $C$ and $P_1(X),\\ldots,P_s(X)\\in C[X]$ nonzero polynomials. Then, if $\\{k\\in\\N\\mid \\sum_{i=1}^sP_i(k)\\gamma_i^k=0\\}$ is an infinite set, then $\\gamma_i/\\gamma_j$ is a root of unity for some distinct $i,j$.\\end{lem}\\begin{proof}[Proof of Lemma \\ref{lem:prime}]We define a subset $\\mathcal{R}_1$ of $(K[\\bm{y};\\bm{x}'])^\\N$ by$$\\mathcal{R}_1:=\\Set{\\left\\{\\sum_{\\gamma\\in\\Gamma}p_{\\gamma}(k)\\gamma^k\\right\\}_{k\\geq0}\\ | \\begin{array}{l}\\Gamma {\\rm\\ is\\ a\\ finite\\ subset\\ of\\ }G{\\rm\\ independent\\ of\\ }k,\\\\p_\\gamma(Y)\\in(K[\\bm{y};\\bm{x}'])[Y]\\ (\\gamma\\in\\Gamma)\\end{array}}.$$Then $\\mathcal{R}_1$ forms a commutative ring including $K[\\bm{y};\\bm{x}']$ under termwise addition and multiplication. If we put $A^k=:(a_{ij}^{(k)})$, then $\\{a_{ij}^{(k)}\\}_{k\\geq0}\\in \\mathcal{R}_1$ for any $1\\leq i,j\\leq l$. Since $T_{\\bl\\m}(\\bm{\\tau};X;\\bm{y};\\bm{x}')\\in(\\Z[\\bm{y};\\bm{x}'])[\\{x_{ij}\\}]$, we have $\\{T_{\\bl\\m}(\\bm{\\tau};A^k;\\bm{y};\\bm{x}')\\}_{k\\geq0}\\in \\mathcal{R}_1$ for any $\\bl\\in\\La$ and $\\m\\in\\M$. Therefore, if $P(\\bt)\\in K[\\bt]$, then $\\{P(\\bm{T}(\\bm{\\tau};A^k;\\bm{y};\\bm{x}'))\\}_{k\\geq0}\\in \\mathcal{R}_1$, so that there exist a finite subset $\\Gamma=\\Gamma(P)$ of $G$ and nonzero polynomials $p_{\\gamma}(Y)\\in(K[\\bm{y};\\bm{x}'])[Y]$ $(\\gamma\\in\\Gamma)$ such that$$P(\\bm{T}(\\bm{\\tau};A^k;\\bm{y};\\bm{x}'))=\\sum_{\\gamma\\in\\Gamma}p_\\gamma(k)\\gamma^k$$for all $k\\geq0$.To prove the lemma, we let $P_1(\\bt),P_2(\\bt)\\in K[\\bt]$ and suppose that $P_1(\\bt)P_2(\\bt)\\in V(\\bm{\\tau})$. Since $P_1(\\bm{T}(\\bm{\\tau};A^k;\\bm{y};\\bm{x}'))P_2(\\bm{T}(\\bm{\\tau};A^k;\\bm{y};\\bm{x}'))=0$ for all $k\\geq0$, we may assume that $P_1(\\bm{T}(\\bm{\\tau};A^k;\\bm{y};\\bm{x}'))=0$ for infinitely many $k$. Hence, if $\\Gamma(P_1)\\neq\\emptyset$, then Lemma \\ref{lem:slm} implies that there exist distinct $\\gamma,\\gamma'\\in\\Gamma(P_1)$ such that $\\gamma/\\gamma'$ is a root of unity, which contradicts the fact that $G$ is torsion free. Thus $\\Gamma(P_1)=\\emptyset$ and $P_1(\\bt)\\in V(\\bm{\\tau})$.\\end{proof}<!--TRUNCATED-->""]"
"['\\section{Introduction}Due to the principle of quantum rigidity, quantum algebras exhibit few classical symmetries, i.e., linear group actions. For example, the automorphism group of the quantum plane $\\kk_q[u,v]$ with $q^2 \\neq 1$ is isomorphic to $(\\kk^\\times)^2$ \\cite{AC}. Here quantum algebras will not take on a specific meaning, but will generally be understood to represent some algebra whose relations depend on parameters in $\\kk$. This includes quantum affine spaces and quantum matrix algebras, both of which are fundamental objects in the study of noncommutative algebra and noncommutative algebraic geometry.In many cases, the (graded/filtered) automorphism group for quantum algebras are known, see, e.g., \\cite{AC,CPWZ2,y-llc}. The natural next step, then, is to study quantum symmetries, or actions by Hopf algebras. Semisimple Hopf actions on quantum planes and quantum Weyl algebras are well-understood \\cite{CWWZ,CKWZ}. Our goal is to better understand non-semisimple Hopf actions, specifically actions by pointed Hopf algebras, which themselves have attracted much recent interest \\cite{Cl,KW}.The impetus for this work was a classification by Won, Yee, and the second-named author of Taft algebra actions on quantum planes and quantum Weyl algebras \\cite{GWY}. Here we ask how much this classification can be extended. We do this in several ways. First, we look at actions of {\\it generalized} Taft algebras and find that the classification problem is not significantly different. Secondly, we consider actions on higher-dimensional algebras, specifically quantum affine spaces and quantum matrix algebras. Finally, we study actions of bosonizations of quantum linear spaces (see \\cite{AS2,EW}).Bosonizations of quantum linear spaces form an important subclass within the classification of finite-dimensional pointed Hopf algebras of Andruskiewitsch and Schneider \\cite{AS1}. In some sense, they may be thought of as higher rank generalized Taft algebras. Under mild hypotheses --- in particular, we require all parameters have order greater than 2 --- we classify actions of generalized Taft algebras on quantum affine spaces and quantum matrix algebras. This is then extended to determine all actions of bosonizations of quantum linear spaces, again under mild hypotheses. Specifically, we achieve bounds on the rank of these bosonizations. It is our hope that our methods may be applied for further classifications and a long term goal is to understand the classification of {\\it all} finite-dimensional pointed Hopf algebras on these algebras. <!--TRUNCATED-->']",,,"['\\section{Preliminaries}Throughout, $\\kk$ is an algebraically closed, characteristic zero field and all algebras are associative $\\kk$-algebras. All unadorned tensor products should be regarded as over $\\kk$. For a Hopf algebra $H$, and grouplike elements $g,h \\in G(H)$, we denote by $P_{g,h}$ the $(g,h)$-skew-primitive elements, i.e. all $x \\in H$ so that $\\Delta(x) = g \\otimes x + x \\otimes h$.An algebra $A$ is {\\sf ($\\NN$)-graded} if there exists a vector space decomposition $A = \\bigoplus_{i \\in \\NN} A_{(i)}$ such that ${A_{(i)} \\cdot A_{(j)} \\subset A_{(i+j)}}$.Further, $A$ is {\\sf connected} if $A_{(0)} = \\kk$, {\\sf affine} if $A_{(k)}$ is finite-dimensional as a $\\kk$-vector space for all $k$, and {\\sf generated in degree one} if $A_{(1)}$ generates $A$ as an algebra.With the exception of one family in Section \\ref{sec.add}, all algebras considered in this work are affine connected graded and generated in degree one. <!--TRUNCATED-->']"
,,,"['\\begin{lemma}\\label{lem.genpatch}Suppose $B(G,\\gu,\\cu)$ has rank $\\theta\\geq 2$, and that $B$ acts linearly and inner faithfully on $A=\\kk_\\bp[u_1,\\hdots,u_t]$.Assume $t$, $m_i$ for all $i$, and $\\ord(p_{ij})$ for all $i \\neq j$ are at least 3. Write $x_1=(\\eta_{ij})$ and $x_2=(\\mu_{ij})$.\\begin{enumerate}\\item If $\\eta_{ij},\\mu_{jk} \\neq 0$, then $k\\neq i$ and both $B_1$ and $B_2$ act as trivial extensions of an action on $A_{ijk}$.\\item If $\\eta_{ij},\\mu_{kj} \\neq 0$, then $\\lambda_1=\\lambda_2\\inv$.%\\item If $B_2$ acts as a trivial extension on $A_{ik}$, then $\\ord(p_{ik}p_{kj}) \\mid n_1$ and $\\ord(p_{jk}p_{ij}) \\mid n_2$.\\item There may be at most two $x_i$ with nonzero entries in the same column.\\end{enumerate}\\end{lemma}']"
,,,"['\\begin{proposition}\\label{prop:matrixactions2}    Let $q \\neq \\pm 1$ and $N,m \\geq 3$.    Then $T_n(\\lambda, m, 0)$ acts on $\\mc O_q(M_N(\\kk))$ with $x$ acting linearly and nonzero, and $g$ acting as an element of $\\mc H \\rtimes \\langle \\tau \\rangle$, if and only if $\\lambda = q^{\\pm 2}$ and $\\ord(q) \\mid n$.    The actions are given by Table \\ref{table.matn}. \\end{proposition}']"
,,,"['\\subsection{Quantum general and special linear groups}The {\\sf quantum determinant} of $\\qmn$ is the central element\\[ \\detq = \\sum_{\\pi \\in \\cS_N} (-q)^{\\ell(\\pi)} Y_{1,\\pi(1)} Y_{2,\\pi(2)} \\cdots Y_{n,\\pi(n)},\\]where $\\ell(\\pi)$ denotes the length of the permutation $\\pi$. In the case $n=2$, this is the element $AD-qBC$.The quantum determinant can then be used to define the corresponding {\\sf quantum general linear group} $\\qgl=\\qmn[\\detq\\inv]$ and {\\sf quantum special linear group} $\\qsl=\\qmn/(\\detq-1)$. \\begin{proposition}\\label{prop.slgl}Suppose $T_n(\\lambda,m,0)$ acts on $\\mc O_q(M_N(\\kk))$ according to \\begin{itemize}    \\item rows 1,2,4, or 5 of Table \\ref{table.mat2} in the case $N=2$, or    \\item rows 1,2,5, or 6 of Table \\ref{table.matn} in the case $N>2$.\\end{itemize}Then the action descends to an action on $\\mc O_q(\\SL_N(\\kk))$ and lifts to an action on $\\mc O_q(\\GL_N(\\kk))$.\\end{proposition}<!--TRUNCATED-->']"
"[""\\section{Introduction} \\label{section1}In the 1980's, Joyce \\cite{joyce} and Matveev \\cite{matveev} introduced the notion of {\\it quandle}. This notion has been derived from the knot theory, in the way that the axioms of a quandle are the algebraic expressions of Reidemeister moves (I,II,III) for an oriented knot diagram \\cite{elhamdadi}. The quandles provide many knot invariants. The fundamental quandle or knot quandle was introduced by Joyce who showed that it is a complete invariant of a knot (up to a weak equivalence). Racks which are a generalization of quandles were introduced by Brieskorn \\cite{bri} and Fenn and Rourke \\cite{fenn}.A rack is a non-empty set $\\mathit{X}$ together with a map $\\rhd : \\mathit{X} \\times \\mathit{X} \\longrightarrow \\mathit{X}$, $(a, b) \\mapsto a \\rhd b$ such that, for any $a,b,c\\in \\mathit{X}$, the map $\\mathrm{L}_{a} : \\mathit{X} \\longrightarrow \\mathit{X}$, $b \\mapsto a\\rhd b$ is a bijection and \\begin{equation}\\label{r1} a \\rhd (b \\rhd c) = (a \\rhd b)\\rhd (a \\rhd c).\\end{equation}A rack $\\mathit{X}$ is called $\\textit{pointed}$ if there exists a distinguished element $e \\, \\in \\mathit{X}$ such that, for any $a\\in \\mathit{X}$,\\begin{equation}\\label{r2} a \\rhd e = e \\esp \\mathrm{L}_{e}=\\mathrm{Id}_X.\\end{equation}A Lie rack is a rack $(X,\\rhd)$ such that $X$ is a smooth manifold, $\\rhd$ is a smooth map and the left translations $\\mathrm{L}_{a}$ are diffeomorphisms. Any Lie group $G$ has a Lie rack structure given by $g\\rhd h=g^{-1}hg$.{\\it Leibniz algebras} were first introduced and investigated in the papers of Bloh \\cite{bloh, bloh1} under the name of D-algebras. Then they were rediscovered by Loday \\cite{loday} who called them Leibniz algebras. A left Leibniz algebra is an algebra $(\\h,\\br)$ over a field $\\mathbb{K}$ such that, for every element $u\\in\\h$, $\\ad_u:\\h\\too\\h$, $v\\mapsto[u,v]$ is a derivation of $\\mathfrak{h}$, i.e., \\begin{equation}\\label{l1} [u,[v,w]]=[[u,v],w]+[v,[u,w]],\\quad v,w\\in\\h. \\end{equation}In 2004, Kinyon \\cite{kinyon} proved that if $(X,e)$ is a pointed Lie rack, $T_eX$ carries a structure of left Leibniz algebra. Moreover, in the case when the Lie rack structure is associated to a Lie group $G$ then the associated left Leibniz algebra is the Lie algebra of $G$.A {\\it linear} Lie rack structure on a finite dimensional vector space $V$ is a Lie rack operation $(x,y)\\mapsto x\\rhd y$ pointed at $0$ and such that for any $x$, the map $\\mathrm{L}_x:y\\mapsto x\\rhd y$ is linear. A linear Lie rack operation $\\rhd$ is called {\\it analytic} if for any $x,y\\in V$,\\begin{equation}\\label{eq4}x\\rhd y=y+\\sum_{n=1}^\\infty A_{n,1}(x,\\ldots,x,y),\\end{equation}where for each $n$, $A_{n,1}:V\\times\\ldots\\times V\\too V$ is an $n+1$-multilinear map which is symmetric in the $n$ first arguments. In this case, $A_{1,1}$ is the left Leibniz bracket associated to $\\rhd$.If $(\\h,\\br)$ is a left Leibniz algebra then the operation $\\stackrel{c}{\\rhd}:\\h\\times\\h\\too\\h$ given by $$u\\rhdl v""]",,,"['\\section{Some classes of non rigid left Leibniz algebras,  proofs of Proposition \\ref{pr1} and Theorem \\ref{main}} \\label{section2}The proof of Proposition \\ref{pr1} is a consequence of the following well-known result.\\begin{pr} \\label{pr2} Let $(X,\\rhd)$ be a rack and $J:X\\too X$ a map such that, for any $x,y\\in X$, $J(x\\rhd y)=x\\rhd J(y)$, i.e., $J\\circ \t\\mathrm{L}_x=\\mathrm{L}_x\\circ J$ for any $x$. Then the operation\t\\[ x\\rhd_Jy=J(x)\\rhd y \\]defines a rack structure on $X$.\\end{pr}\\begin{proof} We have, for any $x,y,z\\in X$,\t\\begin{eqnarray*}\tx\\rhd_J(y\\rhd_J z)&=&J(x)\\rhd(J(y)\\rhd z)\\\\\t&=&(J(x)\\rhd J(y))\\rhd (J(x)\\rhd z)\\\\\t&=&(J(J(x)\\rhd y))\\rhd (x\\rhd_Jz)\\\\\t&=&(x\\rhd_Jy)\\rhd_J(x\\rhd_Jz).\\qedhere\t\\end{eqnarray*}\\end{proof}\\noindent{\\bf Proof of Proposition \\ref{pr1}.}\\begin{proof} We consider the map $J:\\h\\too\\h$ given by $J(x)=F(P(x,\\ldots,x))x$. Since $P$ is invariant, we have $P(\\exp(\\ad_x)(y),\\ldots,\\exp(\\ad_x)(y))=P(y,\\ldots,y)$ and hence $J(x\\rhdl y)=x\\rhdl J(y)$ and one can apply Proposition \\ref{pr2} to conclude.\\end{proof}']"
,,,
"[""\\section{Introduction}In algebraic topology, an important resource for analyzing the stable homotopy groups of spheres is given by the unit map of the complex cobordism spectrum $\\mathrm{MU}$. This map has at least two features:1) it induces an isomorphism on $\\pi_0 = \\Z$;2) it detects nilpotence, giving rise to the field of chromatic homotopy theory. It would be interesting to know if similar techniques apply in motivic homotopy theory, for studying the motivic stable homotopy groups of spheres. There is not much yet known about motivic nilpotence phenomena (see the recent work of Bachmann and Hahn~\\cite{BHNilpotence}). On the other hand, the abelian group $\\pi_0$ of a spectrum is replaced by a richer invariant in motivic settings. For a motivic $\\Pp^1$-spectrum $\\E$ one considers a sequence of Nisnevich sheaves of abelian groups  $\\{\\underline{\\pi}_0(\\E)_l\\}_{l \\in \\Z}$, called a homotopy module. One may ask an analogous question: does the unit map of a motivic cobordism spectrum induce an isomorphism of homotopy modules? The first guess would be to consider the unit map of the algebraic cobordism spectrum $\\mgl$, which is the motivic analogue of $\\mathrm{MU}$, constructed by Voevodsky~\\cite{VoevodskyA1-HomThy}. As it turns out, the induced map on homotopy modules kills $\\eta$, the motivic Hopf element. More precisely, Hoyois has shown that the unit map of $\\mgl$ factors through the map $\\unit_S / \\eta \\to \\mgl$, which induces an isomorphism of homotopy modules~\\cite[Theorem~3.8]{HoyoisAlgCobordism}. One could ask if there is another algebraic cobordism spectrum \\glqq closer\\grqq  \\,to the motivic sphere spectrum $\\unit_S$. Indeed, for the algebraic special linear cobordism spectrum $\\msl$ (a motivic analogue of  $\\mathrm{MSU}$, constructed by Panin and Walter~\\cite{PW-MSLMSp}) the unit map induces an  isomorphism of homotopy modules. This can be shown by studying the geometry of oriented Grassmanians in a similar fashion to Hoyois' proof, as stated in~\\cite[Example~16.34]{BHNorms}. However, we would like to understand this comparison in an explicit way. In this paper, we interpret both homotopy modules in terms of geometric generators and relations, and then compare them directly. Classically, the celebrated Pontryagin-Thom theorem identifies the $n$-th stable homotopy group of spheres with the group of $n$-dimensional smooth compact manifolds equipped with a trivialization of the stable normal bundle (so called framing), modulo the bordism equivalence relation. An approach for getting an analogous result for motivic stable homotopy groups was suggested by Voevodsky in his unpublished notes~\\cite{VoevodskyNotesFrames}, where he introduced a notion of a a framed correspondence between smooth schemes $X$ and $Y$ over a base field $k$. In the simplest case $X = Y = \\spk$ his construction gives a geometric version of framed points in topology. In more detail, a framed correspondence $c$ of level $n \\geqslant 0$ is given by a closed subscheme $Z \\subset \\Aa^n_X$ (the support of $c$), finite over $X$; an \\'etale neighborhood $U$ of $Z$ in $\\Aa^n_X$; a morphism $\\phi\\colon U \\to \\Aa^n$, cutting out $Z$ as the preimage of $0$ (the framing of Z); and a morphism $g\\colon U \\to Y$. In a series of papers, Ananyevskiy, Druzhinin, Garkusha, Neshitov and Panin  developed a theory of framed motives~\\cite{GPFramedMotives}, \\cite{AGPCancellation}, \\cite{GPHomInvPresheaves}, \\cite{GNPRelativeMotivic""]",,,"['\\section{$E$-framed correspondences}In this section we recall the defintion of an $E$-framed correspondence from~\\cite[Section~2.2]{deloop3}\\footnote{In~\\textit{op. cit.} there was defined a stabilized version of $E$-framed correspondences, in a bigger generality, and they were called ``twisted equationally framed correspondences"".}, which generalizes Voevodsky\'s original definition of a framed correspondence~\\cite{VoevodskyNotesFrames}.We recall functoriality properties of $E$-framed correspondences and related notions, generalizing the properties of framed correspondences studied in~\\cite{GPFramedMotives}. Afterwards we recall from~\\cite{deloop3} the computation of infinite $\\Pp^1$-loop spaces of certain motivic Thom spectra via $E$-framed correspondences. \\subsection{Main definitions and functoriality}\\begin{df}\\label{def: E-framed cor}Let $X, Y$ be smooth $k$-schemes and $E$ a vector bundle over $Y$ of rank $r$. An \\textit{$E$-framed correspondence}  $c=(U, \\phi, g)$ of level $n \\in \\N$ from $X$ to $Y$ consists of the following data:\\begin{itemize}\\item a closed subscheme $Z \\subset \\Aa^{n+r}_X$, finite over $X$;\\item an \\\'etale neighborhood $p \\colon U \\to \\Aa^{n+r}_X$ of $Z$;\\item a morphism $(\\phi, g) \\colon U \\to \\Aa^n \\times E$ such that $Z$ as a closed subscheme of $U$ is the preimage of the zero section $z(0 \\times Y) \\subset \\Aa^n \\times E $.\\end{itemize}We say that $E$-framed correspondences $(U, \\phi, g)$ and  $(U\', \\phi\', g\')$  are equivalent if $Z = Z\'$ and  $(\\phi, g)$ coincides with $(\\phi\', g\')$ in an \\\'etale neighborhood of $Z$ refining both $U$ and $U\'$. We denote the set of $E$-framed correspondences modulo this equivalence relation as $\\Fr_{E, n}(X, Y) $; in the case $E=Y$ we write $\\Fr_{n}(X, Y)$. We call $Z$ the \\textit{support} of $c$ and $\\phi$ the \\textit{framing} of $Z$.\\end{df}<!--TRUNCATED-->']"
,"[""\\section{Computation of the unit map}In this section, we prove that the unit map $\\varepsilon_* \\colon H_0(\\ZF(\\del, \\Gg_m^{\\wedge *})) \\to H_0(\\ZF^{\\SL}(\\del, \\Gg_m^{\\wedge *}))$ is an isomorphism in characteristic $0$. To prove surjectivity, we construct explicit $\\Aa^1$-homotopies between framed correspondences. To proving injectivity, we employ the computation of $H_0(\\ZF(\\del, \\Gg_m^{\\wedge *}))$ by Neshitov~\\cite{NeshitovFramedCorrMW} and the theory of Milnor-Witt correspondences of Calm\\`es and Fasel~\\cite{CFFiniteMWCor}.\\subsection{Surjectivity of the unit map $\\varepsilon_*$}\\label{ssec: surjectivity}\\begin{notation}In this subsection we will use the following abbreviations.\\begin{itemize}\\item $L / k$ is a finite field extension.\\item $s \\in X(L)$ and the corresponding $L$-rational point of $X_L = X \\times \\spl$ are denoted the same way, for $X \\in \\smk$.\\item $c \\sim c'$ denotes equality of classes of $\\SL$-oriented linear framed correspondences $c$ and $c'$ in $H_0(\\ZF^{\\SL}(\\del, Y))$.\\end{itemize}\\end{notation}\\sssec{} Fix a smooth $k$-scheme $Y$. We will show that for any $c \\in \\ZF^{\\SL}_n(\\spk, Y)$ there is $c' \\in \\ZF_n(\\spk, Y)$ such that $c \\sim \\varepsilon(c')$ in $H_0(\\ZF^{\\SL}(\\del, Y))$. This result for $Y = \\Gg_m^l$ for all $l \\geqslant 0$ implies the surjectivity of the unit map $\\varepsilon_*$.We can assume that $c$ is represented by an $\\SL$-oriented framed correspondence with a connected support. That is, $\\supp(c)_{\\red} = \\spl$, where $L$ is some finite extension of $k$. We can also assume that $c$ is of level $n>0$. We will use the following preliminary lemmas, analogous to~\\cite[Section~2]{NeshitovFramedCorrMW}.\\begin{lm}\\label{lm:projection}Let $c = (U, \\phi, g)$ be a correspondence in $Fr_n^{\\SL}(\\spk, Y)$ with support $Z$ such that $Z_{\\red} = \\spl$. Then one can refine $U$ to $U'$, an \\'etale neighborhood of $Z$ such that there is a projection $U' \\to\\mathrm{Spec} \\, L$.\\end{lm}\\begin{proof}It is enough to show that there is a projection from the henselization  $(\\Aa^n_k)^h_Z$, so we can assume $Z = \\spl$. Since $L/k$ is a separable field extension, the projection $\\Aa^n_L \\to \\Aa^n_k$ is an \\'etale neighborhood of $Z$, so we can consider the composition of projections: $(\\Aa^n_k)^h_Z \\to \\Aa^n_L \\to \\spl$.\\end{proof}\\begin{lm}\\label{lm:sl}Let $c = (U, \\phi, g)$ be a correspondence in $Fr_n^{\\SL}(\\spk, Y)$. Assume there is a map $h \\colon U \\to \\spl$. Let $A \\in \\SL(L) = \\col_i \\SL_i(L)$ and assume there is given an action of $\\SL(L)$ on $\\TAU_{n, L}$, that induces an endomorphism of the zero section. Denote by $A \\cdot \\phi$ the composition $U \\xrightarrow{\\phi \\times h} \\TAU_{n, L} \\xrightarrow{A} \\TAU_{n, L} \\xrightarrow{pr} \\TAU_n$.Then $c \\sim c""]",,"[""\\subsection{Framed correspondences and Milnor-Witt K-theory}\\label{ssec: neshitov}To study the graded abelian group $H_0(\\ZF(\\Delta^{\\bullet}_k,  \\Gg_m^{\\wedge *}))$, one first defines a ring structure.  As shown in~\\cite[Section~3]{NeshitovFramedCorrMW}, the product of framed correspondences, defined in~\\eqref{sssec: external product}, descends to a product $$H_0(\\ZF(\\del \\times X, Y)) \\times H_0(\\ZF(\\del \\times X', Y')) \\to H_0(\\ZF(\\del \\times X \\times X', Y \\times Y'))$$for any $X$, $Y$, $X'$, $Y' \\in \\smk $.Taking $X = X' = \\spk$, $ Y = \\Gg_m^n, Y' =  \\Gg_m^m$, we get a multiplicative structure on the graded abelian group $H_0(\\ZF(\\del, \\Gg_m^*))$, which descends to a multiplication on $H_0(\\ZF(\\del, \\Gg_m^{\\wedge *}))$. The main result of~\\cite{NeshitovFramedCorrMW} is the following theorem.\\begin{theorem}[Neshitov]Let $k$ be a field of characteristic $0$. Then the following graded rings are isomorphic:$$H_0(\\ZF(\\del, \\Gg_m^{\\wedge *})) \\simeq \\K_{\\geqslant 0}^{MW}(k),$$where $\\K_{\\geqslant 0}^{MW}(k)$ denotes the non-negative part of the Milnor-Witt K-theory of the field $k$. \\end{theorem}<!--TRUNCATED-->""]"
"[""\\section{Introduction}If $A$ is an $n \\times n$ Hermitian matrix, we denote its $n$ real eigenvalues by $\\lambda_1(A),\\dots,\\lambda_n(A)$. The ordering of the eigenvalues will not be of importance in this survey, but for sake of concreteness let us adopt the convention of non-decreasing eigenvalues: $\\lambda_1(A) \\leq \\dots \\leq \\lambda_n(A)$. If $1 \\leq j \\leq n$, let $M_j$ denote the $n-1 \\times n-1$ minor formed from $A$ by deleting the $j^{\\mathrm{th}}$ row and column from $A$. This is again a Hermitian matrix, and thus has $n-1$ real eigenvalues $\\lambda_1(M_j),\\dots,\\lambda_{n-1}(M_j)$, which for sake of concreteness we again arrange in non-decreasing order.By the spectral theorem, we can find an orthonormal basis of eigenvectors $v_1,\\dots,v_n$ where the $v_i$ are in $\\mathbb C_n$ of $A$ associated to the eigenvalues $\\lambda_1(A),\\dots,\\lambda_n(A)$ respectively. For any $i,j=1,\\dots,n$, let $v_{i,j}$ denote the $j^{\\mathrm{th}}$ component of $v_i$. This survey paper is devoted to the following elegant relation, which we will call the \\emph{eigenvector-eigenvalue identity}, relating this eigenvector component to the eigenvalues of $A$ and $M_j$:\\begin{theorem}[Eigenvector-eigenvalue identity]\\label{eei} With the notation as above, we have\\begin{equation}|v_{i,j}|^2\\prod_{k=1;k\\neq i}^{n}\\left(\\lambda_i(A)-\\lambda_k(A)\\right)=\\prod_{k=1}^{n-1}\\left(\\lambda_i(A)-\\lambda_k(M_j)\\right)\\,.\\label{eq:wts}\\end{equation}\\end{theorem}If one lets $p_A: \\C \\to \\C$ denote the characteristic polynomial of $A$, $p_A(\\lambda) \\coloneqq \\mathrm{det}(\\lambda I_n - A) = \\prod_{k=1}^n (\\lambda - \\lambda_k(A))$, where $I_n$ denotes the $n \\times n$ identity matrix, and similarly let $p_{M_j}: \\C \\to \\C$ denote the characteristic polynomial of $M_j$, $p_{M_j}(\\lambda) \\coloneqq \\mathrm{det}(\\lambda I_{n-1} - M_j) = \\prod_{k=1}^{n-1} (\\lambda - \\lambda_k(M_j))$ then the derivative $p'_A(\\lambda_i(A))$ of $p_A$ at $\\lambda = \\lambda_i(A)$ is equal to $p'_A(\\lambda_i(A)) = \\prod_{k=1;k\\neq i}^{n}\\left(\\lambda_i(A)-\\lambda_k(A)\\right)$ and so \\eqref{eq:wts} can be equivalently written in the characteristic polynomial form $|v_{i,j}|^2 p'_A(\\lambda_i(A)) = p_{M_j}(\\lambda_i(A))$.Despite the simple nature of this identity and the extremely mature state of development of linear algebra, this identity was not widely known until very recently. In this survey we describe the many times that this identity, or variants thereof, have been discovered and rediscovered in the literature (with the earliest precursor we know of appearing in 1834). We also provide a number of proofs and generalizations of the identity. <!--TRUNCATED-->""]","[""\\section{Proofs of the identity}The identity \\eqref{eq:wts} can be readily established from existing standard identities in the linear algebra literature. We now give several such proofs.\\subsection{The adjugate proof}We first give a proof using adjugate matrices, which is a purely ``polynomial'' proof that avoids any invertibility, division, or non-degeneracy hypotheses in the argument; in particular, as we remark below, it has an extension to (diagonalizable) matrices that take values in arbitrary commutative rings. This argument appears for instance in \\cite[Section 7.9]{Parlett}.Recall that if $A$ is an $n \\times n$ matrix, the \\emph{adjugate matrix} $\\mathrm{adj}(A)$ is given by the formula $\\mathrm{adj}( A ) \\coloneqq \\left( (-1)^{i+j} \\mathrm{det}(M_{ji}) \\right)_{1 \\leq i, j \\leq n}$ where $M_{ji}$ is the $n-1 \\times n-1$ matrix formed by deleting the $j^{\\mathrm{th}}$ row and $i^{\\mathrm{th}}$ column from $A$. From Cramer's rule we have the identity $\\mathrm{adj}( A ) A = A \\mathrm{adj}( A ) = \\mathrm{det}(A) I_n$.\\subsection{The Cramer's rule proof}Returning now to the case of Hermitian matrices, we give a variant of the above proof of \\eqref{eq:wts} that still relies primarily on Cramer's rule, but makes no explicit mention of the adjugate matrix; as discussed in the next section, variants of this argument have appeared multiple times in the literature. We first observe that to prove \\eqref{eq:wts} for Hermitian matrices $A$, it suffices to do so under the additional hypothesis that $A$ has simple spectrum (all eigenvalues occur with multiplicity one), or equivalently that $\\lambda_1(A) < \\lambda_2(A) < \\dots < \\lambda_n(A)$.\\subsection{Coordinate-free proof}We now give a proof that largely avoids the use of coordinates or matrices, essentially due to Bo Berndtsson. For this proof we assume familiarity with exterior algebra (see e.g., \\cite[Chapter XVI]{BM}). The key identity is the following statement.\\begin{lemma}[Coordinate-free eigenvector-eigenvalue identity]Let $T: \\C^n \\to \\C^n$ be a self-adjoint linear map that annihilates a unit vector $v$. For each unit vector $f \\in \\C^n$, let $\\Delta_T(f)$ be the determinant of the quadratic form $w \\mapsto (Tw, w)_{\\C^n}$ restricted to the orthogonal complement $f^\\perp \\coloneqq \\{ w \\in \\C^n: (f,w)_{\\C^n} = 0 \\}$, where $(,)_{\\C^n}$ denotes the Hermitian inner product on $\\C^n$. Then one has $|(v,f)_{\\C^n}|^2 \\Delta_T(v) = \\Delta_T(f)$ for all unit vectors $f \\in \\C^n$.\\end{lemma}\\subsection{Proof using perturbative analysis}Now we give a proof using perturbation theory, which to our knowledge first appears in \\cite{Mukherjee1989}. By the usual limiting argument we may assume that $A$ has simple eigenvalues. Let $\\eps$ be a small parameter, and consider the rank one perturbation $A + \\eps e_j e_j^*$ of $A$, where $e_1,\\dots,e_n$ is the standard basis.\\subsection{Proof using a Cauchy-Binet type formula}Now we give a proof based on a Cauchy-Binet type formula, which is also related to Lemma \\ref{cfeei}. This argument first appeared in \\cite{DPTZ}.\\begin{lemma}[Cauchy-Binet type formula]Let $A$ be an $n \\times n$ Hermitian matrix with a zero eigenvalue $\\lambda_i(A) = 0$. Then for any $n \\times n-1$ matrix $B$, one has $\\mathrm{det}( B^* A B ) = (-1)^{n-1} p'_""]",,"[""\\section{History of the identity}\\label{history-sec}In this section we present, roughly in chronological order, all the references to the eigenvector-eigenvalue identity \\eqref{eq:wts} (or closely related results) that we are aware of in the literature. For the primary references, we shall present the identity in the notation of that reference in order to highlight the diversity of contexts and notational conventions in which this identity has appeared.The earliest precursor to the eigenvector-eigenvalue identity that we have found is due to Jacobi \\cite{jacobi}, who in 1834 established a result that implies the identity for the special case of symmetric tridiagonal matrices. Jacobi's work predates the development of modern matrix theory and was framed instead in the language of determinants and quadratic forms.In the early 20th century, the identity appears implicitly in the work of Lwdin \\cite{lowdin} on orthogonal atomic orbitals (1950), where it is used to compute eigenvector components without explicitly solving the full eigenvalue problem. Around the same time, the identity was independently discovered by several authors in the context of inverse eigenvalue problems for Jacobi matrices, including Hochstadt \\cite{hochstadt} and others.The identity gained more widespread attention in the numerical linear algebra community through the work of Parlett \\cite{Parlett} and others in the 1980s, where it was recognized as a useful tool for understanding the behavior of algorithms for symmetric tridiagonal matrices. However, it remained relatively obscure outside these specialized applications.A significant rediscovery occurred in the random matrix theory literature of the early 2000s, where Erds, Schlein and Yau \\cite{ESY} derived a version of the identity as part of their work on universality for Wigner matrices. This was followed by independent rediscoveries in the graph theory literature, particularly in work on inverse eigenvalue problems for graphs and the reconstruction of graphs from their spectrum.Most recently, the identity was brought to broader attention through the work of Denton, Parke, Tao and Zhang \\cite{Denton:2019ovn, DPTZ}, who formulated it in its current general form and initiated the systematic survey presented in this paper. The subsequent popular science article \\cite{wolchover-2019} led to numerous additional references being brought to our attention through crowdsourcing. <!--TRUNCATED-->""]"
,,"[""\\section{Further discussion}\\label{discuss-sec}The eigenvector-eigenvalue identity \\eqref{eq:wts} only yields information about the magnitude $|v_{i,j}|$ of the components of a given eigenvector $v_i$, but does not directly reveal the phase of these components.  On one hand, this is to be expected, since (as already noted in the consistency check (vii) in the introduction) one has the freedom to multiply $v_i$ by a phase; for instance, even if one restricts attention to real symmetric matrices $A$ and requires the eigenvectors to be real $v_i$, one has the freedom to replace $v_i$ by its negation $-v_i$, so the sign of each component $v_{i,j}$ is ambiguous.  However, \\emph{relative} phases, such as the phase of $v_{i,j} \\overline{v_{i,j'}}$ are not subject to this ambiguity.  There are several ways to try to recover these relative phases.  One way is to employ the off-diagonal analogue \\eqref{off-diag} of \\eqref{eq:wts}, although the determinants in that formula may be difficult to compute in general.  For small matrices, it was suggested in \\cite{Mukherjee1989} that the signs of the eigenvectors could often be recovered by direct inspection of the components of the eigenvector equation $A v_i = \\lambda_i(A) v_i$.  In the application in \\cite{Denton:2019ovn}, the additional phase could be recovered by a further neutrino specific identity \\cite{Toshev:1991ku}.  For more general matrices, one way to retrieve such phase information is to apply \\eqref{eq:wts} in multiple bases.  For instance, suppose $A$ was real symmetric and the $v_{i,j}$ were all real.  If one were to apply the eigenvector-eigenvalue identity after changing to a basis that involved the unit vector $\\frac{1}{\\sqrt{2}} (e_j + e_{j'})$, then one could use the identity to evaluate the magnitude of $\\frac{1}{\\sqrt{2}} ( v_{i,j} + v_{i,j'} )$.  Two further applications of the identity in the original basis would give the magnitude of $v_{i,j}, v_{i,j'}$, and this is sufficient information to determine the relative sign of $v_{i,j}$ and $v_{i,j'}$.  We also remark that for real symmetric matrices that are acyclic (such as weighted adjacency matrices of graphs that do not contain loops), one can write down explicit formulae for the coefficients of eigenvectors (and not just their magnitudes) in terms of characteristic polynomials of minors; see \\cite{BK}.  We do not know of any direct connection between such formulae and the eigenvector-eigenvalue identity \\eqref{eq:wts}.For large unstructured matrices, it does not seem at present that the identity \\eqref{eq:wts} provides a competitive algorithm to compute eigenvectors.  Indeed, to use this identity to compute all the eigenvector component magnitudes $|v_{i,j}|$, one would need to compute all $n-1$ eigenvalues of each of the $n$ minors $M_1,\\dots,M_n$, which would be a computationally intensive task in general; and furthermore, an additional method would then be needed to also calculate the signs or phases of these components.  However, if the matrix is of a special form (such as a tridiagonal form), then the identity could be of more practical use, as witnessed by the uses of this identity (together with variants such as \\eqref{yri-2}) in the literature to control the rate of convergence for various algorithms to compute eigenvalues and eigenvectors of tridiagonal matrices.  Also, as noted recently in \\cite{Kausel2018}, if one has an application that requires only the component magnitudes $|v_{1,j}|,\\dots,|v_{n,j}|$ at a single location $j$, then one only needs to compute the characteristic polynomial of a single minor $M_j$ of $A$ at a single value $\\lambda_i(A)$, and this may be more computationally feasible.\\section{Sociology of science issues}As one sees from Section \\ref""]","['The earliest appearance of identities equivalent to \\eqref{eq:wts} that we know of is due to Jacobi \\cite[\\S 8, (33)]{jacobi}.  In modern notation, Jacobi diagonalizes a symmetric quadratic form $\\sum_{\\chi=1}^n \\sum_{\\lambda=1}^n a_{\\chi,\\lambda} x_\\chi x_\\lambda$ as $\\sum_{m=1}^n G_m (\\sum_{i=1}^n \\alpha_i^{(m)} x_i)^2$ for an orthogonal matrix $(\\alpha_i^{(m)})_{1 \\leq i,m \\leq n}$, and then the for each $m$ the cofactors $B^{(m)}_{\\chi \\lambda}$ of the form $\\sum_{\\chi=1}^n \\sum_{\\lambda=1}^n a_{\\chi,\\lambda} x_\\chi x_\\lambda - G_m \\sum_{\\chi=1}^n x_\\chi^2$ are extracted.  Noting that the columns of this cofactor matrix are proportional to the eigenvector $(\\alpha_i^{(m)})_{1 \\leq i \\leq n}$, Jacobi concluded that\\begin{equation}\\label{denom}(G_1-G_m) \\dots (G_n-G_m) \\alpha_\\chi^{(m)} \\alpha_\\lambda^{(m)} = B_{\\chi\\lambda}^{(m)}\\end{equation}with the factor $G_m-G_m$ omitted from the left-hand side; this is essentially \\eqref{adji} for real symmetric matrices. In \\cite[\\S 8, (36)]{jacobi} an identity essentially equivalent to \\eqref{eq:wts-alt} for real symmetric matrices is also given.']"
"['\\section{Introduction}What\'s common among the great batsman Sachin Tendulkar, the great athlete Usion Bolt, the great tennis player Roger Fedrer and the great footballer Maradonna?{\\bf Answer : They spend hours after hours, days after days training!} Over the years, the author has taught material presented here to the graduate and undergraduate students at IISER Pune and in various summer and winter schools meant for graduate students and/or teachers. The author would like to thank the participants of various ATM schools who enthusiastically responded to the lectures and tutorials.  \\begin{enumerate}\\item {\\bf Why this?} The idea behind these notes is to introduce matrix groups. The groups we mostly encounter are matrix groups: most of the finite simple groups, algebraic groups, Lie groups etc. Yet, in our undergraduate course on group theory we do not introduce matrix groups. This is an attempt to correct that mistake.\\item {\\bf For whom?} This book grew out of an attempt to convince audience that they can make use of Linear Algebra they know, to study some of the matrix groups. Any PhD student, undergraduate student or teacher will benefit from this. The topics presented here form bread and butter for group theorists, number theorists, physicists and engineers.  \\item {\\bf How to read this?}This note is meant to be a supplement for those who have studied first course in Linear Algebra and Group Theory. That is why its in the form of exercises. The author confesses that some of the exercises throw big, unknown words but they are not meant to scare you. It\'s hoped that this will enthuse you to look further.\\end{enumerate}We present some motivation behind this attempt. Author hopes that these stories will enthuse the audience towards this material.\\section{CFSG}One of the biggest achievements of the last century is ``The Classification of Finite Simple Groups\'\'. It was born out of the following: {\\bf Can we classify all finite groups?}Let $G$ be a finite group. If $G$ has a proper normal subgroup $N$, it can be broken into two pieces: $N, G/N$. However, usually it is not easy to construct the group $G$ given the smaller groups. They fit in an exact sequence:$$1\\rightarrow N \\rightarrow G \\rightarrow G/N \\rightarrow 1.$$The problem of constructing $G$ out of given other two is called ""Extension Problem"" and is a difficult problem. For example, to determine central extension one uses some cohomology theory. However, using the above idea we reduce the problem of classifying all finite groups to the classification of finite simple groups. This has been achieved now. It has taken efforts of more than a hundred year and several hundred mathematicians. Thus it is usually not attributed to any one of them.\\begin{theorem}[CFSG]A finite simple group is one of the following:\\begin{enumerate}\\item Cyclic groups $\\mathbb Z/p\\mathbb Z$, where $p$ is a prime.\\item Alternating groups $A_n$ for $n\\geq 5$.\\item Finite groups of Lie type\\begin{enumerate}\\item classical type \\item exceptional type\\end{enumerate} \\item The sporadic groups $26$ of them.\\end{enumerate}\\end{theorem}In our undergraduate group theory course we learn the first two family. The idea here is to attempt to convince the audience that they can get familiar with the third family as well using their knowledge of Linear Algebra, at least for sure the group of classical type. For the history and more information please read the wikipedia article. Here are the classical groups:\\begin{enumerate}\\item Special Linear Groups\\item Orthogonal Groups of odd dimension\\item Symplectic Groups\\item Orthogonal Groups of even dimension\\item Unitary Groups\\end{enumerate}<!--TRUNCATED-->']",,,"['\\section{Representation theory}Let $G$ be a group. While working with some examples of groups we quickly realise that there is not much we know about them. One way is possibly to compare them with some known groups. In this case we take the known groups as linear group, $GL(V)$, and the comparison is made via all group homomorphisms from $G$ to linear groups. More precisely, we define a representation.Fix a field $k$. A representation of $G$ is a group homomorphism $\\rho \\colon G \\rightarrow GL(V)$ where $V$ is a vector space over the field $k$. A subspace $W\\subset V$ is said to be invariant if $\\rho(g)(W)\\subset W$, for all $g\\in G$. Further, a representation is said to be irreducible if it has no proper invariant subspace.For a finite group $G$, it turns out that over algebraically closed field $k$ when $char(k)\\nmid |G|$ the representations of the group can be understood via character theory. In this case every representation can be written as a direct sum of irreducible representations. Thus, the character theory is built on knowing basic group theory and linear algebra well.']"
,,,"['\\subsection{$PGL_2(k)$}The group $$PGL_2(k) = \\frac{GL_2(k)}{ \\mathcal Z(GL_2(k))} = \\frac{GL_2(k)}{\\{\\diag(\\lambda, \\lambda)\\mid \\lambda\\in k^*\\}}.$$ We represent the maximal torus by $T=\\{\\diag(\\lambda , 1)\\mid \\lambda\\in k^*\\}$ thought of as the image of diagonal torus in $GL_2(k)$ identified as follows: $\\diag(\\lambda_1, \\lambda_2) \\mapsto \\diag(\\lambda_1 \\lambda_2^{-1}, 1)$.\\begin{exercise}Show that the character group $X(T)=<\\chi> \\cong \\mathbb Z$ where the character $\\chi$ is given by $\\chi(\\diag(\\lambda_1 \\lambda_2^{-1}, 1)) = \\frac{\\lambda_1}{\\lambda_2}$. \\end{exercise}The Lie algebra is $pgl_2= M_2(k)/\\{\\diag(\\lambda, \\lambda)\\mid \\lambda\\in k\\}$ and hence we can take $e_{11}, e_{12}, e_{21}$ as a basis of this Lie algebra.Use the restriction of $Ad$ map to the maximal torus and do the simultaneous diagonalisation to prove the following.\\begin{exercise}The root-space decomposition of the Lie algebra is$$pgl_2 = <h> \\bigoplus <e_{12}> \\bigoplus <e_{21}>.$$To prove this verify the following,$Ad(\\diag(\\lambda_1, \\lambda_2))(e_{11})=e_{11}$ and $Ad(\\diag(\\lambda_1, \\lambda_2))(e_{12})=\\frac{\\lambda_1}{\\lambda_2}e_{12}$.\\end{exercise}<!--TRUNCATED-->']"
,,,"['The similitude symplectic group is $$GSp_{2l}=\\{X\\in GL_{2l} \\mid \\tr XJX = \\lambda J, \\text{\\ for some\\ } \\lambda\\in k^*\\}.$$The scalar $\\lambda=\\lambda_X$ varies with $X$ and is called similitude factor of the element $X$. \\begin{exercise}The map $\\Psi \\colon GSp_{2l} \\rightarrow k^*$ given by $X\\mapsto \\lambda_X$ is a group homomorphism with kernel $Sp_{2l}$. \\end{exercise} \\begin{exercise} The set of scalars $\\{\\alpha. I \\in GL_{2l} \\mid \\alpha\\in k^*\\}$ is the center of $GSp_{2l}$.\\end{exercise}\\begin{exercise}The elements of $GSp_{2l}$ induce automorphisms of $Sp_{2l}$ acting via conjugation.\\end{exercise}']"
"['\\section{Introduction}One of the interesting issues in mathematics is the determination of the structure of linear (additive) mappings on algebras (rings) that act through zero products in the same way as certain mappings, such as homomorphisms, derivations, centralizers, etc. Among these issues, one can point out the problem of characterizing a linear (additive) map $\\delta$ from an algebra (ring) $\\mathcal{A}$ into an $\\mathcal{A}$-bimodule $\\mathcal{M}$, which satisfy \\begin{equation}\\label{d3}ab = 0 \\Longrightarrow a \\tau ( b) + \\delta ( a ) b = 0 , ~~~ (a,b\\in \\mathcal{A}) . \\end{equation}where $\\delta : \\mathcal{A} \\rightarrow \\mathcal{M}$ and $\\tau: \\mathcal{A} \\rightarrow \\mathcal{M}$ are linear (additive) maps. The condition \\eqref{d3} has also been studied by some authors and the mappings $\\delta$ and $ \\tau$ have been characterized on different algebras (rings). In this paper, we describe the linear mappings of the standard operator algebras in a Banach space that satisfy \\eqref{d3}.<!--TRUNCATED-->']",,,"['\\section{The main results}Throughout this paper, all algebras and vector spaces will be over the complex field $\\mathbb{C}$. Let $\\mathcal{X}$ be a Banach space. We denote by $B(\\mathcal{X})$ the algebra of all bounded linear operators on $\\mathcal{X}$, and $F(\\mathcal{X})$ denotes the algebra of all finite rank operators in $B(\\mathcal{X})$. Recall that a \\textit{standard operator algebra} is any subalgebra of $B(\\mathcal{X})$ which contains $F(\\mathcal{X})$. We shall denote the identity matrix of $B(\\mathcal{X})$ by $I$. In Theorem~\\ref{t21} of this article we characterize the linear maps $\\delta, \\tau: \\mathcal{A} \\rightarrow B(\\mathcal{X})$ satisfying \\eqref{d3}, where $\\mathcal{A}$ is a unital standard operator algebra. \\begin{thm} \\label{t21}Let $ \\mathcal{X} $ be a Banach space, $dim \\mathcal{X} \\geq 2$, and let $ \\mathcal{A} \\subseteq B ( \\mathcal{X} ) $ be a unital standard operator algebra. Suppose that $ \\delta $ and $ \\tau $ be linear maps from $ \\mathcal{A} $ into $ B ( \\mathcal{X} ) $ satisfying\\[ A B = 0 \\Longrightarrow A \\tau ( B) + \\delta ( A ) B = 0 , ~~~ (A, B \\in \\mathcal{A}) .  \\]Then there exist $ R , S, T  \\in B ( \\mathcal{X} ) $ such that \\[ \\delta (A) = A S   - R A  , ~~  ~\\tau ( A) = A T - S A \\]for all $ A \\in \\mathcal{A} $. \\end{thm}<!--TRUNCATED-->']"
"[""\\section*{Introduction}Left-symmetric algebras (called also pre-Lie algebras) arise in many areas of mathematics and physics. They have already been introduced by A. Cayley in 1896, in the context of rooted tree algebras. Then they were forgotten for a long time until Vinberg in 1960 and Koszul in 1961 who introduced them in the context of convex homogeneous cones.Representations and cohomology theory of left-symmetric algebras were studied by Askar Dzhumadil'daev in \\cite{DA}. Recently, representations and cohomology of Hom-left-symmetric algebras were developed in \\cite{sheng3,rephomprelie}.In the present paper, we will study the representation and the cohomology theory of BiHom-left-symmetric algebras. Recall that a BiHom-algebra is an algebra in such a way that the identities defining the structure are twisted by two homomorphisms $\\a$ and $\\b$. This class of algebras was introduced from a categorical approach in \\cite{bihomass} as an extension of the class of Hom-algebras \\cite{MS2}. Other classes of BiHom-algebras can be found in \\cite{bihomalt,bihomprealt,bihomliesuper}.The notion of a BiHom-left-symmetric algebra (or BiHom-pre-Lie algebras) was introduced in \\cite{Liu&Makhlouf&Menini&Panaite}. There is a close relationship between BiHom-left-Lie algebras and BiHom-Lie algebras: a BiHom-left-symmetric algebra $(A,\\cdot,\\alpha,\\b)$ gives rise to a BiHom-Lie algebra $(A,[\\cdot,\\cdot]_C,\\alpha,\\b)$ via the commutator bracket (in the BiHom sense), which is called the sub-adjacent BiHom-Lie algebra and denoted by $A^C$. On the other hand, any BiHom-Lie algebra gives rise to a BiHom-left-symmetric algebra using the Rota-Baxter operator.Despite there are important applications of BiHom-left-symmetric algebras, cohomologies of BiHom-left-symmetric algebras have not been well developed due to its complexity. The aim of this work is to develop representations and cohomology theories of this class of algebras and give some of its applications.In the present work, we give the natural formula of the representation of a BiHom-left-symmetric algebra and provide some related results. Moreover, we show that there is well-defined tensor product of two representations of a BiHom-left-symmetric algebra. This is the content of Section 2.In Section 3, we give the cohomology theory of a BiHom-left-symmetric algebra $(A,\\cdot,\\a,\\b)$ with the coefficient in a representation $(V,L,R,\\f,\\p)$. The main contribution is to define the coboundary operator $\\partial: C^n(A;V)\\longrightarrow C^{n+1}(A;V)$.In Section 4, we study linear deformations of a BiHom-left-symmetric algebra using the cohomology with the coefficient in the adjoint representation defined in Section 3. We introduce the notion of a Nijenhuis operator on a BiHom-left-symmetric algebra and show that it gives rise to a trivial deformation. Finally, we study the relation between linear deformations of a BiHom-left-symmetric algebra and linear deformations of its sub-adjacent BiHom-Lie algebra.We work over a base field $\\mathbb{K}$ of characteristic zero. All algebras, linear spaces etc. will be over $\\mathbb{K}$. Throughout the paper, all the BiHom-algebras are considered regular.""]",,,"[""\\section{Preliminaries}In this section, we recall definition, representations and some key results of BiHom-Lie algebras \\cite{bihomass} and introduce the notion of an $\\mathcal{O}$-operator on a BiHom-Lie algebra.\\begin{defi}A BiHom-Lie algebra is a tuple  $(\\g,[\\cdot,\\cdot],\\a,\\b)$ consisting of a linear space $\\g$, a linear map $[\\cdot,\\cdot]:\\otimes^2\\g\\longrightarrow \\g$ and two algebra morphisms $\\a,\\b:\\g\\longrightarrow \\g $, satisfying\\begin{align} \\a \\circ \\b =& \\b \\circ \\a, \\nonumber\\\\ [\\b(x),\\a(y)]=& -[\\b(y),\\a(x)],\\nonumber\\\\ \\circlearrowleft_{x,y,z \\in \\g} [\\b^2(x)&,[\\b(y),\\a(x)]]= 0\\label{Bihom-jaco},\\end{align}for any $x,y,z \\in \\g$.\\end{defi}\\begin{defi}A morphism of BiHom-Lie algebra $f:(\\g,[\\cdot,\\cdot]_\\g,\\a,\\b)\\longrightarrow (\\mathfrak{g'},[\\cdot,\\cdot]_\\mathfrak{g'},\\a',\\b')$ is a linear map $f:\\g\\longrightarrow \\mathfrak{g'}$ such that \\begin{eqnarray*} &&f[x,y]_\\g=[f(x),f(y)]_\\mathfrak{g'},\\quad \\forall~x,y\\in \\g,\\\\&& f\\circ \\alpha=\\a' \\circ f,  ~~ f\\circ \\b=\\b' \\circ f.  \\end{eqnarray*}  \\end{defi}""]"
,,,"['Let $(A,\\cdot,\\alpha,\\b)$ be a BiHom-left-symmetric algebra. In the sequel, we will also denote the BiHom-left-symmetric multiplication $\\cdot$ by $\\Pi$.\\begin{defi}Let $(A,\\Pi,\\a,\\b)$ be a BiHom-left-symmetric algebra and $\\pi \\in C^2(A;A)$ be a bilinear operator, satisfying $\\pi\\circ (\\alpha\\otimes\\alpha)=\\alpha\\circ \\pi$ and $\\pi \\circ (\\b\\otimes\\b)=\\b\\circ \\pi$. If  $$\\Pi_t=\\Pi+t\\pi$$ is still a BiHom-left-symmetric multiplication on $A$ for all $t$, we say that $\\omega$ generates a (one-parameter) linear deformation of a BiHom-left-symmetric algebra $(A,\\Pi,\\a,\\b)$.\\end{defi}<!--TRUNCATED-->']"
"['\\section{ Introduction}Algebras and vector spaces in this paper are assumed to be those over the complex field $\\mathbb{C}$. Let $A$ and $M$ be an algebra and an $A$-bimodule, respectively. Recall that a linear map $d:A\\rightarrow M$ is said to be a \\textit{derivation} if $d(ab) = ad(b)+d(a)b $ for all $a, b \\in A$. Also, $d$ is called inner derivation if for some $m \\in M$, $d$ takes the form $d(a)=am-ma$ for all $a \\in A$. We also call $d$ \\textit{anti-derivation} if $d(ab) =bd(a)+d(b)a$ for all $a,b \\in A$. Derivation is an important field of research, and have been studied and applied extensively both in theory and applications. There are a number of papers investigating the conditions under which mappings of (Banach) algebras are thoroughly determined by actions on some sets of points. The following condition has been drawing many researchers attention working in this field:\\[a, b \\in A, \\,\\, ab=z\\Rightarrow \\delta(ab) = a\\delta(b)+\\delta(a)b \\quad (\\blacklozenge),\\]where $z\\in A$ is fixed and $\\delta:A\\rightarrow M$ is a linear (additive) map. Motivated by these reasons, in this paper we consider the problem of characterizing continuous linear maps on $C^\\star$-algebras behaving like derivations or anti-derivations at orthogonal elements for several types of orthogonality conditions. In particular, in this paper we consider the subsequent conditions on a continuous linear map $\\delta:A\\rightarrow \\A$ where $A$ is a  $C^\\star$-algebra:\\begin{enumerate}\\item[(i)] \\textit{derivations through one-sided orthogonality conditions} \\[ ab=0 \\Longrightarrow a\\delta(b)+\\delta(a)b=0 ;\\] \\[ab^{\\star}=0 \\Longrightarrow a\\delta(b)^{\\star}+\\delta(a)b^{\\star}=0 ;\\] \\[a^{\\star}b=0 \\Longrightarrow a^{\\star}\\delta(b)+\\delta(a)^{\\star}b=0 ;\\]\\item[(ii)] \\textit{anti-derivations through one-sided orthogonality conditions} \\[ ab=0 \\Longrightarrow b\\delta(a)+\\delta(b)a=0 ;\\] \\[ ab^{\\star}=0 \\Longrightarrow \\delta(b)^{\\star}a+b^{\\star}\\delta(a)=0;\\] \\[a^{\\star}b=0 \\Longrightarrow \\delta(b)a^{\\star}+b\\delta(a)^{\\star}=0;\\]\\item[(iii)] \\textit{Derivations through two-sided orthogonality conditions} \\[ab=ba=0 \\Longrightarrow a\\delta(b)+\\delta(a)b=b\\delta(a)+\\delta(b)a=0;\\]\\[ ab^{\\star}=b^{\\star}a=0 \\Longrightarrow a\\delta(b)^{\\star}+\\delta(a)b^{\\star}=\\delta(b)^{\\star}a+b^{\\star}\\delta(a)=0;\\]\\end{enumerate}where $a,b \\in A$. Our purpose is to investigate whether the above conditions characterize continuous derivations ($\\star$-derivations) or continuous anti-derivations ($\\star$-anti-derivations) on $C^\\star$-algebras. Also we give applications of our results for von Neumann algebras and unital simple $C^{\\star}$-algebras.']",,,"['\\section{Derivations and anti-derivations at orthogonality product}In this section we will consider a continuous linear map on a $ C^{\\star}  $-algebra behaving like derivation or anti-derivation at one-sided orthogonality conditions. \\par In order to prove our results we need the following result for continuous bilinear maps on  $ C^{\\star}  $-algebras.\\begin{lem} (Alaminos et al. \\cite{al1}).\\label{l21}Let $A$ be a $ C^{\\star}  $-algebra, let $ X $ be a Banach space, and let $ \\phi: A \\times A \\rightarrow X $ be a continuous bilinear map such that $ \\phi (a, b) =0 $ whenever $ a,b \\in A $ are such that $ ab=0 $, Then $ \\phi ( ab ,c  ) = \\phi (a, bc) $ for all$ a,b , c \\in A $. Also there is a continuous linear map $ \\Phi: A \\rightarrow X $ such that $ \\phi (a, b ) = \\Phi (ab) $  for all$ a,b  \\in A $.\\end{lem}It should be noted that in the above lemma if $A$ is a commutative $ C^{\\star}  $-algebra, then $\\phi$ is symmetric, that is $\\phi(a,b)=\\phi(b,a)$ for all $a,b\\in A$. \\par Now, we characterize continuous linear maps on  $ C^{\\star}  $-algebras behaving like derivations through one-sided orthogonality conditions. \\begin{thm}\\label{o1}Let $A$ be a $ C^{\\star}  $-algebra, and let $\\delta: A \\rightarrow \\A$ be a continuous linear map.\\begin{enumerate}\\item[(i)] $ \\delta $ satisfies \\begin{equation*} a b=0 \\Longrightarrow a   \\delta(b)+\\delta(a)   b =0\\quad (a , b \\in A).\\end{equation*}if and only if there is a continuous derivation $ d: A \\rightarrow \\A $ and an element $ \\eta \\in \\mathcal{Z}(\\A) $ such that $\\delta(a)= d(a) + \\eta    a  $ for all $ a \\in A$. \\item[(ii)] $ \\delta $ satisfies \\begin{equation*} a b ^{\\star}=0 \\Longrightarrow a   \\delta(b)^{\\star}+\\delta(a)   b ^{\\star}=0\\quad (a , b \\in A). \\end{equation*}if and only if there is a continuous $\\star$-derivation $ d: A \\rightarrow \\A $ and an element $ \\eta \\in \\A $ such that $\\delta(a)= d(a) + \\eta    a  $ for all $ a \\in A$. \\end{enumerate}\\end{thm}<!--TRUNCATED-->']"
,,,
"[""\\section*{Introduction}\\addcontentsline{toc}{section}{Introduction}Infinitesimal generators of semigroups or groups are mathematical objects which arise in various contexts. One of the most general ones is that of locally convex spaces, where they appear as particular kinds of linear operators, defined from an action of the additive semigroup $[0, +\\infty)$ or the additive group $\\mathbb{R}$ of real numbers on the subjacent space. This action is usually implemented by continuous linear operators and subject to a condition of continuity.Within the context of normed spaces, there exist two very important classes of such strongly continuous actions: contraction semigroups and groups of isometries. In Hilbert space theory, for example, strongly continuous one-parameter groups of isometries are implemented by unitary operators and their generators are precisely the anti-self-adjoint operators.For more general complete locally convex spaces, there is a theorem which characterizes the generators of equicontinuous semigroups in an analogous way that the Lumer-Phillips Theorem does. When the locally convex space under consideration has an additional algebraic structure, turning it into an algebra, the actions of interest are by strongly continuous one-parameter groups of automorphisms and their generators then become derivations.There is another very important direction for generalization, which consists in passing from one-parameter groups to more general groups. One framework that comes to mind here would be to consider abstract topological groups and their actions on locally convex spaces. But often, the study of smooth and analytic elements of a given action is very important and useful, so a natural requirement on the group is that it should be a Lie group.Conversely, one may investigate the exponentiability of a (real finite-dimensional) Lie algebra $\\mathcal{L}$ of linear operators: when can its elements be obtained (as generators of one-parameter groups) from a strongly continuous representation $V$ of a (connected) Lie group $G$, having a Lie algebra $\\mathfrak{g}$ isomorphic to $\\mathcal{L}$?For representations on finite-dimensional vector spaces, a classical theorem states that every Lie algebra of linear operators is exponentiable, provided only that one chooses $G$ to be simply connected. In the infinite-dimensional case, things are much more complicated, and when formulating the question some reasonable a priori requirements are usually assumed: the elements of $\\mathcal{L}$ are linear operators defined on a common, dense domain $\\mathcal{D}$ which they all map into itself. But there are counterexamples showing that these are not sufficient.The main objective of this work is to report some new results regarding the exponentiation of (in general, noncommutative) finite-dimensional real Lie algebras of linear operators acting on complete Hausdorff locally convex spaces, focusing on the equicontinuous case, and to search for applications within the realm of locally convex algebras. To the knowledge of the author, there are very few theorems in the literature dealing with the exponentiability of Lie algebras of dimension $d > 1$ of linear operators on locally convex spaces beyond the context of Banach spaces.The main exponentiation theorem of this paper is Theorem 2.14, whose proof is divided into three steps. The first one is to show how to construct a group invariant dense C$^\\infty$ domain from a mere dense C$^\\infty$ domain. The second step consists in formulating ``locally convex equicontinuous versions'' of three exponentiation theorems found in the literature. In the final step, necessary conditions for exponentiability are obtained in Theorem 2.13, and in the main theorem of the paper (Theorem 2.14), a characterization of exponentiability in complete Hausdorff locally convex spaces is given. Finally, in Section 3, some applications to complete locally convex algebras are given, with special attention to locally C$^*$-algebras. <!--TRUNCATED-->""]",,,"['\\section{Some General Facts}\\subsection{One-Parameter Semigroups and Groups}In the classical theory of one-parameter semigroups and groups, a strongly continuous one-parameter semigroup on a normed space $(\\mathcal{Y}, \\|\\, \\cdot \\,\\|)$ is a family of continuous linear operators $\\left\\{V(t)\\right\\}_{t \\geq 0}$ satisfying $V(0) = I, \\, V(s + t) = V(s)V(t)$ for $s, t \\geq 0$ and $\\lim_{t \\rightarrow t_0} \\|V(t)y - V(t_0)y\\| = 0$ for $t_0 \\geq 0, \\, y \\in \\mathcal{Y}$. It is a well-known fact that in this setting there exist $M > 0$ and $a \\geq 0$ satisfying $\\|V(t)y\\| \\leq M \\exp (at) \\, \\|y\\|$ for all $y \\in \\mathcal{Y}$, as a consequence of the Uniform Boundedness Principle.The type of a strongly continuous semigroup $t \\longmapsto V(t)$ on a Banach space $(\\mathcal{Y}, \\|\\, \\cdot \\,\\|)$ is defined as the number $\\inf_{t > 0} \\, \\frac{1}{t} \\, \\text{log }\\|V(t)\\|$. Analogously, if $t \\longmapsto V(t)$ is a strongly continuous group, its type is defined as $\\inf_{t \\in \\mathbb{R} \\backslash \\left\\{0\\right\\}} \\frac{1}{|t|} \\, \\text{log }\\|V(t)\\|$.The task, now, will be to formulate analogous concepts for locally convex spaces. A family of seminorms $\\Gamma := \\left\\{p_\\lambda\\right\\}_{\\lambda \\in \\Lambda}$ defined on a locally convex space $\\mathcal{X}$ is said to be saturated if, for any given finite subset $F$ of $\\Lambda$, the seminorm defined by $p_F \\colon x \\longmapsto \\max \\left\\{p(x): p \\in F\\right\\}$ also belongs to $\\Gamma$. Every fundamental system of seminorms can always be enlarged to a saturated one by including the seminorms $p_F$, as defined above, in such a way that the resulting family generates the same topology. <!--TRUNCATED-->']"
,,,
,,,
,,,
,,,
,,,
,,,
"[""\\section*{Introduction}Koszul complexes are classical objects of study in commutative algebra. Their structure reflects many important properties of the rings over which they are defined, and these reflections are often encoded in the product structure of their homology. Indeed, Koszul complexes are more than just complexes --- they are, in fact, the prototypical examples of differential graded ($=$ DG) algebras in commutative ring theory, and thus the homology of a Koszul complex carries an algebra structure. These homology algebras encode (among other things) the Gorenstein condition \\cite{AvramovGolod1971}, the Golod condition \\cite{Golod1962}, and whether or not the ring is a complete intersection \\cite{Assmus1959}. The present paper follows in the spirit of these results by studying how certain homological properties of Koszul complexes and their homology algebras are interrelated with homological properties of the rings from which they originate.In more detail, let $R$ be a standard graded commutative algebra over a field $k$, let $K$ be a Koszul complex on a minimal set of generators of the augmentation ideal of $R$ (which we call \\textit{the} Koszul complex of $R$), and let $H$ be the homology algebra of $K$. The ground field $k$ can then be resolved by free modules over the algebras $R$ and $H$, and by semifree modules over the algebra $K$ (the definition of \\textit{semifree module} belongs to \\textit{DG homological algebra}, and it will be recalled in the body of the paper). Our goal is then to compare the three resolutions of $k$ over the three different algebras and to identify when linearity of one of the resolutions implies linearity of one of the others. Linearity of the resolution over $R$ means that the algebra $R$ belongs to the class of \\textit{Koszul algebras}, a familiar class whose theory was first systematically laid down by Priddy in \\cite{Priddy1970} and which has become a class of central importance in the homology theory of both noncommutative and commutative graded algebras. See, for example, the survey \\cite{Froberg1999} and the monograph \\cite{PolishchukPositselski2005} for expositions on the noncommutative side, and the survey \\cite{Conca2013} directed at applications in commutative algebra.On the other hand, what we mean by ``linearity'' of the resolutions over $K$ and $H$ requires further explanation. As we will recall below, there is a DG version of $\\Tor^K{(k,k)}$ which can be computed via a semifree resolution of $k$ over $K$. The grading on $R$ induces an extra internal degree on $\\Tor^K{(k,k)}$ in addition to its usual homological degree, and we then say that $K$ is a \\textit{Koszul DG algebra} when the ``non-diagonal'' part of $\\Tor^K{(k,k)}$ vanishes, in complete analogy with the classical definition of Koszulness.This definition follows in the spirit of (but is not exactly identical to) the definition offered by He and Wu in \\cite{HeWu2008} who deal with connected \\textit{cochain} DG algebras instead of \\textit{chain} algebras, which is where our focus lies. The differential accompanying the former type of algebra raises homological degree, while the one accompanying the latter lowers homological degree. %Cochain DG algebras tend to occur more naturally in geometric and topological settings whereas chain DG algebras occur in algebraic ones; for example, He and Wu's first example of a Koszul cochain DG algebra is the minimal model of the de Rham complex of a smooth manifold satisfying certain extra hypotheses. See also the dictionary \\cite{AvramovHalperin1986}, where some distinctions (and similarities) between the topological and algebraic sides of DG homological algebra are elucidated.The relationship between Koszulness of the algebras $R$ and $K$ is then satisfyingly simple and should not surprise specialists in this field.\\theoremstyle{theorem}\\newtheorem*{thmA}{Theorem A}\\begin{thmA}Let $R$ be""]",,,"[""\\section{Definitions and preliminaries}Throughout this paper $k$ denotes a field. Elements of graded objects will always be assumed homogeneous and thus all elements of such objects have degrees.We assume that the reader is familiar with the basic terminology and theory of connected $\\bbz$-graded $k$-algebras; everything that we need can be found in sections 0 and 4 in Chapter 1 of \\cite{PolishchukPositselski2005}, or the Appendice in \\cite{Lemaire1974}. We single out just a few definitions and terms from this theory that are of particular importance to us. First, we make\\begin{defn}\\label{defn:Koszul}Let $A$ be a connected $\\bbz$-graded $k$-algebra. We will say $A$ is \\textit{Koszul} if $\\beta_{ij}^A(k) \\neq 0$ implies $i=j$ for all $i,j\\geq 0$.\\end{defn}Here\t\\[\\beta_{ij}^A(k) = \\dim_k{ \\Tor^A_{i}{(k,k)}_j}\t\\]is the \\textit{$(i,j)$-th Betti number} of $k$. All of these numbers are finite, and the generating function built from them is denoted\t\\[\\upP^A_k(s,t) = \\sum_{i,j} \\beta^A_{ij}(k) s^i t^j\t\\]and is called the \\textit{bigraded Poincar\\'e series} of $k$.A $\\bbz$-graded module $M$ over a connected $\\bbn$-graded $k$-algebra is said to be \\textit{finite} if $M_j=0$ for all $j\\ll 0$ and each $M_j$ is finite-dimensional over $k$. The \\textit{$q$-th shift} of $M$ is defined to be the $\\bbz$-graded module\t\\[M(q) = \\bigoplus_{j\\in \\bbz} M(q)_j \\quad \\tn{with} \\quad M(q)_j = M_{q+j}.\t\\]""]"
,,,"['\\begin{prop}Let $R$ be a standard graded algebra and let $H$ be its Koszul homology algebra. If $R$ is Koszul and Golod, then $H$ is strand-Koszul.\\begin{proof}If $R$ is a Golod algebra, then the inequalities in \\eqref{eqn:iyengar} are equalities, and by Proposition \\ref{prop:inequalities} and Corollary \\ref{cor:simulDegen} this means the Koszul complex $K$ is quasi-formal. If $R$ is Koszul as well, we can apply Theorem \\ref{thm:main} to obtain the desired conclusion.\\end{proof}\\end{prop}<!--TRUNCATED-->']"
"[""\\section{Introduction}\\label{intro}Since Baker \\cite{Bak1, Bak2} found lower bounds for linear forms in logarithms\\begin{equation}b_1\\log \\alpha_1+b_2\\log \\alpha_2+\\cdots +b_n\\log \\alpha_n\\end{equation}with $\\alpha_i$ complex algebraic numbers and $b_i$ integers,many authors such as Matveev \\cite{Mat} have given improved lower bounds for linear forms in logarithms of algebraic numbers.Lower bounds for linear forms in two logarithms\\begin{equation}\\Lm=b_2\\log \\alpha_2-b_1\\log \\alpha_1,\\end{equation}with $\\alpha_1, \\alpha_2$ two complex algebraic numbers and $b_1, b_2$ two positive integershad already been given by Gel'fond \\cite{Gel}and several authors such as Laurent \\cite{Lau1, Lau2} and Laurent, Mignotte and Nesterenko \\cite{LMN}have given improved lower bounds.For any algebraic number $\\alpha$ of degree $d$ over $\\Q$,we define the absolute logarithmic height of $\\alpha$ by\\begin{equation}\\h(\\alpha)=\\frac{1}{d}\\left(\\log\\abs{a}+\\sum_{i=1}^d\\log\\max\\{1, \\abs{\\alpha^{(i)}}\\}\\right),\\end{equation} where $a$ is the leading coefficient of the minimal polynomial of $\\alpha$ over $\\Z$and $\\alpha^{(i)} (i=1, \\ldots, d)$ denote the conjugates of $\\alpha$ in complex numbers.As an application of their lower bound for linear forms in two logarithms,Laurent, Mignotte and Nesterenko \\cite[Th\\'{e}or\\`{e}me 3]{LMN} gave an lower boundfor this special logarithmic form\\begin{equation}\\Lm_0=b_2\\log \\alpha-b_1\\pi i,\\end{equation}where $\\alpha$ is an algebraic number of absolute value one but not a root of unity,$\\log\\alpha$ takes the principal branch, and  $b_1, b_2$ are positive integers.Putting\\begin{equation}\\begin{split}D= ~ & [\\Q(\\alpha): \\Q]/2, \\\\a= ~ & \\max \\{20, 10.98\\abs{\\log\\alpha}+2D\\h(\\alpha)\\}, \\\\h= ~ & \\max\\{17, \\sqrt{D}/10, D(\\log (b_1/2a+b_2/68.9)+2.35)+5.03\\},\\end{split}\\end{equation}we have\\begin{equation}\\abs{\\Lm_0}\\geq -8.87ah^2.\\end{equation}We note that the quantity $h$ here is denoted by $H$ in \\cite{LMN}.We use $h$ in order to be consistent with the notation \\cite{Lau2}.Later, Laurent \\cite{Lau2} obtained the stronger lower bound for general linear forms in two logarithmsin the following form:<!--TRUNCATED-->""]",,,"[""\\section{Preliminaries to the proof}We note that we work with a slightly generalized form $\\Lm_1$ rather than $\\Lm_0$.We may assume that $\\Re \\alpha\\geq 0$ and $\\Im \\alpha\\geq 0$by taking an appropriate one among $\\pm \\alpha$ or $\\pm \\overline\\alpha$ as $\\alpha$.Moreover, we may assume that $\\log\\alpha$ takes its principal value.Hence, we have $\\log\\alpha=\\kappa i$ with $0<\\kappa<\\pi/2$ and $b_1, b_2$ are positive integers.If $d=\\gcd(b_1, b_2)>1$, then we divide $b_i$'s by $d$to have another logarithmic form $\\Lm_1/d=(b_2/d)\\log\\alpha-(b_1/d) \\pi i/2$.If Theorem \\ref{thm2} holds for $\\abs{\\Lm_1/d}$, then this would givethe desired lower bound for $\\abs{\\Lm_1}$.Thus we may assume that $\\gcd(b_1, b_2)=1$.Moreover, we may assume that $\\bp>5.52h^2$.Indeed, if $\\bp\\leq 5.52h^2$,then, after observing that $ah>153.85\\pi>480$ and $D\\log 2<h\\log 2<0.002ah^2$, Liouville's inequalityin the form of Exercise 3.6.b in p. 109 of \\cite{Wal} gives\\begin{equation}\\log \\abs{\\Lm_1}\\geq -\\bp D\\h(\\alpha) -D\\log 2>-2.76ah^2-D\\log 2>-2.762ah^2.\\end{equation}""]"
"[""\\section{Introduction}\\label{intro}Let $\\mathcal{R}$ be a $*$-algebra. For $A,B\\in\\mathcal{R}$, denoted by$A\\bullet B=AB+BA^{*}$ and $[A,B]_{*}=AB-BA^{*}$, which are $\\ast$-Jordan product and $\\ast$-Lie product, respectively. These products are found playing amore and more important role in some research topics, and its studyhas recently attracted many author's attention (for example, see\\cite{cui,li2,mol,taghavi}).Recall that a map $\\Phi:\\mathcal{R}\\to\\mathcal{R}$ is said to be an additive derivation if $$\\Phi(A+B)=\\Phi(A)+\\Phi(B)$$ and $$\\Phi(AB)=\\Phi(A)B+A\\Phi(B)$$for all $A,B\\in\\mathcal{R}$. A map $\\Phi$ is additive $\\ast$-derivation if it is an additive derivation and $\\Phi(A^{*})=\\Phi(A)^{*}$. Derivations are very important maps both in theory and applications, and have been studied intensively (\\cite{chr,sak,sem1,sem2}).Let us define $\\lambda$-Jordan $\\ast$-product by $A\\bullet_{\\lambda}B =AB + \\lambda BA^{\\ast}$. We say that the map $\\Phi$ with the property of$\\Phi(A\\bullet_{\\lambda} B) = \\Phi(A)\\bullet_{\\lambda}B +  A\\bullet_{\\lambda}\\Phi(B)$ is a $\\lambda$-Jordan $\\ast$-derivationmap. It is clear that for $\\lambda = - 1$ and $\\lambda = 1,$ the $\\lambda$-Jordan $\\ast$-derivation map is a $\\ast$-Lie derivation and$\\ast$-Jordan derivation, respectively \\cite{bai}. A von Neumann algebra $\\mathcal{A}$ is a self-adjoint subalgebra of some $B(H)$, the algebra of bounded linear operators acting on a complex Hilbert space, which satisfies the double commutant property: $\\mathcal{A}^{''}=\\mathcal{A}$ where $\\mathcal{A}^{'}=\\{T\\in B(H), TA=AT, \\forall A\\in\\mathcal{A}\\}$ and $\\mathcal{A}^{''}=\\{\\mathcal{A}^{'}\\}^{'}$. Denote by $\\mathcal{Z}(\\mathcal{A})=\\mathcal{A}^{'}\\cap \\mathcal{A}$ the center of $\\mathcal{A}$. A von Neumann algebra $\\mathcal{A}$ is called a factor if its center is trivial, that is, $\\mathcal{Z}(\\mathcal{A})=\\mathbb{C}I$. For $A\\in\\mathcal{A}$, recall that the central carrier of $A$, denoted by $\\overline{A}$, is the smallest central projection $P$ such that $PA=A$. It is not difficult to see that $\\overline{A}$ is the projection onto the closed subspace spanned by $\\{BAx : B\\in \\mathcal{A}, x\\in H\\}$. If $A$ is self-adjoint, then the core of $A$, denoted by $\\underline{A}$, is $\\sup\\{S\\in\\mathcal{Z}(\\mathcal{A}): S=S^{*}, S\\leq A\\}$. If $A=P$ is a projection, it is clear that $\\underline{P}$ is the largest central projection $Q$ satisfying $Q\\leq P$. A projection $P$ is said to be core-free if $\\underline{P}=0$ (see \\cite{mie2}). It is easy to see that $\\underline{P}=0$ if and only if $\\overline{I-P}=I$, \\cite{kad1,kad2}.Recently, Yu and Zhang in \\cite{yu} proved that every non-linear $\\ast$-Lie derivation  from a factor von Neumann algebra into itself is an additive $\\ast$-derivation. Also, Li, Lu and Fangin \\cite{li} have investigated a non-linear $\\lambda$-Jordan$\\ast$-derivation. They showed that if$\\mathcal{A}\\subseteq\\mathcal{B(H)}$ is a von Neumann algebrawithout central abelian projections and $\\lambda$ is a non-zero scalar,then $\\Phi:\\mathcal{A} \\longrightarrow \\mathcal{B(H)}$ is anon-linear $\\lambda$-Jordan $\\ast$-derivation if and only if $\\Phi$ isan additive $\\ast$-derivation.On the other hand, many mathematician devoted""]",,,"['\\section{Main Results}Our main theorem is as follows:\\begin{theorem}\\label{mainsh}Let $\\mathcal{A}$ be a prime $\\ast$-algebra. Let $\\Phi:\\mathcal{A}\\to \\mathcal{A}$ satisfies in  \\begin{equation}\\label{sh1}\\Phi(A\\diamond B)=\\Phi(A)\\diamond B+A\\diamond\\Phi(B) \\end{equation}where  $A\\diamond B = A^{*}B - B^{*}A$ for all $A,B\\in\\mathcal{A}$. If $\\Phi(\\alpha \\frac{I}{2})$ is self-adjoint operator for $\\alpha\\in\\{1,i\\}$ then $\\Phi$ is additive $\\ast$-derivation.   \\end{theorem}\\textbf{Proof.}Let $P_{1}$ be a nontrivialprojection in $\\mathcal{A}$ and $P_{2}=I_{\\mathcal{A}}-P_{1}$.Denote $\\mathcal{A}_{ij}=P_{i}\\mathcal{A}P_{j},\\ i,j=1,2,$ then$\\mathcal{A}=\\sum_{i,j=1}^{2}\\mathcal{A}_{ij}$. For every$A\\in\\mathcal{A}$ we may write $A=A_{11}+A_{12}+A_{21}+A_{22}$. Inall that follow, when we write $A_{ij}$, it indicates that$A_{ij}\\in\\mathcal{A}_{ij}$. For showing additivity of $\\Phi$ on $\\mathcal{A}$, we  use abovepartition of $\\mathcal{A}$ and give some claims that prove $\\Phi$ isadditive on each $\\mathcal{A}_{ij}, \\ i,j=1,2$.\\\\We prove the above theorem by several claims.\\begin{claim}\\label{cl1}We show that $\\Phi(0)=0$.\\end{claim}This claim is easy to prove.\\begin{claim}\\label{cl103}$\\Phi(i\\frac{I}{2})=\\Phi(\\frac{I}{2})=\\Phi(-i\\frac{I}{2})=0$.\\end{claim}Consider $\\Phi(\\frac{I}{2} \\diamond i\\frac{I}{2})=\\Phi(\\frac{I}{2}) \\diamond i\\frac{I}{2}+\\frac{I}{2} \\diamond \\Phi(i \\frac{I}{2})$that imply\\begin{equation}\\label{l4}\\Phi(i \\frac{I}{2})=\\frac{i}{2}\\Phi(\\frac{I}{2})^{*}+\\frac{i}{2} \\Phi(\\frac{I}{2})+\\frac{1}{2} \\Phi(i \\frac{I}{2})-\\frac{1}{2} \\Phi(i \\frac{I}{2})^{*}=i\\Phi(\\frac{I}{2}).\\end{equation}By taking the adjoint of above equation we have $\\Phi(i\\frac{I}{2})=\\Phi(\\frac{I}{2})=0$<!--TRUNCATED-->']"
"[""\\section{Introduction}In this paper we study several types of {\\em orthogonal tensor decompositions} and give algebraic characterizations of the set of decomposable tensors. That is, we give explicit systems of polynomial equations whose zero set is the set of decomposable tensors. When that is not possible, i.e., when the set of decomposable tensors is not Zariski closed, we describe it as a constructible set (using polynomial inequalities in addition to polynomial equalities). In the non-closed case we begin a study of the closure. The main goal here would be to obtain an explicit description of the closure, and we give partial results in this direction.The decompositions that we study can be defined in two equivalent languages: the language of tensors, and the language of polynomials. Indeed, as is well known one can associate to a symmetric tensor (respectively, to an ordinary tensor) a homogeneous polynomial (respectively, a multilinear polynomial) in the same way that a quadratic form is associated to a symmetric matrix and a bilinear form is associated to an arbitrary matrix.The main open problem that arises from this work is to obtain a complete description of the closures. This question is akin to that of characterizing {\\em border rank} of tensors in algebraic complexity. We give partial results using in particular a connection with approximate simultaneous diagonalization (the so-called {\\em ASD property}).In this paper we have studied decompositions of order 3 tensors. In~\\cite{boralevi17}, tensors of higher order are handled by reduction to the case of order 3. Namely, they show that a tensor of order $d \\geq 4$ admits a unitary or real orthogonal decomposition iff the same is true for certain flattenings of the tensor. It would be interesting to find out whether a similar property holds for complex orthogonal decompositions. Following this approach would also require a generalization of our results to ``rectangular'' tensors of order 3. Indeed, we have only studied ``cubic'' tensors (of format $n \\times n \\times n$). But even if we start from a higher order cubic tensor (e.g., a tensor $T$ of order 4 and format $n \\times n \\times n \\times n$), its flattenings will not be cubic in general. For instance, we would obtain from $T$ flattenings of format $n \\times n \\times n^2$.<!--TRUNCATED-->""]",,,"['\\section{Background}In this section we first present some background on tensors and matrices. Indeed, as explained in the introduction simultaneous reduction of matrices (and in particular simulataneous diagonalization) plays an important role in this paper. For the study of complex tensors we will also need some elements of the theory of quadratic forms, which we present in Section~\\ref{quadratic}.\\subsection{Tensors and their slices} \\label{tensorbackground}An order 3 tensor $T \\in K^{n \\times n \\times n}$ can be represented by the trilinear form\\begin{equation} \\label{trilinear}  t(x,y,z)=\\sum_{i,j,k=1}^n T_{ijk}x_iy_jz_k\\end{equation}where $x,y,z$ denote three $n$-tuples of variables. There are 3 ways of decomposing $T$ into a tuple of $n$ matrices: we can decompose in the $x$, $y$ or $z$ direction. We call the resulting matrices the $x$-slices, $y$-slices, and $z$-slices. For instance, the $z$-slices are the matrices of the bilinear forms $\\partial t / \\partial z_k$ ($1 \\leq k \\leq n$).<!--TRUNCATED-->']"
,,,"['\\begin{proposition} \\label{zeroproduct}  If a symmetric tensor $S$ admits a decomposition of the form  $$S=\\sum_{i=1}^r u_i^{\\otimes 3}$$ where the $u_i$  are pairwise orthogonal nonzero vectors then the slices of $S$  commute. Moreover, for such a decomposition the two following properties  are equivalent:  \\begin{itemize}  \\item[(i)] All the $u_i$ are isotropic.  \\item[(ii)] The product of any two (possibly equal) slices of $S$    is equal to 0.  \\end{itemize}  As a result if (i) holds then in any other decomposition  $S=\\sum_{i=1}^q v_i^{\\otimes 3}$ where the $v_i$  are pairwise orthogonal nonzero vectors, all the $v_i$ must be isotropic. \\end{proposition}']"
,,,"[""\\begin{proof}[Proof of Theorem~\\ref{cortho}] If $T$ admits an orthogonal decomposition, Proposition~\\ref{partialorthoC}  shows that the $x$, $y$ and $z$ slices satisfy  conditions (i) and (ii) of Theorem~\\ref{SDCOE}.  For the converse, we begin as in the proof Theorem~\\ref{realortho}  with the case where the trilinear form $f$ associated to $T$  is as in~(\\ref{fABId}). This case can be treated in the same way except  for one important difference. In the proof of Theorem~\\ref{realortho}  we pointed out that the null rows of $C$ must be treated separately  because they cannot be normalized. Over $\\cc$ there is a further  complication: there might be rows $c_k$ such that $c_k^Tc_k=0$ but  $c_k \\neq 0$; such rows could not be normalized.  Fortunately, it turns out that there are no such rows in $C$.  Recall indeed from the  proof of Theorem~\\ref{realortho}  that the $k$-th $x$-slice of $T$ is $X_k=D_kC$,  where $D_k$ is the diagonal matrix with an entry equal to 1  at row $k$ and column $k$, and zeroes elsewhere.  In other words, row $k$ of $X_k$ is equal to $c_k$ and all other rows are identically 0. Moreover $X_k^T X_k$ has one entry (at row $k$ and  column $k$) equal to $c_k^Tc_k$ and only 0's elsewhere.  By condition (i) of Theorem~\\ref{SDCOE} we must  have $\\rk X_k = \\rk X_k^T X_k$.  It follows that $c_k^Tc_k=0$ implies $c_k=0$.  We have therefore shown that all rows of $C$ can be normalized  except the null rows. Morever, any two distinct rows are  orthogonal as in the proof of Theorem~\\ref{realortho}.  We can therefore conclude essentially as in that proof: the set of normalized non-null rows of $C$  is an orthonormal family, and can therefore be extended to an orthonormal  basis of $\\cc^n$ by Witt's theorem (Theorem~\\ref{witt}  and Corollary~\\ref{wittcor}).    Again, the coefficients $\\alpha_i$ corresponding to the new vectors  in this basis are set to 0.  It remains to reduce the general case of Theorem~\\ref{cortho}  to~(\\ref{fABId}) and this can be done essentially as in the proof of  Theorem~\\ref{realortho}. Indeed, let $Z_1,\\ldots,Z_n$  be the $z$-slices of $T$. By Theorem~\\ref{SDCOE} there are  orthogonal matrices $U,V$ such that all the matrices $D_k = U^T Z_k V$  are diagonal. We set $h(x,y,z)=f(Ux,Vy,z)$ like in the proof  of Proposition~\\ref{partialortho} and the $z$-slices of $h$  are the diagonal matrices $D_k$ by Proposition~\\ref{slices}.  This implies that $h$ is as in~(\\ref{ABId}).  Moreover, $h$ satisfies conditions (i) and (ii) of Theorem~\\ref{SDCOE}  by Lemma~\\ref{lem:cortho}. This completes the reduction, and the proof  of Theorem~\\ref{cortho}.\\end{proof}""]"
"[""\\section{Introduction.}\\label{sec:intro}A homogeneous polynomial $$p(x,y,z)=\\sum_{i+j+k=4} p_{ijk}x^iy^jz^k$$ in three variables of degree 4 is called a ternary quartic. Hilbert's classical theorem \\cite{Hilbert}, dating back to 1888, states that every ternary quartic that takes only nonnegative values, i.e., such that $p(x,y,z) \\ge 0$ for all $x,y,z \\in{\\mathbb R},$ can be written as a sum of three squares of homogeneous quadratic polynomials. This theorem stood as a precursor of Hilbert's 17th problem and subsequent development, and to this day attracts a lot of attention.One distinguishes two parts to Hilbert's theorem: the existence of a representation as a sum of squares (qualitative part) and the assertion that at most three squares suffice (quantitative part). Hilbert's original proof, cast in modern form, takes roots in advanced topology and algebraic geometry. Many attempts have been made in search of more elementary proofs. In 1977, Choi and Lam \\cite{ChoiLam} gave an elementary proof of the qualitative part, based on properties of extremal positive semidefinite forms. In 2004, Pfister \\cite{Pfister} gave a different elementary proof, which was constructive. New approaches to Hilbert's theorem were developed in \\cite{PS} and \\cite{PRS}. But no simple elementary explanation of the quantitative part has been found.In this note we would like to offer a new elementary proof of the qualitative part of Hilbert's theorem. Our approach uses linear algebra and convex geometry. <!--TRUNCATED-->""]","[""\\section{The PSD$_6$ cone and Hilbert's theorem.}\\label{sec:cone}The connection to Hilbert's theorem can now be explained.A polynomial $p(x,y,z)$ is a sum of squares of homogeneous quadratic polynomials if and only if it can be represented in the form\\begin{equation}\\label{pxyz} p(x,y,z) =  \\begin{bmatrix} x^2 & xy & xz & y^2 & yz & z^2 \\end{bmatrix} A \\begin{bmatrix} x^2 \\cr xy \\cr xz \\cr y^2 \\cr yz \\cr z^2 \\end{bmatrix}, \\end{equation}where $A \\in {\\rm PSD}_6$. Indeed, if $A=\\sum_{i=1}^k a_i a_i^\\top$, where $a_i\\in {\\mathbb R}^6$, then \\eqref{pxyz} turns into a desired sum-of-squares representation:$$ p(x,y,z) = \\sum_{i=1}^k (v^\\top a_i)^2,\\qquad v^\\top= \\begin{bmatrix} x^2 & xy & xz & y^2 & yz & z^2 \\end{bmatrix}.$$Moreover, if ${\\mathcal W}$ is the subspace of $S_6$ consisting of the matrices $$\\begin{bmatrix} 0 & 0 & 0 & w_1 & w_2 & w_3 \\cr 0 & -2w_1 & -w_2 & 0 & w_4 & w_5 \\cr0 & -w_2 & -2w_3 & -w_4 & -w_5 & -0 \\cr w_1 & 0 & -w_4 & 0 & 0 & w_6 \\cr w_2 & w_4 & -w_5 & 0 & -2w_6 & 0\\cr w_3 & w_5 & 0 & w_6 & 0 & 0\\end{bmatrix},\\qquad w_1, \\ldots, w_6 \\in {\\mathbb R}, $$then \\eqref{pxyz} holds if and only if $A\\in A_0 + {\\mathcal W}$. Thus it suffices to show that \\begin{equation}\\label{ne} (A_0 + {\\mathcal W}) \\cap {\\rm PSD}_6 \\neq \\varnothing . \\end{equation}The key in proving implication \\eqref{ne2} is our main result, which we now state.\\begin{theorem}\\label{main}Let $\\ \\mathcal C$ be the cone of positive semidefinite matrices in $S_6$ satisfying \\eqref{C}.Then every extreme ray of $\\ \\mathcal C$ is generated by a rank 1 matrix $vv^\\top$, where\\begin{equation}\\label{v} v^\\top= v(x,y,z)^\\top:= \\begin{bmatrix} x^2 & xy & xz & y^2 & yz & z^2 \\end{bmatrix}, \\end{equation} for some $ x,\\ y,\\ z \\in\\mathbb R$. Thus every element of $\\ {\\mathcal C}$ is a nonnegative linear combination of matrices $vv^\\top$. \\end{theorem}The second assertion of Theorem \\ref{main} follows by Minkowski's theorem.Note that if $p(x,y,z) $ takes only nonnegative values, we obtain that $${\\rm tr} (A_0 vv^\\top ) ={\\rm tr} (v^\\top A_0 v ) = p(x,y,z) \\ge 0.$$ By Theorem \\ref{main}, each element $C$ of the cone ${\\mathcal C} =  {\\rm PSD}_6 \\cap {\\mathcal W}^\\perp$ is of the form $C = \\sum_i \\rho_i v_i v_i^\\top$ with $\\rho_i \\ge 0$ and $v_i=v(x_i,y_i,z_i)$, and thus $${\\rm tr} (A_0 C ) = \\sum_i \\rho_i p(x_i , y_i , z_i ) \\ge 0.$$ Consequently, Theorem \\ref{main} proves""]","[""\\section{The number of squares.}\\label{sec:number}As noted in the introduction, Hilbert's result is actually stronger than what we have shown: the sum-of-squares representation can always be chosen to have at most three squares. The known proofs of this fact, including Hilbert's original proof \\cite{Hilbert}, are much less elementary \\cite{PS, PRS}, and a linear-algebraic argument, if it exists, is yet to be found. From the proof of Choi and Lam \\cite{ChoiLam} one extracts additional information that every nonnegative ternary quartic is a sum of five squares. Pfister's proof \\cite{Pfister} shows that at most four squares suffice.We note that the four squares conclusion can be reached in a different way, using the geometry of the PSD$_6$ cone.Namely, by letting $\\mathcal A=A_0+\\mathcal W$ and $n=6$ in the following lemma of Barvinok.We conclude with a few words on how one may numerically find a sum-of-squares representation using semidefinite programming (SDP). General references on SDP and convex optimization are \\cite{B, BV}. When we let $V_1, \\ldots , V_{15}$ be a basis for ${\\mathcal W}^\\perp$, finding a sum-of-squares representation comes down to finding $A \\in {\\rm PSD}_6$ with ${\\rm tr} (A V_i ) = {\\rm tr} (A_0 V_i)=: b_i$, $i=1,\\ldots , 15$, which is exactly a feasibility problem in SDP. Choosing a positive definite $C$, one can perform the SDP$$ \\inf {\\rm tr} (CA),\\qquad {\\rm subject \\ to }\\quad  A\\in {\\rm PSD}_6,\\quad {\\rm tr} (A V_i ) =  b_i,\\quad i=1,\\ldots , 15. $$In \\cite[Section 6]{PSV} it is observed that for random $C$ there is a positive probability to find a rank 3 optimal $A$. Thus a repeated performance of the above SDP with random $C$, ultimately yields a representation as a sum of three squares. Here we accept a solution as having rank at most 3 when its fourth singular value is sufficiently small. <!--TRUNCATED-->""]","['\\section{Proof of the Theorem.}\\label{sec:proof}We now prove Theorem \\ref{main}. The argument hinges on the following lemma.\\begin{lemma} \\label{vectors}Let $u,v,w,y \\in {\\mathbb R}^2$ be such that $\\langle u,v \\rangle = \\langle w,y \\rangle$. Then there exists a rotation $R$ with the property that $$R\\begin{bmatrix} u & v & w & y \\end{bmatrix}=\\begin{bmatrix} u_1 & v_1 & w_1 & y_1 \\cr u_2 & v_2 & w_2 & y_2 \\end{bmatrix}$$satisfies $u_1v_1 = w_1 y_1$ and $u_2 v_2 = w_2 y_2 $. \\end{lemma}We alert the reader that the subscripts in Lemma \\ref{vectors} are used to indicate the components of the rotated, not original, vectors. The same convention applies further below in the proof of Theorem \\ref{main}.{\\it Proof.\\rm} Though the statement is about vectors in $u,v,w,y \\in{\\mathbb R}^2$, it is convenient to treat them as complex numbers $\\alpha , \\beta , \\gamma , \\delta$,$$ \\begin{bmatrix} \\alpha & \\beta & \\gamma & \\delta \\end{bmatrix} =  \\begin{bmatrix} 1 & i \\end{bmatrix}  \\begin{bmatrix} u & v & w & y \\end{bmatrix} . $$ The task  amounts to choosing an angle of rotation $\\theta$ so that the rotated complex numbers satisfy\\begin{equation}\\label{rot}\\begin{aligned}&{\\rm Re} (e^{i\\theta}\\alpha) {\\rm Re}(e^{i\\theta} \\beta) - {\\rm Re}(e^{i\\theta}\\gamma)  {\\rm Re}(e^{i\\theta} \\delta)=u_1v_1-w_1y_1=0\\\\ &{\\rm Im}(e^{i\\theta}\\alpha) {\\rm Im}(e^{i\\theta} \\beta)- {\\rm Im}(e^{i\\theta}\\gamma)  {\\rm Im}(e^{i\\theta}  \\delta)=u_2v_2-w_2y_2=0.\\end{aligned}\\end{equation}But since the scalar products $\\langle u,v \\rangle,\\ \\langle w,y \\rangle$ are rotation invariant, i.e.,\\begin{align*}&{\\rm Re} (e^{i\\theta}\\alpha) {\\rm Re}(e^{i\\theta} \\beta)+{\\rm Im} (e^{i\\theta}\\alpha) {\\rm Im}(e^{i\\theta} \\beta)={\\rm Re}(e^{i\\theta}\\alpha\\,\\overline{e^{i\\theta}\\beta})={\\rm Re}(\\alpha\\,\\overline{\\beta})\\\\&{\\rm Re} (e^{i\\theta}\\gamma) {\\rm Re}(e^{i\\theta} \\delta)+{\\rm Im} (e^{i\\theta}\\gamma) {\\rm Im}(e^{i\\theta} \\delta)={\\rm Re}(e^{i\\theta}\\gamma\\,\\overline{e^{i\\theta}\\delta})={\\rm Re}(\\gamma\\,\\overline{\\delta}),\\end{align*}the assumption $\\langle u,v \\rangle=\\langle w,y \\rangle$ means that the sum of numbers$${\\rm Re} (e^{i\\theta}\\alpha) {\\rm Re}(e^{i\\theta} \\beta)-{\\rm Re} (e^{i\\theta}\\gamma) {\\rm Re}(e^{i\\theta} \\delta),\\quad {\\rm Im}(e^{i\\theta}\\alpha) {\\rm Im}(e^{i\\theta} \\beta)- {\\rm Im}(e^{i\\theta}\\gamma)  {\\rm Im}(e^{i\\theta}  \\delta)$$is zero. Thus, to obtain \\eqref{rot} it suffices to choose $\\ \\theta\\ $ so that the difference of these numbers is also zero, which reduces to %$${\\rm Re} (e^{i\\theta}\\alpha) {\\rm Re}(e^{i\\theta} \\beta) - {\\rm Re}(e^{i\\theta}\\gamma)  {\\rm Re}(e^{i\\theta} \\delta) -[ {\\rm Im']"
,,['\\section{Conclusion}\\label{sec:conclusion}As observed previously by various authors a possibly available factorization of $A$ can be reused both for the tangent ($A \\cdot \\X^{(1)}=\\B^{(1)}-A^{(1)} \\cdot \\X$) and the adjoint ($A^T \\cdot \\B_{(1)} = \\X_{(1)}$) systems. The additionalworst case computational cost of $O(n^3)$ can thus be reduced to $O(n^2).$Higher-order tangents [adjoints] of linear systems amount to repeated solutions of linear systems with the same [transposed] system matrix combined with tangent [adjoint] BLAS. <!--TRUNCATED-->'],"['\\section{BLAS Revisited}In its basic form AD builds on known tangents and adjoints of the arithmeticfunctions and operators built into programming languages. Tangents and adjointsare propagated along the flow of data according to the chain rule of differential calculus.We enumerate entries of vectors $\\V \\in \\R^n$ staring from zeroas $v_0,\\ldots,v_{n-1}.$From the perspective of AD adjoint versions of higher-level BLAS are derived asadjoints of lower-level BLAS. Optimization of the result aims for implementation using the highest possible level of BLAS. For example, adjoint matrix-matrix multiplication (level-3 BLAS) is derived from adjoint matrix-vector multiplication (level-2 BLAS) yielding efficient evaluation as two matrix-matrix products (level-3 BLAS) as shown in Lemma~\\ref{lem:AX}.Rigorous derivation of this result requires bottom-up investigation of the BLAS hierarchy. We start with basic scalar multiplication (Lemma~\\ref{lem:ax}) followed by the inner vector (Lemma~\\ref{lem:aTx}) and matrix-vector (Lemma~\\ref{lem:Ax}) products as prerequisites for the matrix-matrix product. <!--TRUNCATED-->']"
,,,
"['\\section{Introduction}\\label{sec:intro}The scalability of optimization solvers relies quite heavily on the solution of the underlying linear algebra systems. Advances in direct sparse linear algebra solvers have been instrumental in the widespread use of quadratic programming and nonlinear programming solvers such as {\\tt Ipopt}, {\\tt OOQP}, and {\\tt Knitro} \\citep{HSL, MUMPS1, MUMPS2}. Specialized direct solution techniques have also been developed to tackle large-scale and {\\em block-structured} systems (using variants of Schur complement decomposition techniques) \\citep{zavalalaird, WordDaniel2014Epso, pipsnlp, GondzioJacek2009Esip, GondzioJacek2003Pisf}.  Block structures appear in many important applications such as parameter estimation, stochastic programming, network optimization, and optimal control.  Schur decomposition techniques can also leverage {\\em parallel computing architectures} and have enabled the solution of problems with millions to billions of variables and constraints.  Unfortunately, many applications of interest still remain inaccessible due to fundamental scalability limitations of Schur complement techniques. Specifically, Schur complement decomposition does not scale well in problems that exhibit high degrees of {\\em block coupling}.  This is because high degrees of coupling require assembling and factorizing large Schur complement matrices (which are often highly dense). Iterative solution techniques \\cite{QuarteroniAlfio2007Nm, MaC.F.2015TcUm, ForsgrenAnders2007ISoA, ElmanHowardC.1994IaPU, BenziMichele2006Oteo, BenziMichele2005Nsos} and associated preconditioning strategies  \\cite{CaoYankai2016Cpfs, MoralesJoseLuis2000APbL, GolubGeneH.2003OSBI, RustenTorgeir1992APIM, GillPhilipE.1992PfIS, WalterZulehner2002Aoim} have been proposed to address fundamental scalability issues of direct linear algebra strategies. In the context of block-structured problems, attempts have been made to solve the Schur complement system by using iterative solution techniques (to avoid assembling and factorizing the Schur complement). Preconditioners for Schur complements arising in special problem classes such as multi-commodity networks and stochastic programs have been developed \\cite{CaoYankai2016Cpfs}. Unfortunately, preconditioning strategies for general problem classes are still lacking.  Another important issue that arises in this context is that the implementation of advanced linear algebra strategies is non-trivial (e.g., it requires intrusive modifications of optimization solvers). Along a separate line of research, significant advances have been made in the development of {\\em problem-level} decomposition techniques such as the alternating direction method of multipliers (ADMM) and Lagrangian dual decomposition \\cite{HanDeren2013LLCo, GuoKe2017CoAf, HongMingyi2017Otlc, HanDeren2013LLCo, GoldfarbDonald2012FMAf, HeBingsheng2012OtOC, RodriguezJoseS.2018BAin}. Such approaches are flexible and rather easy to implement but suffer from slow convergence.  Recently, it has been proposed to use ADMM as a preconditioner for Krylov-based iterative solvers such as GMRES \\cite{ZhangRichard2016PIiA, ZhangRichardY.2018GAfQ}. In this work, we provide a detailed derivation of this ADMM-GMRES approach and test its performance in the context of block-structured linear algebra systems.  We demonstrate that this approach overcomes the scalability issues of Schur complement decomposition. We also demonstrate that this approach is significantly more effective than using ADMM directly. Our tests are facilitated by the use of {\\tt PyNumero}, a recently-developed Python framework that enables the implementation and benchmarking of optimization algorithms. We use the proposed framework to tackle problems with hundreds of thousands to millions of variables generated from standard benchmark sets and power grid applications.  The paper is structured as follows. In Section \\ref{sec:prelims} we define the problem of interest and provide preliminary information on the use of Schur complement decomposition and ADMM approaches. In Section \\ref{sec:derivation} we provide a detailed derivation of the ADMM-GMRES approach and']","['\\section{Solution using ADMM-GMRES}\\label{sec:derivation}The key observation that motivates our work is that ADMM can be used as a {\\em preconditioner} for iterative linear algebra techniques such as GMRES \\cite{ZhangRichard2016PIiA, ZhangRichardY.2018GAfQ}. To derive the ADMM preconditioning strategy, we consider the {\\em regularized} QP \\eqref{eq:block-structured-qp}:\\begin{subequations}\\begin{align}\\underset{x_i, z}{\\text{min}} \\;\\; & \\;\\;   \\sum_{i \\in \\mathcal{P}} \\frac{1}{2}x_i^T D_i x_i + c_i^T x_i + \\frac{\\rho}{2}\\|Ax_i+B_iz\\|^{2}& \\label{eq:AUGQB2-a} \\\\\\text{s.t.} \\;\\; &\\;\\; J_i x_i  = b_i,\\; \\;\\;\\;\\;\\;\\; \\quad (\\lambda_i)\\quad i \\in \\mathcal{P} \\label{eq:AUGQB2-b}\\\\&\\;\\; A_{i}x_i + B_{i}z = 0,\\; (y_i)\\quad i \\in \\mathcal{P} \\label{eq:AUGQB2-c}.\\end{align}\\label{eq:block-structured-augmented-qp}\\end{subequations}The optimality conditions of the regularized QP are given by:\\begin{equation}\\label{eq:kkt-aug-compact}\\underbrace{\\begin{bmatrix}K_{\\rho} &  \\rho A^{T}B &  A^{T}   \\\\ \\rho B^TA &       \\rho B^{T}B         & B^{T}                \\\\A  &   B      &                 \\\\   \\end{bmatrix}}_{H_\\rho}\\underbrace{\\begin{bmatrix}\\upsilon\\\\z\\\\y\\\\\\end{bmatrix}}_{u}=\\underbrace{\\begin{bmatrix}\\gamma \\\\0 \\\\0 \\\\\\end{bmatrix}}_r.\\end{equation}ADMM can be interpreted as a Gauss-Seidel (alternating) minimization of the block and coupling variables and the dual variables \\citep{boyd2011distributed, eckstein1992douglas, RodriguezJoseS.2018BAin}. This induces a splitting operator $H_{\\rho}=M_{\\rho}-N_{\\rho}$ satisfying:\\begin{equation}\\label{eq:splitting}\\underbrace{\\begin{bmatrix}K_{\\rho} &  \\rho A^{T}B &  A^{T}   \\\\ \\rho B^TA&       \\rho B^{T}B         & B^{T}                \\\\A  &   B      &                 \\\\   \\end{bmatrix}}_{H_\\rho}=\\underbrace{\\begin{bmatrix}K_{\\rho} &   & \\\\\\rho B^TA &       \\rho B^TB   &               \\\\A  &   B      &       -\\frac{1}{\\rho}I           \\\\   \\end{bmatrix}}_{M_{\\rho}}-\\underbrace{\\begin{bmatrix}\\; &  -\\rho A^TB &  -A^{T}   \\\\\\; &                   & -B^{T}                \\\\\\; &                   &  -\\frac{1}{\\rho}I          \\\\   \\end{bmatrix}}_{N_{\\rho}}\\end{equation}Applying splitting \\eqref{eq:splitting} to \\eqref{eq:kkt-aug-compact} gives the operator:\\begin{equation}\\label{eq:fixed-point}T_\\rho(u) := G_{\\rho}u + f_{\\rho}\\end{equation}where $G_{\\rho}{=}M_{\\rho}^{-1}N_{\\rho}$ and $f_{\\rho} {=} M_{\\rho}^{-1}r$. Note that any $u$ satisfying the fixed-point $T_\\rho(u){=}u$ also satisfies $(I-G_{\\rho})u {=} f_{\\rho}$ and is a solution of the preconditioned KKT system:\\begin{equation}\\label{eq:preconditioned-sys}M_{\\rho}^{-1}H_{\\rho}u = M_{\\rho}^{-1}r.\\end{equation}The key idea behind ADMM-GMRES is to solve the system $M_{\\rho}^{-1}H_{\\rho}u {=} M_{\\rho}^{-']",,"['\\section{Preliminaries}\\label{sec:prelims}We study the solution of block-structured quadratic programs (QP) of the form:\\begin{subequations}\\begin{align}\\underset{x_i, z}{\\text{min}} \\;\\; & \\;\\;   \\sum_{i \\in \\mathcal{P}} \\frac{1}{2}x_i^T D_i x_i + c_i^T x_i& \\label{eq:QB2-a} \\\\\\text{s.t.} \\;\\; &\\;\\; J_i x_i  = b_i,\\; \\;\\;\\;\\;\\;\\; \\quad (\\lambda_i)\\quad i \\in \\mathcal{P} \\label{eq:QB2-b}\\\\&\\;\\; A_{i}x_i + B_{i}z = 0,\\; (y_i)\\quad i \\in \\mathcal{P} \\label{eq:QB2-c}.\\end{align}\\label{eq:block-structured-qp}\\end{subequations}\\noindent Here, $\\mathcal{P} := \\{1,\\dots,P\\}$ is a set of block variable partitions. Each partition contains a vector of primal variables $ x_i \\in \\mathbb{R}^{n_{x_i}}$ and the vector $ z \\in \\mathbb{R}^{n_z}$ contains the primal variables that the couple partitions. The total number of primal variables is $n := n_z + \\sum_{i\\in\\mathcal{P}}n_{x_i}$. Equation \\eqref{eq:QB2-b} are the partition constraints with their respective dual variables $\\lambda_i\\in \\mathbb{R}^{m_i}$. Equation \\eqref{eq:QB2-c} are the constraints that {\\em link} partitions across set $\\mathcal{P}$ and have associated dual variables $y_i\\in\\mathbb{R}^{l_i}$. We assume that the partition matrices $J_i\\in\\mathbb{R}^{m_i\\times n_i}$ have full rank and that the right-hand-side coefficients $b_i\\in \\mathbb{R}^{m_i}$ are in the column space of $J_i$. The total number of partition constraints is $m :=\\sum_{i\\in\\mathcal{P}}m_i$. We refer to $A_i\\in \\mathbb{R}^{n_z\\times n_i}$ and $B_i\\in \\mathbb{R}^{n_z\\times n_z}$ as linking matrices and we assume them to have full rank. The total number of linking constraints is $l:=\\sum_{i\\in\\mathcal{P}}l_i$.  The QP under study is the main computational kernel behind nonlinear programming strategies because it is used for computing primal-dual search steps. We make the blanket assumption that the block-structured QP is strongly convex and that the combined Jacobian matrix (obtained by assembling partition and coupling constraints) has full rank. Strong convexity can be obtained by ensuring that all block Hessian matrices $D_i$ are positive definite. Strong convexity and full-rank conditions guarantee that the primal-dual solution of the QP exists and is unique. Moreover, these assumptions guarantee that the QP solution is a unique minimizer and that this can be found by solving the first-order stationarity conditions.  Additional assumptions will also be needed on the nature of the building blocks of the QP (associated with each partition). Such assumptions are needed to ensure that proposed decomposition schemes are well-defined and will be stated as we proceed (in order to maintain clarity in the presentation). <!--TRUNCATED-->']"
,,"['\\section{Conclusions and Future Work}\\label{sec:conclusions}We have demonstrated that ADMM provides an effective mechanism to precondition iterative linear solvers and with this overcome scalability limitations of Schur complement decomposition.  Our results also demonstrate that the approach is robust to the choice of the penalty parameter. As part of future work, we will investigate the performance of ADMM-GMRES within a nonlinear interior-point framework. Here, it will be necessary to relax our assumptions on strong convexity and on the full rank of the Jacobian. Preliminary results reported  in the literature indicate that different types of primal-dual regularized KKT systems can be used to compute search steps within interior-point methods under such relaxed conditions \\citep{ChiangNai-Yuan2017AALF}. We will investigate ADMM variants to precondition such systems. The effectiveness of using ADMM as a preconditioner makes us wonder whether other approaches can be used for preconditioning as well. For instance, inexact dual Newton strategies can potentially be used to precondition structured KKT systems. This is an interesting direction of future work. We will also investigate advanced ADMM strategies that use second-order multiplier updates to accelerate the preconditioner. <!--TRUNCATED-->']","['\\begin{figure}[!htb]  \\begin{center}   \\includegraphics[width=0.7\\textwidth]{Figures/residuals_large_instances.eps} \\caption{Residuals for Schur decomposition, ADMM, and ADMM-GMRES for problems with $n_z\\geq 100$.}\\label{fig:residuals-large-results}  \\end{center}\\end{figure} \\begin{figure}[!htb]  \\begin{center}   \\includegraphics[width=0.7\\textwidth]{Figures/residuals_small_instances.eps} \\caption{Residuals for Schur decomposition, ADMM, and ADMM-GMRES for problems with $n_z < 100$.}\\label{fig:residuals-small-results}  \\end{center}\\end{figure}']"
